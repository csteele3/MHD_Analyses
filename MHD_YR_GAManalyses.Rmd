---
title: "GAM_MHD"
author: "Christen"
date: "March 25, 2020"
output:
  html_document:
    df_print: paged
---

GAM Work with Monarch Health Data



*Overall Research Objectives*
1.	Determine how OE infection prevalence changes seasonally (within a year) and across years in the southeastern U.S. 

2.	Determine where OE infection prevalence is most prevalent within each year and how the spatial patterns of infection have changed over time? 

3.	Determine whether climatic variables are correlated with the spatial and temporal changes in OE infection prevalence in the southeastern U.S.

*Why use a GAM?*
We do not expect the patterns in disease prevalence as described by our predictor variables to be linear. For example, infection prevalence may experience increases during periods of warm weather such as in late fall and early spring, while mid winter low temperatures may result in a decline in infection prevalence as population density and therefore transmission potential declines. Graphs of the dataset are provided below for visual inspection of the linear or non-linear trends.  

*Dataset*
```{r MHD Dataset}
#The below dataset should be used to examine winter OE data  
#wdata <- read.csv("./data/MHD_WD.csv", colClasses = c("Pop_density_CAT" = "factor", "Hardiness2" = "factor", "Infection_Severity" = "factor", "Month"= "factor", "Year"= "factor"))

#The dataset below should be used to examine annual OE data
data<- read.csv("./data/YR_MHD_weather_hardi2.csv")
data<- YR_MHD_weather_hardi2
summary (data)
```


*Exploration of the Data through Graphing*
Checking the distributions of variables to be used for analysis 
```{r}
Month <- data$Month
hist(Month)

latitude <- data$latitude
hist(latitude)

longitude <- data$longitude
hist(longitude)
```


Graph the relationship between infection and day of year to visually inspect the relationship for linearity. 
From this figure it is hard to discern any clear pattern. There seems to be a mostly random distribution of infection prevalence across all year (by day of year). #Note: Day 200 is mid-July 
```{r}
#Convert variables into different forms for graphing purposes
data$Day_Year<-as.factor(data$Day_Year)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
library(plyr)
r2<-ddply(data,.(Day_Year), summarize, mean=mean(Infection))
r3<- (r2$mean)
r2$r3 <- r3

library(ggplot2)
ggplot(data=data, aes(x=as.numeric(Day_Year),shape = Year, r3))+geom_point() + geom_smooth()
```

Graph the relationship between infection and year to visually inspect the relationship for linearity. 
```{r}
#Convert variables into different forms for graphing purposes
##data$Season<-revalue(data$Season, c("11_12"="1", "12_13"="2", "13_14"="3", "14_15"="4", "15_16"="5", "16_17"="6", "17_18"="7"))

data$Year.Sampled<-as.factor(data$Year.Sampled)
data$Infection<-as.numeric(data$Infection)


#Calculate the probability of infection on each day of year 
library(plyr)
s1<-ddply(data,.(Year.Sampled), summarize, mean=mean(Infection))
s2<- (s1$mean)
s1$s2 <- s2

#Plot 
plot(as.numeric(s1$Year.Sampled), s1$
       s2, xlab = "Year", ylab = "Probability of Infection", type='o')
```


Graph the spatial distribution of the data.  
```{r}
library("maps")
library("dplyr")
library("ggplot2")

states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

gg<-ggplot() + 
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group), fill="grey", colour = "white")+
   geom_point(data=data, aes(x=Longitude, y=Latitude, color = factor(Infection)), position = "jitter", size=1)+
  scale_colour_manual(values = c("1"="chocolate3", "0"= "cyan"))+
 
 # geom_point(data=data, aes(x=Longitude, y=Latitude),size=1, colour = "black")
  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))+
   xlab("Longitude") + ylab("Latitude")

  gg + theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
```


*Objective 1: Analyzing the Temporal Patterns in OE Infection Prevalence in the SE*
  
  *Hypothesis 1: Within Year*
If OE infection prevalence is primarily driven by reproductive behavior and breeding is  occurring year-round in the southeast, then  OE infection prevalence may remain high, showing little correlation with day of year. 

  *Hypothesis 2: Between Years*
  If the population of winterbreeding monarchs in the southeastern US is growing (and therefore becoming more dense), we expect that OE infection prevalence may increase with year as increased monarch density leads to higher rates of transmission.
  
*Model Construction*
This model is designed to characterize temporal infection patterns in the southeast by including day of year, year and an interaction of these two terms in a single model. 

Details on the model:
1. Why "ti"?
  Tensor product smooths are used to model interactions between variables that have different natural scales. They multiply the smooth terms of one variable by the factor levels of the other. 

2. Why "by"?
  "By" variables are used for constructing "varying coefficient models" and for letting smooths interact with parametric terms. In this case our smooth is the Day_Year variable and the Season term is the parametric term (and a factor). 
The interaction between two parametric terms would use the : or * connotation, but smooth interactions cannot. "By" generates an indicator vector for each level of a factor  unless the factor is ordered. If it is ordered then a different smooth is generated for each fator level (except the first level). 


3. Why k=6? 
The term k is also referred to as "knots". These are the natural breaks in the basis functions that make up the "basis" or "smooth function" or "spline". Where the knots break along the spline is usually defined by quantiles. K is usually the max number of degrees of freedom allowed for a smooth term in the model. K in the models will always =  (max degrees of freedom - 1). In this case 

4. Why "family=binomial"? 
The response variable is either 0 (uninfected) or 1 (infected), this we have 2 curves for our response variable distribution and this requires that the model be fit with a binomial family distribution. 

5. Why is "Season" parametric?


6. Why is Volunteer "bs=re"? 
In order to include Volunteer as a random effect, we tell the model to use the random effect basis function to model this variable. This method only applies to mcgv, as a different method is used for gamm. 

7. Why REML? 

8. Should I be using a different basis function? Cubic? 

```{r eval, = FALSE if exists (gam_modIF.Rdata) }
require(mgcv)

###Make sure each variable is in the correct format 
data$Day_Year<-as.numeric(data$Day_Year)
data$Year.Sampled<-as.numeric(data$Year.Sampled)
data$Infection<-as.factor(data$Infection)

gam_modTIME <- gam(Infection ~  s(Day_Year, bs = "cc") + s(Year.Sampled,  k = 8) + ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

save(gam_modTIME, file="gam_modII.Rdata")

summary(gam_modTIME)



```



The following models are the different iterations that eventually led to the final model selected above. They change terms in the model from smooth to linear or investigate the way the interaction term is used. 
```{r eval, = FALSE if exists (gam_modIF.Rdata) }

# #gam_mod: Model with Year as a linear effect
# gam_mod <- gam(Infection ~  Year.Sampled + ti(Day_Year, by = Year.Sampled, k =6) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# summary(gam_mod) 
# save(gam_mod, file="gam_mod.Rdata")
# 
# 
# ###gam_modSS: Fit model with Year smoothed (numeric)
# 
# data$Year.Sampled<-as.numeric(data$Year.Sampled)
# 
# gam_modSS <- gam(Infection ~  s(Year.Sampled, k = 8) + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# summary(gam_modSS)
# save(gam_modSS, file="gam_modSS.Rdata")
# 
# 
# ###Fit model with Day_Year as an added  individual effect with both day_year and year smoothed
# 
# #Running Both as numeric variables
# data$Year.Sampled<-as.numeric(data$Year.Sampled)
# data$Day_Year<-as.numeric(data$Day_Year)
# 
# gam_modIY <- gam(Infection ~  s(Day_Year) + s(Year.Sampled, k = 8) + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modIY, file="gam_modIY.Rdata")
# summary(gam_modIY)
# 
# 
# ###Fit model with Day_Year as an added  individual effect with year as a factor
# data$Year.Sampled<-as.factor(data$Year.Sampled)
# data$Day_Year<-as.numeric(data$Day_Year)
# 
# 
# gam_modIF <- gam(Infection ~  s(Day_Year) + Year.Sampled + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modIF, file="gam_modIF.Rdata")
# summary(gam_modIF)
# 
# 
# #WHen comparing all 4 models:
# #gam_modIF = 12266.17
# #gam_modIY = 12813.5. 
# #gam_mod =   12317.86
# #gam_modSS = 12859.28
# AIC(gam_modIY, gam_modIF, gam_mod, gam_modSS)
# 
# 
# #Model without "by" as the interaction term# 
# gam_modNoby <- gam(Infection ~  s(Day_Year, bs = "cc") + s(Year.Sampled,  k = 8) + ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modNoby, file="gam_modNoby.Rdata")
# 
# AIC(gam_modIY, gam_modIF, gam_mod, gam_modSS, gam_modII)

#Could it be that the AIC value is higher when year is a factor because it is more closely fitting the data, but it is not answering the question we have about Year, which is how does infection change OVER TIME? WIth year as a factor, the question seems to be how does infection change from year to year or within a single year. 

# gam_modIY	264.3734	14116.17		
# gam_modIF	278.3352	13646.39		
# gam_mod	267.6086	  12317.86		
# gam_modSS	242.2135	12859.28		
# gam_modII	295.0423	13675.55	
#with te added: gam_modII	292.4565	13674.03	(without "by")
#with Day_Year as cyclic: 13671.31 (without "by")
#with Day as cyclic and Year as cubic: 13671.31 (so not needed) (without "by")
# "by" added back in 14281.25
#remove "Te"

# #Model with only interaction term for contour plot#
# gam_modCC <- gam(Infection ~   ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

```


*Summarize output of model*
```{r}
summary.gam(gam_modTIME) 

#For standard deviations and confidence intervals 
gam.vcomp(gam_modTIME)
```
Key points from model output: 

The model output summary tells us that the additive temporal variables explain 42.5% of the deviance.We also can convert the outputs from the model to actual probabilities because the GAM model uses a log-odds scale for estimating outputs. After converting the intercept estimate below we find that the model predicts a 44.1% chance of infection overall. 
```{r}
plogis(-0.2373)
```


*Checking the Model*
```{r}
#gam.check(gam_modTIME)

#gam.check does not seem to be the best method for visualizing logistic models

library(arm)

binnedplot(fitted(gam_modTIME), 
           residuals(gam_modTIME, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           main = "S Year", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")
```


*Plot the results of the temporal model* 
Plots of the partial effects of the interaction fo Day_Year and Season for each level of Season. 

```{r}
require(mgcViz)
require(mgcv)

#Year.Sampled = c("2011-2012", "2012-2013", "2013-2014", "2014-2015", "2015-2016", "2016-2017", "2017-2018")
#axis(, at=1:8, xaxp=letters[2011:2018])

#Version1
plot(gam_modII, pages = 1,  trans = plogis,
     shift = coef(gam_modIY)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

#Version2
b <- getViz(gam_modII, trans = plogis)

print(plot(b, allTerms = T))
```

Visualization of the interaction between Year and Day_Year in a 3D plot. 
```{r}

vis.gam(gam_modII, n.grid = 50, theta = 400, phi = 20, ylab = "Day of Year", xlab = "Year", zlab = "",
        ticktype = "detailed", color = "terrain", main = "Interaction between day of year and season")



##theta= rotates the view around horizontally 
##phi = rotates the view vertically 
##n.grid =  controls the number of grid boxes
##z scale = GAM model predictions 

```

Visualization of the interaction between Year and Day_Year in a contour plot.
```{r}
require(mgcv)

vis.gam(gam_modTIME, view=c("Day_Year","Year.Sampled"),plot.type="contour", color = "terrain", contour.col = "black") 

#The contour lines represent points of equal predicted values, and they are labeled. The dotted lines show uncertainty in prediction; they represent how contour lines would move if predictions were one standard error higher or lower.

#Run a model without an interaction to help understand the contour plot 
  #gam_modNI <- gam(Infection ~  s(Day_Year) + s(Year.Sampled, k = 8)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
  #save(gam_modNI, file="gam_modNI.Rdata" )
  #vis.gam(plog, view=c("Day_Year","Year.Sampled"),plot.type="contour", color = "terrain", contour.col = "yellow") 


devtools::install_github('gavinsimpson/gratia')
library('gratia')
mod <- gam(y ~ s(x, z, k = 30), data = dat$data, method = "REML")

mod <- gam(Infection ~   ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)


draw(gam_modII)



```


Summary of plot results: 

*Spatio-temporal Model Example* 

Joint models of space and time. 

Should an interaction between space and time be included? This is compared here and the interaction is found to be significant and produces a significantly lower AIC value compared to the non-interaction term model. 
```{r}
data$Year.Sampled<-as.numeric(data$Year.Sampled)


#First, run a model without an interaction between space and time as done in the example. By comparing this model to the model with a space-tie interaction, we will be able to determine the relative importance of the interaction term. 

STno = gam(Infection~ s(Longitude_city,Latitude_city) + s(Year.Sampled, k = 8) + s(Volunteer, bs="re"),
                          data=data,family=binomial,method="REML")

save(STno, file="STno.Rdata")
summary(STno)
plot(STno,scheme=2)


#Now run the model with the interaction term 

STyes = gam(Infection~ s(Longitude,Latitude) + s(Year.Sampled, k = 8)+ ti(Longitude, Latitude, Year.Sampled, d=c(2,1)) + s(Volunteer, bs="re"),
                          data=data,family=binomial,method="REML")
save(STyes, file="STyes.Rdata")
summary(STyes)
plot(STyes,scheme=2)

load("STyes.Rdata")
AIC(STyes)


```
WHile including the interaction term improves the fit of the model, it doesnt make it easy to determine where we are seeing 1) OE hotspots within each year 2) the greatest change in hotspots over time. 

We will next use the "predict" function from mgcv to better visualize the output of the STyes model. 


Graphing of space-time interactions 


We need to select a volunteer whose observations represent the average infection prevalence found by all volunteers. To do this, we will first determine the average infection prevalence per volunteer and then select the volunteer whose average is closest to this number. 
```{r}
#First use plyr to calculate the average per volunteer 
library(plyr)
r2<-ddply(data,.(Volunteer), summarize, Avg=mean(Infection))
r2
#We now have a table r2, with the average infection prevalence per volunteer for all 269 volunteers 
```

We can visualize the distribution of infection prevalence per volunteer. 
```{r}
library(ggplot2)
ggplot(data=r2, aes(x=as.factor(Volunteer), Avg)) + geom_point()+ geom_smooth()

hist(r2$Avg)
```
Next we summarize the r2 dataframe to determine the overall mean per volunteer. 
```{r}
summary(r2)

#The mean in 0.4681
```
Select the volunteers with an average closest to the average of 0.4681
```{r}

Average_Volunteers <- r2[r2$Avg >= 0.4 & r2$Avg < 0.5, ]
Average_Volunteers$Differ<-0.4681-Average_Volunteers$Avg
Average_Volunteers

#We find that 2 volunteers Ana Maria Agrusa (13) and Laninda Sande (154) have averages closest to the overall average (0.4615385), therefore we can use either volunteer in the model 
```


Create the background map on which the model predictions will be mapped 
```{r}
library("maps")
library("dplyr")
library("ggplot2")


states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

```

We can now move on to creating a grid of predicted infection prevalence using the average observation values of volunteer #13.  
```{r}
data$Latitude<-as.numeric(data$Latitude)
data$Longitude<-as.numeric(data$Longitude)

se_map$Latitude<-as.numeric(se_map$lat)
se_map$Longitude<-as.numeric(se_map$long)

data$Year.Sampled<-as.factor(data$Year.Sampled)

#First we'll create gridded data

predict_infection = expand.grid(
  Latitude= seq(min(data$Latitude), 
                max(data$Latitude),
                length=100),
  Longitude = seq(min(data$Longitude),
                  max(data$Longitude),
                  length=100),
  Volunteer = levels(data$Volunteer)[13],  
  Year.Sampled = seq(2011,2018,by=1))

#head(predict_infection)

#summary(data)
```

Create the Model Fit Column 
```{r}

predict_infection$model_fit = predict(STyes,
                                    predict_infection,type = "response")

```

Create a for loop to build polygons for each group in the dataset 
```{r}
library(ggplot2)
library(viridis)
library(dplyr)

se_map<-se_map %>% mutate(Longitude=long,Latitude=lat)
predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)

for(g in 1:length(groups))
{
  temp<-predict_infection[with(predict_infection %>% dplyr::select("Longitude","Latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("Longitude","Latitude"),Longitude,Latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}

library(mapproj)
library(maps)
library(ggrepel)

#Process for Making cities labels 
data(us.cities)
capitals <- subset(us.cities)
capitals$city <- sub(' [^ ]*$','',capitals$name) # split out city for the label

plotcities <- subset(world.cities, name %in% c("Atlanta", "Houston", "New Orleans", "Austin", "San Antonio", "Tampa", "Orlando", "Miami")) %>% filter(country.etc=="USA")


 
```

Graph the data
```{r}
gg<-ggplot(aes(Longitude, Latitude,  fill= model_fit), 
       data=predict_infection_clipped)+ #%>% filter(Year.Sampled==2011))+ 
  geom_tile()+
  facet_wrap(~Year.Sampled,nrow=4)+
  scale_fill_viridis("Infection Probability")+
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+
  
  # geom_point(x = -84.42, y = 33.76, colour="red", size = 1)+
  # annotate(geom = "text", x = -84.42, y = 33.5, label = "Atlanta", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -95.39, y = 29.77, colour="red", size = 1)+
  # annotate(geom = "text", x = -95.39, y = 29.5, label = "Houston", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -89.93, y = 30.07, colour="red", size = 1)+
  # annotate(geom = "text", x = -88.00, y = 30.1, label = "New Orleans", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -82.48, y = 27.96, colour="red", size = 1)+
  # annotate(geom = "text", x = -83.7, y = 27.88, label = "Tampa", 
  #   fontface = "bold", color = "white", size = 1)+

  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))

  g<-gg+ theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
g
  
save_plot("~/g.png", plot_tmp, base_height = NULL, base_aspect_ratio = 1.618, 
base_width = 6)

#library(OpenImageR)

#img<-OpenImageR::readImage("~/plot_tmp.png")
#imageShow(img)
 
```


Add space by day of year may be the next step in the analysis in order to see if infection changes in different regions through time
It seems like anytime I seem to know what I ma talking about she rears up 
We need to have a story with this. Especially from the spatiotemporal data. Think about story for publication 


Calculate the difference from year to year 
```{r}
predict_infection$model_change =predict(STyes,
                                      predict_infection%>%mutate(Year.Sampled=Year.Sampled+1),
                                      type = "response") - predict_infection$model_fit 



se_map<-se_map %>% mutate(Longitude=long,Latitude=lat)

predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)

for(g in 1:length(groups))
{
  temp<-predict_infection[with(predict_infection %>% dplyr::select("Longitude","Latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("Longitude","Latitude"),Longitude,Latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}
```

```{r}

library(RColorBrewer)

summary(predict_infection_clipped)
#mimimum change = -0.71, max= 0.94

#Labels for each facet 


ggplot(aes(Longitude, Latitude, fill= model_change),
       data=predict_infection_clipped%>% filter(Year.Sampled < 2018))+
  geom_tile()+
  facet_wrap(~ (Year.Sampled = c("2011-2012", "2012-2013", "2013-2014", "2014-2015", "2015-2016", "2016-2017", "2017-2018")),nrow=2)
  #scale_fill_gradient2("Infection Probability", low = "green", high = "red", na.value = NA, limits = c(-1, 1),    breaks = c(-1, -0.5, 0, 0.5, 1),
   #labels = c(-1, -0.5, 0, 0.5, 1))





gg<-ggplot(aes(Longitude, Latitude,  fill= model_change), 
       data=predict_infection_clipped %>% filter(Year.Sampled < 2018))+ 
  geom_tile()+
  facet_wrap(~Year.Sampled,nrow=4)+
   scale_fill_gradient2("Rate of change\n(species per year)")+
  #scale_fill_drsimonj("Infection Probability", palette = "mixed", guide = "none")+
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+ 
  theme_bw(10)+
  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))
  
  geom_point(x = -84.42, y = 33.76, colour="red", size = 2)+
  annotate(geom = "text", x = -84.42, y = 33.5, label = "Atlanta", 
    fontface = "bold", color = "white", size = 2)+
  
  geom_point(x = -95.39, y = 29.77, colour="red", size = 2)+
  annotate(geom = "text", x = -95.39, y = 29.5, label = "Houston", 
    fontface = "bold", color = "white", size = 2)+

  geom_point(x = -89.93, y = 30.07, colour="red", size = 2)+
  annotate(geom = "text", x = -88.00, y = 30.1, label = "New Orleans", 
    fontface = "bold", color = "white", size = 2)+
  
  geom_point(x = -82.48, y = 27.96, colour="red", size = 2)+
  annotate(geom = "text", x = -83.7, y = 27.88, label = "Tampa", 
    fontface = "bold", color = "white", size = 2)+

  

  gg+ theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
 gg     
```


Can we examine the relationship between season and location? 


```{r}


want <- seq(1, nrow(data), length.out = 200)
pdat <- with(data,
              data.frame(Day_Year = Day_Year[want], Year.Sampled = Year.Sampled[want],
                         Volunteer = Volunteer[want]))
 
 ## predict trend contributions
p  <- predict(gam_modII,  newdata = pdat, type = "terms", se.fit = TRUE)
p1 <- predict(m1$gam, newdata = pdat, type = "terms", se.fit = TRUE)
p2 <- predict(m2$gam, newdata = pdat, type = "terms", se.fit = TRUE)
p3 <- predict(m3$gam, newdata = pdat, type = "terms", se.fit = TRUE)

## combine with the predictions data, including fitted and SEs
pdat <- transform(pdat,
                   p  = p$fit[,2],  se  = p$se.fit[,2],
                   p1 = p1$fit[,2], se1 = p1$se.fit[,2],
                   p2 = p2$fit[,2], se2 = p2$se.fit[,2],
                   p3 = p3$fit[,2], se3 = p3$se.fit[,2])




op <- par(mar = c(5,4,2,2) + 0.1)
ylim <- with(pdat, range(p, p1, p2, p3))
> ylim[1] <- floor(ylim[1])
> ylim[2] <- ceiling(ylim[2])
> ylab <- expression(Temperature ~ (degree*C ~ centred))



plot(Infection - mean(Infection) ~ Day_Year, data = data, type = "n")
```





*Research Question 2*
How do the listed exogenous variables impact the spatial dynamics of OE infection prevalence change within year and between years in the sedentary population of monarch butterflies in the SE U.S.?
  
  1. Temperature  
  2. Precipitation 
  3. Monarch larval density 
  

*Environmental Variables Model* 
In the Brown et al. paper, they used the GAMs in the previous step to determine the end of season infection prevalence for each year. They then looked for "bivariate correlations" between these infection prevalences and different environmental variables. 

  *Examining the effect of freezing temperatures at 3 scales*
    1. How does the number of freezes per year impact the overall infection prevalence? 
    2. How does the number of freezes within each hardiness zone impact the overall infection prevalence?
    3. How does the number of freezes preceeding an observation impact infection? 
   


Import both the infection and weather data sets. 
The infection data includes an infection score for each observation. The weather data includes the weather data for each weather station within range of an observation for every day during the study period. 
```{r EV Models: Import Data}
###########################################################################Import the infection data 
library(readr)
MHD<- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/YR_MHD_weather_hardi2.csv")
MHD <- read_csv("./data/YR_MHD_weather_hardi2.csv")

 #Make sure date column is in correct format 
library(lubridate)
MHD$date<- parse_date_time(MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
MHD$date<-as.Date(MHD$date)

###########################################################################Import the weather data 

Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Freeze.csv")
# Weather_Hardi <- read_csv("./data/Weather_Freeze.csv")
# 
# #Modify the weather data to prepare it for analysis 
#   #Rename Columns to match MHD 
 library(dplyr)
Weather_Hardi<-Weather_Hardi %>% rename(Year = Year.Sampled,Station = StationName)
# 
 #Make sure date column is in correct format
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)
```



*Build Freeze Dataframe*
Code Days when *Freezes* Occurred as 1 in the Weather Data Set 
```{r EV Models: Create Daily Freeze Dataframe}
############################################
###########DAILY FREEZE DATA################
############################################

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week.
#Function for counting the number of freezes. Starting with identifying freeze days.
freeze.days<-function (TMIN, Day){
  freeze.days<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      freezes<-NA
    }else if (TMIN[i]>0){
      freezes<-0
    }else{
      freezes<-1
    }
    freeze.days<-c(freeze.days, freezes)
  }
  return(freeze.days)
}

#and now the freeze day counter
#this took my computer 1.5 minutes to run
Weather_Hardi$freeze.day<-freeze.days(Weather_Hardi$TMIN, Weather_Hardi$Day)
#Weather_Hardi$freeze.day<- Weather_Hardi$TMIN < 0

Weather_Hardi %>% group_by(freeze.day) %>% tally()
```

Next, the data need to go through a series of steps to arrange them for calculating the number of freezes inside of the desired window
```{r Preparing freeze data for 7 day window tabulation}
###############################################################
#Prepare data for the "roll apply"" function by arranging the stations as columns with date 
###############################################################
  
  #Create a data frame with only the id of the weather station, the date and the freeze code 
DailyFreezeSubset = subset(Weather_Hardi, select = c(id, date, freeze.day) )

  #Split the "id" column into 95 separate columns (one for each station)
library(tidyr)
#DFS<-DailyFreezeSubset %>% 
#   mutate(rn = row_number()) %>%
#   spread(id,  freeze.day) %>%
#   select(-rn)

DFS<-DailyFreezeSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=freeze.day) %>% arrange(date)

DFS %>% group_by(USC00083163) %>% tally()
#Each station should have over 2500 observations (note: this station is in Fort Lauderdale and should have no freezes)

```


*7 Day Freeze Window Calculations*
Count the number of *freezes* in the previous 7 days, 30 days and 60 days from when an observation was made 
```{r Count the number of freezes in the previous 7 days}
############################################
###########Apply RollApply Function#########
############################################

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
DFSroll<-rollapply(data = DFS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
DFSroll<-as.data.frame(DFSroll)

#Add date column back into
Freeze7 <- cbind(date = DFS$date,DFSroll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
Freeze7<-Freeze7 %>% pivot_longer(cols=2:ncol(Freeze7),names_to="id",values_to="freeze.day")

#Check to ensure that there is a range of freezes 
Freeze7 %>% group_by(freeze.day) %>% tally()

#Now Merge the freeze data with the monarch health observation data, allowing each monarch health observation to be paired with a corresponding weather observation including the summed window freeze data. 

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
Freeze7$id<-as.factor(Freeze7$id)

levels(Freeze7$id)
levels(MHD$id)

#Merge 
MHD_freeze<-left_join(MHD, Freeze7, by=c("date", "id"))
summary(MHD_freeze)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))

```


*30 Day Freeze Window*
```{r 30 Day Freeze Window Calculations}
DailyFreezeSubset = subset(Weather_Hardi, select = c(id, date, freeze.day) )
DFS<-DailyFreezeSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=freeze.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
DFSroll<-rollapply(data = DFS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
DFSroll<-as.data.frame(DFSroll)

#Add date column back into
Freeze7 <- cbind(date = DFS$date,DFSroll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
Freeze7<-Freeze7 %>% pivot_longer(cols=2:ncol(Freeze7),names_to="id",values_to="freeze.day")

#Check to ensure that there is a range of freezes 
Freeze7 %>% group_by(freeze.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
Freeze7$id<-as.factor(Freeze7$id)

levels(Freeze7$id)
levels(MHD$id)

#Merge 
MHD_freeze30<-left_join(MHD, Freeze7, by=c("date", "id"))
summary(MHD_freeze30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```



*Developmental Zero Calculations* 

```{r}
###########################################
##########DAILY DevZero DATA###############
###########################################
#Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Hardi.csv")
Weather_Hardi <- read_csv("./data/Weather_Hardi.csv")

#Modify the weather data to prepare it for analysis
  #Rename Columns to match MHD
library(dplyr)
Weather_Hardi<-Weather_Hardi %>%
  rename(Year = Year.Sampled,
    Station = StationName)

 #Make sure date column is in correct format
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)
#Create the week variable
Weather_Hardi$week <- week(Weather_Hardi$date)

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week.
#Function for counting the number of freezes. Starting with identifying freeze days.
DevZero.days<-function (TMIN, Day){
  DevZero.days<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      devzero<-NA
    }else if (TMIN[i]>10){
      devzero<-0
    }else{
      devzero<-1
    }
    DevZero.days<-c(DevZero.days, devzero)
  }
  return(DevZero.days)
}

#and now the dev zero day counter
Weather_Hardi$devzero.day<-DevZero.days(Weather_Hardi$TMIN, Weather_Hardi$Day)

#Examine the number of dev zero days to ensure a reasonable number have been recorded
Weather_Hardi %>% group_by(devzero.day) %>% tally()
```


*Dev Zero 7 Day Calculations*
```{r Dev Zero 7 Day Calculations}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
DailyDevZeroSubset = subset(Weather_Hardi, select = c(id, date, devzero.day) )
DZS<-DailyDevZeroSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=devzero.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
DevZeroData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
DevZero_7DayData<-DevZeroData %>% pivot_longer(cols=2:ncol(DevZeroData),names_to="id",values_to="devzero.day")

#Check to ensure that there is a range of freezes 
DevZero_7DayData %>% group_by(devzero.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
DevZeroData$id<-as.factor(DevZeroData$id)

levels(DevZeroData$id)
levels(MHD$id)

#Merge 
DevZeroData7<-left_join(MHD, DevZero_7DayData, by=c("date", "id"))
summary(DevZeroData30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```

*Dev Zero 30 Day Calculations*
```{r}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
DailyDevZeroSubset = subset(Weather_Hardi, select = c(id, date, devzero.day) )
DZS<-DailyDevZeroSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=devzero.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
DevZeroData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
DevZeroData<-DevZeroData %>% pivot_longer(cols=2:ncol(DevZeroData),names_to="id",values_to="devzero.day")

#Check to ensure that there is a range of freezes 
DevZeroData %>% group_by(devzero.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
DevZeroData$id<-as.factor(DevZeroData$id)

levels(DevZeroData$id)
levels(MHD$id)

#Merge 
DevZeroData30<-left_join(MHD, DevZeroData, by=c("date", "id"))
summary(DevZeroData30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```


*TMIN Calculations*
```{r TMIN 7 Day Window Calculations}
library(tidyr)
TMINSubset = subset(Weather_Hardi, select = c(id, date, TMIN) )
TMINS<-TMINSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=TMIN) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = TMINS [,c(2:96)],width = 7, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
TMINData <- cbind(date = TMINS$date,TRoll )

#Turn station ID back into a single column for model analysis   
TMINData<-TMINData %>% pivot_longer(cols=2:ncol(TMINData),names_to="id",values_to="TMIN.7day")

#Check to ensure that there is a range of freezes 
TMINData %>% group_by(TMIN.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMINData$id<-as.factor(TMINData$id)

#Merge 
TMINData7<-left_join(MHD, TMINData, by=c("date", "id"))
summary(TMINData7)
```

*TMIN 30 Day Window Calculations*
```{r TMIN 30 Day Window Calculations}
library(tidyr)
TMINSubset = subset(Weather_Hardi, select = c(id, date, TMIN) )
TMINS<-TMINSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=TMIN) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = TMINS [,c(2:96)],width = 30, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
TMINData <- cbind(date = TMINS$date,TRoll )

#Turn station ID back into a single column for model analysis   
TMINData<-TMINData %>% pivot_longer(cols=2:ncol(TMINData),names_to="id",values_to="TMIN.30day")

#Check to ensure that there is a range of freezes 
TMINData %>% group_by(TMIN.30day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMINData$id<-as.factor(TMINData$id)
TMINData$date<-as.Date(TMINData$date)


#Merge 
TMINData30<-left_join(MHD, TMINData, by=c("date", "id"))
summary(TMINData30)
```



*TMAX 7 Day Window Calculations*
```{r TMAX 7 Day Window Calculations}
library(tidyr)
library(dplyr)
TMAXSubset = subset(Weather_Hardi, select = c(id, date, TMAX) )
TMAXS<-TMAXSubset %>% pivot_wider(id_cols=date,names_from = id,values_from= TMAX) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TMRoll<-rollapply(data = TMAXS [,c(2:96)],width = 7, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TMRoll<-as.data.frame(TMRoll)

#Add date column back into
TMAXData <- cbind(date = TMAXS$date,TMRoll )

#Turn station ID back into a single column for model analysis   
TMAXData<-TMAXData %>% pivot_longer(cols=2:ncol(TMAXData),names_to="id",values_to="TMAX.7day")

#Check to ensure that there is a range of freezes 
TMAXData %>% group_by(TMAX.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
TMAXData$id<-as.factor(TMAXData$id)
TMAXData$date<-as.Date(TMAXData$date)

#Merge 
TMAXData7<-left_join(MHD, TMAXData, by=c("date", "id"))
summary(TMAXData7)
```


*TMAX 30 Day Window Calculations*
```{r TMAX 30 Day Window Calculations}
library(tidyr)
TMAXSubset = subset(Weather_Hardi, select = c(id, date, TMAX) )
TMAXS<-TMAXSubset %>% pivot_wider(id_cols=date,names_from = id,values_from= TMAX) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TMRoll<-rollapply(data = TMAXS [,c(2:96)],width = 30, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TMRoll<-as.data.frame(TMRoll)

#Add date column back into
TMAXData <- cbind(date = TMAXS$date,TMRoll )

#Turn station ID back into a single column for model analysis   
TMAXData<-TMAXData %>% pivot_longer(cols=2:ncol(TMAXData),names_to="id",values_to="TMAX.30day")

#Check to ensure that there is a range of freezes 
TMAXData %>% group_by(TMAX.30day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMAXData$id<-as.factor(TMAXData$id)

#Merge 
TMAXData30<-left_join(MHD, TMAXData, by=c("date", "id"))
summary(TMAXData30)
```



*PRCP 7 Day Average Window Calculations*
```{r PRCP 7 Day Window Calculations}
library(tidyr)
library(dplyr)
PRCPSubset = subset(Weather_Hardi, select = c(id, date, PRCP) )
PRCPS<-PRCPSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=PRCP) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = PRCPS [,c(2:96)],width = 7, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
PRCPData <- cbind(date = PRCPS$date,TRoll )

#Turn station ID back into a single column for model analysis   
PRCPData<-PRCPData %>% pivot_longer(cols=2:ncol(PRCPData),names_to="id",values_to="PRCP.7day")

#Check to ensure that there is a range of freezes 
PRCPData %>% group_by(PRCP.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
PRCPData$id<-as.factor(PRCPData$id)
PRCPData$date<-as.Date(PRCPData$date)


#Merge 
PRCPData7<-left_join(MHD, PRCPData, by=c("date", "id"))
summary(TMINData7)
```

*PRCP 7 Day Additive Window Calculations*
```{r PRCP 7 Day Additive Window Calculations}
library(tidyr)
library(dplyr)
PRCPSubset = subset(Weather_Hardi, select = c(id, date, PRCP) )
PRCPS<-PRCPSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=PRCP) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TaRoll<-rollapply(data = PRCPS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TaRoll<-as.data.frame(TRoll)

#Add date column back into
PRCPData <- cbind(date = PRCPS$date,TaRoll )

#Turn station ID back into a single column for model analysis   
PRCPData<-PRCPData %>% pivot_longer(cols=2:ncol(PRCPData),names_to="id",values_to="PRCP.7day")

#Check to ensure that there is a range of freezes 
PRCPData %>% group_by(PRCP.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
PRCPData$id<-as.factor(PRCPData$id)
PRCPData$date<-as.Date(PRCPData$date)


#Merge 
PRCPData7add<-left_join(MHD, PRCPData, by=c("date", "id"))
summary(TMINData7)
```


*PRCP 30 Day Average Window Calculations*
```{r PRCP 30 Day Average Window Calculations}
library(tidyr)
library(dplyr)
PRCPSubset = subset(Weather_Hardi, select = c(id, date, PRCP) )
PRCPS<-PRCPSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=PRCP) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TaRoll2<-rollapply(data = PRCPS [,c(2:96)],width = 30, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TaRoll2<-as.data.frame(TaRoll2)

#Add date column back into
PRCPData2 <- cbind(date = PRCPS$date,TaRoll2 )

#Turn station ID back into a single column for model analysis   
PRCPData2<-PRCPData2 %>% pivot_longer(cols=2:ncol(PRCPData2),names_to="id",values_to="PRCP.30day")

#Check to ensure that there is a range of freezes 
PRCPData2 %>% group_by(PRCP.30day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
PRCPData2$id<-as.factor(PRCPData2$id)
PRCPData2$date<-as.Date(PRCPData2$date)


#Merge 
PRCPData30<-left_join(MHD, PRCPData2, by=c("date", "id"))
summary(TMINData7)
```


****************GAM Models******************* 

*Base GAM Model* 
The GAM model without Environmental Variables 
```{r Base GAM Model}
library(mgcv)

###################Base###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
O_gam <-mgcv:: gam(Infection ~ s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(O_gam)
AIC(O_gam)
#14165.96
plot(O_gam, pages = 1,  trans = plogis,
     shift = coef(O_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

*FREEZE GAM Models* 
Freeze 7 Day Average GAM
```{r Freeze 7 Day Average GAM}
###################FREEZE 7 Day Average##################################

MHD_freeze$Volunteer<-as.factor(MHD_freeze$Volunteer)
F7_gam <-mgcv:: gam(Infection ~ s(freeze.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = MHD_freeze)
summary(F7_gam)
AIC(F7_gam)
#12523.63
plot(F7_gam, pages = 1,  trans = plogis,
     shift = coef(F7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


Freeze 30 Day Average GAM
```{r Freeze 30 Day Average GAM}

###################FREEZE 30 Day Average##################################
library(mgcv)
MHD_freeze30$Volunteer<-as.factor(MHD_freeze30$Volunteer)
F30_gam <-mgcv:: gam(Infection ~ s(freeze.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = MHD_freeze30)
summary(F30_gam)
AIC(F30_gam)
#12529.84
plot(F30_gam, pages = 1,  trans = plogis,
     shift = coef(F30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```


*DevZero GAM Models* 
DevZero 7 Day Average GAM
```{r DevZero 7 Day Average GAM}

###################DevZero 7 Day Average##################################
library(mgcv)
DevZeroData7$Volunteer<-as.factor(DevZeroData7$Volunteer)
Z7_gam <-mgcv:: gam(Infection ~ s(devzero.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = DevZeroData7)
summary(Z7_gam)
AIC(Z7_gam)
#12527.52
plot(Z7_gam, pages = 1,  trans = plogis,
     shift = coef(Z7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```

DevZero 30 Day Average GAM
```{r DevZero 30 Day Average GAM}

###################DevZero 30 Day Average##################################
library(mgcv)
DevZeroData30$Volunteer<-as.factor(DevZeroData30$Volunteer)
Z30_gam <-mgcv:: gam(Infection ~ s(devzero.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = DevZeroData30)
summary(Z30_gam)
AIC(Z30_gam)
#12499.83
plot(Z30_gam, pages = 1,  trans = plogis,
     shift = coef(Z30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```

*TMIN GAM Models* 
TMIN Raw GAM
```{r TMIN Raw GAM}
library(mgcv)

###################TMIN RAW###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
T_gam <-mgcv:: gam(Infection ~ s(TMIN2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(T_gam)
AIC(T_gam)
#13429.8
plot(T_gam, pages = 1,  trans = plogis,
     shift = coef(T_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMIN 7 Day Average GAM
```{r TMIN 7 Day GAM}
###################TMIN 7 Day Average##################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
T7_gam <-mgcv:: gam(Infection ~ s(TMIN.7day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(T7_gam)
AIC(T7_gam)
#12114.99
plot(T7_gam, pages = 1,  trans = plogis,
     shift = coef(T7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMIN 30 Day Average GAM
```{r TMIN 30 Day GAM}
###################TMIN 30 Day Average##################################
TMINData30$Volunteer<-as.factor(TMINData30$Volunteer)
T30_gam <-mgcv:: gam(Infection ~ s(TMIN.30day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData30)
summary(T30_gam)
AIC(T30_gam)
#12204.9
plot(T30_gam, pages = 1,  trans = plogis,
     shift = coef(T30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


*TMAX Raw GAM*
```{r TMAX Raw GAM}

###################TMIN RAW###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
Max_gam <-mgcv:: gam(Infection ~ s(TMAX2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(Max_gam)
AIC(Max_gam)
#13372.5
plot(Max_gam, pages = 1,  trans = plogis,
     shift = coef(Max_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMAX 7 Day Average GAM
```{r TMAX 7 Day GAM}
###################TMIN 7 Day Average##################################
TMAXData7$Volunteer<-as.factor(TMAXData7$Volunteer)
TM7_gam <-mgcv:: gam(Infection ~ s(TMAX.7day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMAXData7)
summary(TM7_gam)
AIC(TM7_gam)
#12058.9
plot(TM7_gam, pages = 1,  trans = plogis,
     shift = coef(T7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMAX 30 Day Average GAM
```{r TMAX 30 Day GAM}
###################TMIN 7 Day Average##################################
TMAXData30$Volunteer<-as.factor(TMAXData30$Volunteer)
TM30_gam <-mgcv:: gam(Infection ~ s(TMAX.30day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMAXData30)
summary(TM30_gam)
AIC(TM30_gam)
# 12217.76
plot(TM30_gam, pages = 1,  trans = plogis,
     shift = coef(T7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


*Precipitation GAM Models*
PRCP Raw GAM
```{r PRCP Raw GAM}
library(mgcv)
write.csv(PRCPData7, "PRCPData7.csv")
save(PRCPData7, file = "PRCPData7.RData")
###################PRCP RAW###########################################
PRCPData7$Volunteer<-as.factor(PRCPData7$Volunteer)
PRCPData7$PRCP2<-as.numeric(PRCPData7$PRCP2)

P_gam <-mgcv:: gam(Infection ~ s(PRCP2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)

summary(P_gam)
AIC(P_gam)
#12372.3
plot(P_gam, pages = 1,  trans = plogis,
     shift = coef(P_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


PRCP 7 Day AVerage Model 
```{r PRCP 7 Day GAM}
library(mgcv)

###################TMIN RAW###########################################
PRCPData7$Volunteer<-as.factor(PRCPData7$Volunteer)
PRCPData7$PRCP.7day<-PRCPData7$PRCP.7day/10



P_gam7 <-mgcv:: gam(Infection ~ s(PRCP.7day) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)
summary(P_gam7)
AIC(P_gam7)
#10968.38

#Filtering High Values#
PRCPData7<- filter(PRCPData7, PRCP.7day < "40")

#filtered 9041.548
plot(P_gam7, pages = 1,  trans = plogis,
     shift = coef(P_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


PRCP 7 Day Summed model 
```{r PRCP 7 Day GAM}
library(mgcv)

###################PRCP 7 Day Additive Model ###########################################
PRCPData7add$Volunteer<-as.factor(PRCPData7add$Volunteer)
PRCPData7add$PRCP.7day<-PRCPData7add$PRCP.7day/10



P_gam7add <-mgcv:: gam(Infection ~ s(PRCP.7day) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7add)
summary(P_gam7add)
AIC(P_gam7add)
#12519.95

plot(P_gam7add, pages = 1,  trans = plogis,  shift = coef(P_gam7add)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

####################ReRun Model with a Filter on Samples ######################
#Filtering High Values#
PRCPData7<- filter(PRCPData7, PRCP.7day < "40")

#filtered 9041.548
plot(P_gam7, pages = 1,  trans = plogis,shift = coef(P_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
    
```

PRCP 30 Day Average Model 
```{r PRCP 30 Day Average GAM}
library(mgcv)
#For cluster 
save(PRCPData30, file = "PRCPData30.RData")
load(file= "P_gam30.RData")
##################PRCP 30 Day Average Model ###########################################
PRCPData30$Volunteer<-as.factor(PRCPData30$Volunteer)

P_gam30 <-mgcv:: gam(Infection ~ s(PRCP.30day) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData30)
summary(P_gam30)
AIC(P_gam30)
#10968.38

plot(P_gam30, pages = 1,  trans = plogis,  shift = coef(P_gam30)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```


Additive Models 
```{r PRCP + TMAX Raw GAM}
library(mgcv)

#For cluster
load(file ="PRCPData7.RData")
load(file= "TP_gam.RData")
###################PRCP + TMAX Raw GAM###########################################
PRCPData7$Volunteer<-as.factor(PRCPData7$Volunteer)
PRCPData7$PRCP2<-as.numeric(PRCPData7$PRCP2)



TP_gam <-mgcv:: gam(Infection ~ s(PRCP2) + s(TMAX2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)
summary(TP_gam)
AIC(TP_gam)
#
plot(TP_gam, pages = 1,  trans = plogis,  shift = coef(TP_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

```{r PRCP + TMAX Average GAM}
library(mgcv)

#Create dataframe with merged averaged 7 day values of PRCP and TMIN 
#Had major problems getting my join to work. Found this Explanation of the code below (that seems to work) online. 
  #I add a row number within each group of dates, so that only the first June from df_2 will be joined to the first June entry from       df_1.
PXT_7day<-left_join(PRCPData7 %>% group_by(date) %>% mutate(rowID = row_number()),
          TMAXData7 %>% group_by(date) %>% mutate(rowID = row_number()), 
          by = c("date", "rowID"))

#For cluster 
save(TPa_gam, file = "TPa_gam.RData")
load(file ="PXT_7day.RData")
load(file= "TPa_gam.RData")

###################PRCP + TMIN 7 Day Average GAM###########################################
PXT_7day$Volunteer.x<-as.factor(PXT_7day$Volunteer.x)

TPa_gam <-mgcv:: gam(Infection.x ~ s(PRCP.7day) + s(TMAX.7day) + s(Year.x, k= 5) + s(longitude.x,latitude.x) + s(Volunteer.x,bs="re") , family = binomial, link = logit, data = PXT_7day)
summary(TPa_gam)
AIC(TPa_gam)
#10836.26
plot(TPa_gam, pages = 1,  trans = plogis,  shift = coef(TPa_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

Interaction Models 
```{r PRCP X MAX Raw GAM}
library(mgcv)
#Create dataframe with merged averaged 7 day values of PRCP and TMIN 
  #completed in previous section 
###################PRCP X MAX Raw GAM###########################################

TXPraw_gam <-mgcv:: gam(Infection ~ s(PRCP2) + s(TMAX2) + ti(PRCP, TMAX2)+ s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)
summary(TXPraw_gam)
AIC(TXPraw_gam)
#
plot(TXPraw_gam, pages = 1,  trans = plogis,  shift = coef(TXPraw_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

vis.gam(TXPraw_gam, view=c("TMAX2", "PRCP2"),plot.type="contour", color = "terrain", contour.col = "black") 
```

```{r PRCP X TMAX 7 Day Average GAM}
library(mgcv)

###################PRCP X TMAX 7 Day Average GAM###########################################
PXT_7day$Volunteer.x<-as.factor(PXT_7day$Volunteer.x)

TXPa_gam <-mgcv:: gam(Infection.x ~ s(PRCP.7day) + s(TMAX.7day) + ti(PRCP.7day, TMAX.7day)+ s(Year.x, k= 5) + s(longitude.x,latitude.x) + s(Volunteer.x,bs="re") , family = binomial, link = logit, data = PXT_7day)
summary(TXPa_gam)
AIC(TXPa_gam)
#10813.13
plot(TXPa_gam, pages = 1,  trans = plogis,  shift = coef(TXPa_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

vis.gam(TXPa_gam, view=c("TMAX.7day", "PRCP.7day"),plot.type="contour", color = "terrain", contour.col = "black") 


```





#Checking the Data to Gain a Better Understanding of the Observed Patterns#




Graphing Extreme Values 
```{r}
#According to the interaction figure extreme values where an effect is observed are as follows:
  #PRCP >100
  #TMAX >30 
  #TMAX <15 

TMAXfiltered<- filter(PXT_7day, TMAX.7day > "30", PRCP.7day > "20")
TMAXfiltered<- filter(TMAXfiltered,  PRCP.7day < "50")
TMAXfiltered<- filter(TMAXfiltered,  Infection.x=="1")


5283 just TMAX 
491 plus PRCP 

, Infection.x=="1"
write.csv(TMAXfiltered, file="ColdWetALL.csv")

TMAXfiltered %>% group_by(Sex) %>% tally()
TMAXfiltered %>% group_by(State ) %>% tally()

TMAXfiltered<- filter(PXT_7day, TMAX.7day > "30", PRCP.7day > "30", Infection.x=="1")

write.csv(TMAXfiltered, file="HotWet.csv")

#Figure out average infection prevalence by state 
= 84% in Sept 
= 66% in Oct 

= 78% in November 
= 63% in December 
= 75% in January 

= 90% in May 
= 88% in June 
= 81% in July 
= 89% in August 





TMAXfiltered<- filter(PXT_7day, State.x=="FL", Month.x=="10")
TMAXfiltered<- filter(TMAXfiltered,  PRCP.7day < "50")
TMAXfiltered<- filter(TMAXfiltered,  Infection.x=="1")
TMAXfiltered %>% group_by(Infection.x) %>% tally()

```


Graph the spatial distribution of the data.  
```{r}
library("maps")
library("dplyr")
library("ggplot2")

states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

gg<-ggplot() + 
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group), fill="grey", colour = "white")+
   geom_point(data=data, aes(x=Longitude, y=Latitude, color = factor(Infection)), position = "jitter", size=1)+
  scale_colour_manual(values = c("1"="chocolate3", "0"= "cyan"))+
 
 # geom_point(data=data, aes(x=Longitude, y=Latitude),size=1, colour = "black")
  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))+
   xlab("Longitude") + ylab("Latitude")

  gg + theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
```
































