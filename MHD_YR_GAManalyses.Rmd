---
title: "GAM_MHD"
author: "Christen"
date: "March 25, 2020"
output:
  html_document:
    df_print: paged
---

GAM Work with Monarch Health Data



*Overall Research Objectives*
1.	TEMPORAL: Determine how OE infection prevalence changes seasonally (within a year) and across years in the southeastern U.S. 

2.	SPATIAL: Determine where OE infection prevalence is most prevalent within each year and how the spatial patterns of infection have changed over time? 

3.	ENVIRONMENTAL: Determine whether climatic variables are correlated with the spatial and temporal changes in OE infection prevalence in the southeastern U.S.

*Why use a GAM?*
We do not expect the patterns in disease prevalence as described by our predictor variables to be linear. For example, infection prevalence may experience increases during periods of warm weather such as in late fall and early spring, while mid winter low temperatures may result in a decline in infection prevalence as population density and therefore transmission potential declines. Graphs of the dataset are provided below for visual inspection of the linear or non-linear trends.  

*Dataset*
```{r MHD Dataset}
#data<- read.csv("./data/YR_MHD_weather_hardi2.csv")
data<- read.csv("YR_MHD_weather_hardi2.csv")
data<- YR_MHD_weather_hardi2
summary (data)
```


*Exploration of the Data through Graphing*
Checking the distributions of variables to be used for analysis and identifying outliers. 
```{r Graphical Data Exploration}
PRCP2 <- data$PRCP2
PRCP2<-as.numeric(PRCP2)
hist(PRCP2)
dotchart(data$PRCP2)
#Outliers above 100

TMAX2 <- data$TMAX2
TMAX2<-as.numeric(TMAX2)
boxplot(TMAX2)
dotchart(data$TMAX2)
#Above 40 and below 5 

TMIN2 <- data$TMIN2
TMIN2<-as.numeric(TMIN2)
boxplot(TMIN2)
#Low temp outliers 

latitude <- data$latitude
hist(latitude)
#No outliers 

longitude <- data$longitude
hist(longitude)
#No outliers

```
*Identification and Removal of Outliers*
According to Zuur 2010, outliers may cause overdispersion of the data when using a GLM for analysis. Boxplots are typically used for boxplot exploration. The median is hsows as the horizontal line with the 25% and 75% quartiles forming a box around it. Any point found beyond the whiskers extending from the boxes are outliers. 

Using 95% confidence intervals to identify and remove outliers. 

-Removing outliers produced models that produced a weaker fit to the data and increased variability. FOr now, the outliers will be left in the model. I will also add that even at 99.8% confidence interval, thousands of observations were dropped when trimming out the outliers. 

```{r Removal of Outliers}
# data$PRCP2<-as.numeric(data$PRCP2)
# 
# lower_bound <- quantile(data$PRCP2, 0.01, na.rm = TRUE)
# lower_bound
# 
# upper_bound <- quantile(data$PRCP2, 0.997, na.rm = TRUE)
# upper_bound
# 
# data2<- dplyr::filter(data, PRCP2 < "200")
# data3<- dplyr::filter(data2, TMAX2 < "5")
# data4<- dplyr::filter(data3, TMAX2 < "40")
# 
# 
# #According to this method, all observations below 0 and above 47.8 will be considered as potential outliers. The row numbers of the observations outside of the interval can then be extracted with the which() function:
# 
# outlier_ind <- which(data$PRCP2 < lower_bound | data$PRCP2 > upper_bound)
# outlier_ind
# 
# data[outlier_ind, "PRCP2"]
# #148 PRCP2 outliers are identified 
# 
# #Code modified from: https://www.statsandr.com/blog/outliers-detection-in-r/#descriptive-statistics

```


*Testing weather variables for correlation* 
-This plot showed that, as we would expect, there is correlation between TMIN and TMAX. However there is no correlation between the temperature variables and precipitation. 
```{r Correlation Table}
# Examine correlations between variables 
pairs(data[20:24])
```

*Graph variables of interest against response variable (infection)*
Graph infection and day of year to visually inspect the relationship for linearity. 
- The relationship between day of year and infection appears to be non-linear. There seems to be a decrease in infection prevalence in late summer/early fall. #Note: Day 200 is mid-July 
```{r Graphical Exploration: Day of Year and Infection}
#Convert variables into different forms for graphing purposes
data$Day_Year<-as.factor(data$Day_Year)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
library(plyr)
r2<-ddply(data,.(Day_Year), summarize, InfectionMean=mean(Infection))
r2$InfectionMean <- InfectionMean-1

library(ggplot2)
ggplot(data=r2, aes(x=as.numeric(Day_Year), InfectionMean))+geom_point() + geom_smooth()
```

Graph the relationship between infection and year to visually inspect the relationship for linearity. 
```{r Graphical Exploration: Year and Infection}
#Convert variables into different forms for graphing purposes
##data$Season<-revalue(data$Season, c("11_12"="1", "12_13"="2", "13_14"="3", "14_15"="4", "15_16"="5", "16_17"="6", "17_18"="7"))
data$Year<-as.factor(data$Year)
data$Infection<-as.numeric(data$Infection)


#Calculate the probability of infection on each day of year 
library(plyr)
s1<-ddply(data,.(Year), summarize, InfectionMean=mean(Infection-1))

#Plot 
ggplot(data=s1, aes(x=as.numeric(Year), InfectionMean))+geom_point() + geom_smooth()
```

Graph the relationship between latitude and year to visually inspect the relationship for linearity. 
-There is a dramatic effect of latitude. Latitudes between 0-100 have high infection prevalence compared to higher latitudes. 

```{r Graphical Exploration: Latitude and Infection}
#Convert variables into different forms for graphing purposes
data$latitude<-as.factor(data$latitude)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
library(plyr)
r2<-ddply(data,.(latitude), summarize, InfectionMean=mean(Infection-1))

library(ggplot2)
ggplot(data=r2, aes(x=as.numeric(latitude), InfectionMean))+geom_point() + geom_smooth()
```


Graph the relationship between longitude and infection prevalence to visually inspect the relationship for linearity. 
-There is no obvious relationship between longitude and infection. 

```{r Graphical Exploration: Longitude and Infection}
#Convert variables into different forms for graphing purposes
data$longitude<-as.factor(data$longitude)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
r2<-ddply(data,.(longitude), summarize, InfectionMean=mean(Infection-1))

ggplot(data=r2, aes(x=as.numeric(longitude), InfectionMean))+geom_point() +  scale_x_discrete(labels = c(-80))+ theme(axis.text.x = element_text(angle = 90)) + geom_smooth()
```

Graph the relationship between precipitation and infection prevalence to visually inspect the relationship for linearity. 
-There is no obvious relationship between precip and infection prevalence. 

```{r Graphical Exploration: Precip and Infection}
#Convert variables into different forms for graphing purposes
data$PRCP2<-as.factor(data$PRCP2)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
library(plyr)
r2<-ddply(data,.(PRCP2), summarize, InfectionMean=mean(Infection-1))

library(ggplot2)
ggplot(data=r2, aes(x=as.numeric(PRCP2), InfectionMean))+geom_point() + geom_smooth()
```
Graph the relationship between TMAX and infection prevalence to visually inspect the relationship for linearity. 
-There is a parabolic relationship between TMAX and infection. 

```{r Graphical Exploration: TMAX and Infection}
#Convert variables into different forms for graphing purposes
data$TMAX2<-as.factor(data$TMAX2)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
r2<-ddply(data,.(TMAX2), summarize, InfectionMean=mean(Infection-1))

ggplot(data=r2, aes(x=as.numeric(TMAX2), InfectionMean))+geom_point() + geom_smooth()
```

Graph the relationship between TMIN and infection prevalence to visually inspect the relationship for linearity. 
-There is a parabolic relationship between TMIN and infection. 

```{r Graphical Exploration: TMIN and Infection}
#Convert variables into different forms for graphing purposes
data$TMIN2<-as.factor(data$TMIN2)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
r2<-ddply(data,.(TMIN2), summarize, InfectionMean=mean(Infection-1))

ggplot(data=r2, aes(x=as.numeric(TMIN2), InfectionMean))+geom_point() + geom_smooth()
```








Graph the spatial distribution of the data.  
```{r}
library("maps")
library("dplyr")
library("ggplot2")

states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))


data$Infection<-as.factor(data$Infection)
gg<-ggplot() + 
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group), fill="grey", colour = "white")+
   geom_point(data=data, aes(x=as.numeric(longitude), y=as.numeric(latitude), colour = factor(Infection)), position = "jitter", size=1)+ scale_colour_manual(values = c("1"="chocolate3", "0"= "cyan"))+
   coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))+
   xlab("Longitude") + ylab("Latitude") +
  gg + theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
gg
```


*Objective 1: Analyzing the Temporal Patterns in OE Infection Prevalence in the SE*
  
  *Hypothesis 1: Within Year*
If OE infection prevalence is primarily driven by reproductive behavior and breeding is  occurring year-round in the southeast, then  OE infection prevalence may remain high, showing little correlation with day of year. 

  *Hypothesis 2: Between Years*
  If the population of winterbreeding monarchs in the southeastern US is growing (and therefore becoming more dense), we expect that OE infection prevalence may increase with year as increased monarch density leads to higher rates of transmission.
  
*Model Construction*
This model is designed to characterize temporal infection patterns in the southeast by including day of year, year and an interaction of these two terms in a single model. 

Details on the model:
1. Why "ti"?
  Tensor product smooths are used to model interactions between variables that have different natural scales. They multiply the smooth terms of one variable by the factor levels of the other. 

2. Why "by"?
  "By" variables are used for constructing "varying coefficient models" and for letting smooths interact with parametric terms. In this case our smooth is the Day_Year variable and the Season term is the parametric term (and a factor). 
The interaction between two parametric terms would use the : or * connotation, but smooth interactions cannot. "By" generates an indicator vector for each level of a factor  unless the factor is ordered. If it is ordered then a different smooth is generated for each fator level (except the first level). 


3. Why k=6? 
The term k is also referred to as "knots". These are the natural breaks in the basis functions that make up the "basis" or "smooth function" or "spline". Where the knots break along the spline is usually defined by quantiles. K is usually the max number of degrees of freedom allowed for a smooth term in the model. K in the models will always =  (max degrees of freedom - 1). In this case 

4. Why "family=binomial"? 
The response variable is either 0 (uninfected) or 1 (infected), this we have 2 curves for our response variable distribution and this requires that the model be fit with a binomial family distribution. 

5. Why is "Season" parametric?


6. Why is Volunteer "bs=re"? 
In order to include Volunteer as a random effect, we tell the model to use the random effect basis function to model this variable. This method only applies to mcgv, as a different method is used for gamm. 

7. Why REML? 

8. Should I be using a different basis function? Cubic? 

```{r eval, = FALSE if exists (gam_modTIME.Rdata) }
require(mgcv)

###Make sure each variable is in the correct format
data$Day_Year<-as.numeric(data$Day_Year)
data$Year<-as.numeric(data$Year)
data$Infection<-as.factor(data$Infection)
data$Volunteer<-as.factor(data$Volunteer)


gam_modTIME <- gam(Infection.Status ~  s(Day_Year, bs = "cc") + s(Year,  k = 8) + ti(Day_Year, bs = "cc", by = Year, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

save(gam_modTIME, file="gam_modTIME.Rdata")

summary(gam_modTIME)

```


Key points from model output: 

The model output summary tells us that the additive temporal variables explain 41.6% of the deviance.We also can convert the outputs from the model to actual probabilities because the GAM model uses a log-odds scale for estimating outputs. After converting the intercept estimate below we find that the model predicts a 47.8% chance of infection overall. 
```{r}
plogis(-0.08493)
```


*Plot the results of the temporal model* 
Plots of the partial effects of the interaction fo Day_Year and Year. 

```{r}

#Year.Sampled = c("2011-2012", "2012-2013", "2013-2014", "2014-2015", "2015-2016", "2016-2017", "2017-2018")
#axis(, at=1:8, xaxp=letters[2011:2018])

#provides partial effects plots for individual terms in the model 
require(mgcv)
plot(gam_modTIME, pages = 1,  trans = plogis,
     shift = coef(gam_modTIME)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

#provides contour plot for interaction term in the model 
vis.gam(gam_modTIME, n.grid = 50, theta = 400, phi = 20, ylab = "Day of Year", xlab = "Year", zlab = "",
        plot.type =  "contour", color = "terrain", main = "Interaction between day of year and season")

#Using ggplot to control details of contour plot 
 Time_pred <- expand_grid(
  Day_Year = seq(from=1, 
              to=365, length.out = 300),
  Year = seq(from=min(data$Year), 
              to=max(data$Year), 
              length.out = 35), 
   Volunteer = 1, 
              length.out = 1)
  

Time_pred <- predict(gam_modTIME, newdata = Time_pred, 
                      se.fit = TRUE) %>%  
  as_tibble() %>% 
  cbind(Time_pred)

dim(Time_pred)   

#Turn log-odds ration into probability 
Time_pred$fit<-plogis(Time_pred$fit)

#Determine the min/max of the fit column 
summary(Time_pred$fit)

#Graph using ggplot 
cp<-ggplot(Time_pred, aes(x=Day_Year, y=Year)) + 
  geom_tile(aes(fill = fit)) +
  geom_contour(aes(z = fit), colour = "white") 
               
cp+ scale_fill_distiller(palette = "Spectral", name = "Infection Prevalence", breaks=c(0.2,0.3, 0.4,0.5, 0.6, 0.7), labels=c(0.2,0.3, 0.4,0.5, 0.6, 0.7)) +  xlab("Day of Year") + ylab("Year")

#Code help: https://drmowinckels.io/blog/2019-11-16-plotting-gamm-interactions-with-ggplot2/

```


*Checking the Fit of the Model*
```{r}
#Checking the results from the summary function 

#The deviance explained is a bit like R2 for models where sums of squares doesn't make much sense as a measure of discrepancy between the observations and the fitted values. In generalised models instead we measure this discrepancy using deviance. It is computed using the likelihood of the model and hence has a somewhat different mathematical definition for each error distribution. From Wood, S. N. (2006). Generalized Additive Models: An Introduction with R. Chapman and Hall/CRC.

summary(gam_modTIME)
#Deviance explained = 41.6%

#For standard deviations and confidence intervals 
gam.vcomp(gam_modTIME)

# Histogram of residuals 
gam.check(gam_modTIME)


#gam.check does not seem to be the best method for visualizing logistic models. Here we use a binned plot of the fitted values to evaluate the model fit. The vast majority of the residuals are well predicted by the model. 

library(arm)

binnedplot(fitted(gam_modTIME), 
           residuals(gam_modTIME, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")


acf(gam_modTIME$residuals)
pacf(TXPfiltered_gam$residuals)

pacf(gam_modTIME$residuals) 

 Box.test(resid(gam_modTIME),type="Ljung",lag=1)
Box.test(gam_modTIME, lag=10,type = "Ljung-Box")
```




*Spatio-temporal Model Example* 

Joint models of space and time. 

Note: Should an interaction between space and time be included? This was compared (check "MHD Extra Code") and the interaction was found to be significant and produced a significantly lower AIC value compared to the non-interaction term model. 
```{r}
data$Year<-as.numeric(data$Year)

#Now run the model with the interaction term 

STyes = gam(Infection~ s(longitude,latitude) + s(Year, k = 8)+ ti(longitude, latitude, Year, d=c(2,1)) + s(Volunteer, bs="re"), data=data,family=binomial,method="REML")

save(STyes, file="STyes.Rdata")

summary(STyes)
plot(STyes,scheme=2)

load("STyes.Rdata")
```

*Graphing of space-time interactions* 
We will next use the "predict" function from mgcv to better visualize the output of the STyes model.

We need to select a volunteer whose observations represent the average infection prevalence found by all volunteers. To do this, we will first determine the average infection prevalence per volunteer and then select the volunteer whose average is closest to this number. 
```{r}
#First use plyr to calculate the average per volunteer 
library(plyr)
data$Infection<-as.numeric(data$Infection)
data$Volunteer<-as.factor(data$Volunteer)

r2<-ddply(data,.(Volunteer), summarize, Avg=mean(Infection-1))
r2
#We now have a table r2, with the average infection prevalence per volunteer for all 269 volunteers 
```


Next we summarize the r2 dataframe to determine the overall mean per volunteer. 
```{r}
summary(r2)

#The mean in 0.44793
```
Select the volunteers with an average closest to the average of 0.44793
```{r}
Average_Volunteers <- r2[r2$Avg >= 0.4 & r2$Avg < 0.5, ]
Average_Volunteers$Differ<-0.44793-Average_Volunteers$Avg
Average_Volunteers

#We find that volunteer Dave Leimer (69) has an average closest to the overall average (0.44793), therefore we can use Dave in the model 
```


Create the background map on which the model predictions will be mapped 
```{r}
library("maps")
library("dplyr")
library("ggplot2")


states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

```

We can now move on to creating a grid of predicted infection prevalence using the average observation values of volunteer #13.  
```{r}
data$latitude<-as.numeric(data$latitude)
data$longitude<-as.numeric(data$longitude)

se_map$latitude<-as.numeric(se_map$lat)
se_map$longitude<-as.numeric(se_map$long)

data$Year<-as.factor(data$Year)

#First we'll create gridded data

predict_infection = expand.grid(
  latitude= seq(min(data$latitude), 
                max(data$latitude),
                length=100),
  longitude = seq(min(data$longitude),
                  max(data$longitude),
                  length=100),
  Volunteer = levels(data$Volunteer)[69],  
  Year = seq(2011,2018,by=1))

head(predict_infection)

#summary(data)
```

Create the Model Fit Column 
```{r}

predict_infection$model_fit = predict(STyes,
                                    predict_infection,type = "response")

```

Create a for loop to build polygons for each group in the dataset 
```{r}
library(ggplot2)
library(viridis)
library(dplyr)

se_map<-se_map %>% mutate(longitude=long,latitude=lat)
predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)

for(g in 1:length(groups))
{
  temp<-predict_infection[with(predict_infection %>% dplyr::select("longitude","latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("longitude","latitude"),longitude,latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}

library(mapproj)
library(maps)
library(ggrepel)

#Process for Making cities labels 
data(us.cities)
capitals <- subset(us.cities)
capitals$city <- sub(' [^ ]*$','',capitals$name) # split out city for the label

plotcities <- subset(world.cities, name %in% c("Atlanta", "Houston", "New Orleans", "Austin", "San Antonio", "Tampa", "Orlando", "Miami")) %>% filter(country.etc=="USA")


 
```

Graph the data
```{r}
gg<-ggplot(aes(longitude, latitude,  fill= model_fit), 
       data=predict_infection_clipped)+ #%>% filter(Year.Sampled==2011))+ 
  geom_tile()+
  facet_wrap(~Year,nrow=4)+
  scale_fill_viridis(option="magma", "Infection Prevalence")+
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+
  
  # geom_point(x = -84.42, y = 33.76, colour="red", size = 1)+
  # annotate(geom = "text", x = -84.42, y = 33.5, label = "Atlanta", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -95.39, y = 29.77, colour="red", size = 1)+
  # annotate(geom = "text", x = -95.39, y = 29.5, label = "Houston", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -89.93, y = 30.07, colour="red", size = 1)+
  # annotate(geom = "text", x = -88.00, y = 30.1, label = "New Orleans", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -82.48, y = 27.96, colour="red", size = 1)+
  # annotate(geom = "text", x = -83.7, y = 27.88, label = "Tampa", 
  #   fontface = "bold", color = "white", size = 1)+

  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))

  g<-gg+ theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
g
  
#save_plot("~/g.png", plot_tmp, base_height = NULL, base_aspect_ratio = 1.618, base_width = 6)

#library(OpenImageR)

#img<-OpenImageR::readImage("~/plot_tmp.png")
#imageShow(img)
 
```


Add space by day of year may be the next step in the analysis in order to see if infection changes in different regions through time


Can we examine the relationship between season and location? 
```{r Building the Spatiotemporal Model: Month}
library(mgcv)
#Created a new column "Season" with the months coded into seasons 
  #Nov, Dec, Jan, Feb = Winter 
  #March, April, May = Spring 
  #June, July, August = Summer 
  #September, October = Fall 


data$Year<-as.numeric(data$Year)
data$Infection<-as.factor(data$Infection)
data$Volunteer<-as.factor(data$Volunteer)
data$Season<-as.factor(data$Season)

data$Month<-as.factor(data$Month)


#Now run the model with the interaction term 
#STseason = gam(Infection~ s(longitude,latitude) + Season + ti(longitude, latitude, by = Season, d=c(2,1))  + s(Volunteer, bs="re"), data=data,family=binomial,method="REML")

STmonthasFactor = gam(Infection~ s(longitude,latitude) + s(Month) + ti(longitude, latitude, Month, d=c(2,1))  + s(Volunteer, bs="re"), data=data,family=binomial,method="REML")

save(STmonth, file="STmonth.Rdata")

summary(STmonth)
summary(STmonthasFactor)

plot(STmonthasFactor)
load("STmonth.Rdata")
```

We can now move on to creating a grid of predicted infection prevalence using the average observation values of volunteer #13.  
```{r Preparing Data for Graphing of Spatiotemporal Model: Month }


data$latitude<-as.numeric(data$latitude)
data$longitude<-as.numeric(data$longitude)

se_map$latitude<-as.numeric(se_map$lat)

se_map$longitude<-as.numeric(se_map$long)


#First we'll create gridded data

predict_infection2 = expand.grid(
  latitude= seq(min(data$latitude), 
                max(data$latitude),
                length=100),
  longitude = seq(min(data$longitude),
                  max(data$longitude),
                  length=100),
  Volunteer = levels(data$Volunteer)[69],  
  Month = seq(min(data$Month),
                  max(data$Month),
                  length=12))

head(predict_infection2)

summary(predict_infection2$Month)
predict_infection2$model_fit = predict(STmonthasFactor,
                                    predict_infection2,type = "response")
```

Create the Model Fit Column 
```{r Graphing Spatiotemporal Models: Month}
predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)



for(g in 1:length(groups))
{
  temp<-predict_infection2[with(predict_infection2 %>% dplyr::select("longitude","latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("longitude","latitude"),longitude,latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}

#Create a second month column to preserve the numeric form 
predict_infection_clipped$MonthNames<-predict_infection_clipped$Month


##PLOTTING 

#Turn numbers into month names 
predict_infection_clipped%>%arrange(Month)

predict_infection_clipped$MonthNames<-as.character(predict_infection_clipped$MonthNames)

predict_infection_clipped$MonthNames <- revalue(x = predict_infection_clipped$MonthNames, 
c("1" = "Jan", "2" = "Feb", "3" = "March", "4" = "April", "5" = "May", "6" = "June", "7" = "July", "8" = "Aug",  "9" = "Sept", "10" = "Oct", "11" = "Nov", "12" = "Dec" ))

predict_infection_clipped$MonthNames<-as.factor(predict_infection_clipped$MonthNames)

 # Reordering group factor levels
predict_infection_clipped$MonthNames <- factor(predict_infection_clipped$MonthNames,     
                         levels = c("Jan", "Feb",  "March", "April",  "May", "June", "July",  "Aug",   "Sept",  "Oct",  "Nov",  "Dec" ))

##Plot 

gg<-ggplot(aes(longitude, latitude,  fill= model_fit), 
       data=predict_infection_clipped)+


 #%>% filter(Year.Sampled==2011))+ 
  geom_tile()+
 facet_wrap(~MonthNames)+
 #facet_wrap(~MonthNames, levels= unique(MonthNames))+
              
              #c( "Jan", "Feb",  "March", "April",  "May", "June", "July",  "Aug",   "Sept",  "Oct",  "Nov",  "Dec" )) +
  
  scale_fill_viridis(option="magma", "Infection Prevalence")
  #geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+

  #coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))

 #g<-gg+ theme(panel.background = element_rect(fill = 
        #                                         "skyblue3",
         #                       colour = "skyblue3"),
        #  panel.grid.major = element_blank(),
        #  panel.grid.minor = element_blank())
gg


#, labeller = labeller(Month = monthlabs)
```


*Research Question 2*
How do the listed exogenous variables impact the spatial dynamics of OE infection prevalence change within year and between years in the sedentary population of monarch butterflies in the SE U.S.?
  
  1. Temperature  
  2. Precipitation 
  3. Monarch larval density 
  

*Environmental Variables Model* 
In the Brown et al. paper, they used the GAMs in the previous step to determine the end of season infection prevalence for each year. They then looked for "bivariate correlations" between these infection prevalences and different environmental variables. 

  *Examining the effect of freezing temperatures at 3 scales*
    1. How does the number of freezes per year impact the overall infection prevalence? 
    2. How does the number of freezes within each hardiness zone impact the overall infection prevalence?
    3. How does the number of freezes preceeding an observation impact infection? 
   


Import both the infection and weather data sets. 
The infection data includes an infection score for each observation. The weather data includes the weather data for each weather station within range of an observation for every day during the study period. 
```{r EV Models: Import Data}
###########################################################################Import the infection data 
library(readr)
MHD<- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/YR_MHD_weather_hardi2.csv")
MHD <- read_csv("./data/YR_MHD_weather_hardi2.csv")

 #Make sure date column is in correct format 
library(lubridate)
MHD$date<- parse_date_time(MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
MHD$date<-as.Date(MHD$date)

###########################################################################Import the weather data 

Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Freeze.csv")
# Weather_Hardi <- read_csv("./data/Weather_Freeze.csv")
# 
# #Modify the weather data to prepare it for analysis 
#   #Rename Columns to match MHD 
 library(dplyr)
Weather_Hardi<-Weather_Hardi %>% rename(Year = Year.Sampled,Station = StationName)
# 
 #Make sure date column is in correct format
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)
```



*Build Freeze Dataframe*
Code Days when *Freezes* Occurred as 1 in the Weather Data Set 
```{r EV Models: Create Daily Freeze Dataframe}
############################################
###########DAILY FREEZE DATA################
############################################

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week.
#Function for counting the number of freezes. Starting with identifying freeze days.
freeze.days<-function (TMIN, Day){
  freeze.days<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      freezes<-NA
    }else if (TMIN[i]>0){
      freezes<-0
    }else{
      freezes<-1
    }
    freeze.days<-c(freeze.days, freezes)
  }
  return(freeze.days)
}

#and now the freeze day counter
#this took my computer 1.5 minutes to run
Weather_Hardi$freeze.day<-freeze.days(Weather_Hardi$TMIN, Weather_Hardi$Day)
#Weather_Hardi$freeze.day<- Weather_Hardi$TMIN < 0

Weather_Hardi %>% group_by(freeze.day) %>% tally()
```

Next, the data need to go through a series of steps to arrange them for calculating the number of freezes inside of the desired window
```{r Preparing freeze data for 7 day window tabulation}
###############################################################
#Prepare data for the "roll apply"" function by arranging the stations as columns with date 
###############################################################
  
  #Create a data frame with only the id of the weather station, the date and the freeze code 
DailyFreezeSubset = subset(Weather_Hardi, select = c(id, date, freeze.day) )

  #Split the "id" column into 95 separate columns (one for each station)
library(tidyr)
#DFS<-DailyFreezeSubset %>% 
#   mutate(rn = row_number()) %>%
#   spread(id,  freeze.day) %>%
#   select(-rn)

DFS<-DailyFreezeSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=freeze.day) %>% arrange(date)

DFS %>% group_by(USC00083163) %>% tally()
#Each station should have over 2500 observations (note: this station is in Fort Lauderdale and should have no freezes)

```


*7 Day Freeze Window Calculations*
Count the number of *freezes* in the previous 7 days, 30 days and 60 days from when an observation was made 
```{r Count the number of freezes in the previous 7 days}
############################################
###########Apply RollApply Function#########
############################################

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
DFSroll<-rollapply(data = DFS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
DFSroll<-as.data.frame(DFSroll)

#Add date column back into
Freeze7 <- cbind(date = DFS$date,DFSroll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
Freeze7<-Freeze7 %>% pivot_longer(cols=2:ncol(Freeze7),names_to="id",values_to="freeze.day")

#Check to ensure that there is a range of freezes 
Freeze7 %>% group_by(freeze.day) %>% tally()

#Now Merge the freeze data with the monarch health observation data, allowing each monarch health observation to be paired with a corresponding weather observation including the summed window freeze data. 

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
Freeze7$id<-as.factor(Freeze7$id)

levels(Freeze7$id)
levels(MHD$id)

#Merge 
MHD_freeze<-left_join(MHD, Freeze7, by=c("date", "id"))
summary(MHD_freeze)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))

```


*30 Day Freeze Window*
```{r 30 Day Freeze Window Calculations}
DailyFreezeSubset = subset(Weather_Hardi, select = c(id, date, freeze.day) )
DFS<-DailyFreezeSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=freeze.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
DFSroll<-rollapply(data = DFS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
DFSroll<-as.data.frame(DFSroll)

#Add date column back into
Freeze7 <- cbind(date = DFS$date,DFSroll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
Freeze7<-Freeze7 %>% pivot_longer(cols=2:ncol(Freeze7),names_to="id",values_to="freeze.day")

#Check to ensure that there is a range of freezes 
Freeze7 %>% group_by(freeze.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
Freeze7$id<-as.factor(Freeze7$id)

levels(Freeze7$id)
levels(MHD$id)

#Merge 
MHD_freeze30<-left_join(MHD, Freeze7, by=c("date", "id"))
summary(MHD_freeze30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```



*Developmental Zero Calculations* 

```{r}
###########################################
##########DAILY DevZero DATA###############
###########################################
#Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Hardi.csv")
Weather_Hardi <- read_csv("./data/Weather_Hardi.csv")

#Modify the weather data to prepare it for analysis
  #Rename Columns to match MHD
library(dplyr)
Weather_Hardi<-Weather_Hardi %>%
  rename(Year = Year.Sampled,
    Station = StationName)

 #Make sure date column is in correct format
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)
#Create the week variable
Weather_Hardi$week <- week(Weather_Hardi$date)

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week.
#Function for counting the number of freezes. Starting with identifying freeze days.
DevZero.days<-function (TMIN, Day){
  DevZero.days<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      devzero<-NA
    }else if (TMIN[i]>10){
      devzero<-0
    }else{
      devzero<-1
    }
    DevZero.days<-c(DevZero.days, devzero)
  }
  return(DevZero.days)
}

#and now the dev zero day counter
Weather_Hardi$devzero.day<-DevZero.days(Weather_Hardi$TMIN, Weather_Hardi$Day)

#Examine the number of dev zero days to ensure a reasonable number have been recorded
Weather_Hardi %>% group_by(devzero.day) %>% tally()
```


*Dev Zero 7 Day Calculations*
```{r Dev Zero 7 Day Calculations}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
DailyDevZeroSubset = subset(Weather_Hardi, select = c(id, date, devzero.day) )
DZS<-DailyDevZeroSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=devzero.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
DevZeroData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
DevZero_7DayData<-DevZeroData %>% pivot_longer(cols=2:ncol(DevZeroData),names_to="id",values_to="devzero.day")

#Check to ensure that there is a range of freezes 
DevZero_7DayData %>% group_by(devzero.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
DevZeroData$id<-as.factor(DevZeroData$id)

levels(DevZeroData$id)
levels(MHD$id)

#Merge 
DevZeroData7<-left_join(MHD, DevZero_7DayData, by=c("date", "id"))
summary(DevZeroData30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```

*Dev Zero 30 Day Calculations*
```{r}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
DailyDevZeroSubset = subset(Weather_Hardi, select = c(id, date, devzero.day) )
DZS<-DailyDevZeroSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=devzero.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
DevZeroData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
DevZeroData<-DevZeroData %>% pivot_longer(cols=2:ncol(DevZeroData),names_to="id",values_to="devzero.day")

#Check to ensure that there is a range of freezes 
DevZeroData %>% group_by(devzero.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
DevZeroData$id<-as.factor(DevZeroData$id)

levels(DevZeroData$id)
levels(MHD$id)

#Merge 
DevZeroData30<-left_join(MHD, DevZeroData, by=c("date", "id"))
summary(DevZeroData30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```










*30C  Calculations* 

```{r}
###########################################
##########30C DATA###############
###########################################
#Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Hardi.csv")
Weather_Hardi <- read_csv("./data/Weather_Hardi.csv")

#Modify the weather data to prepare it for analysis
  #Rename Columns to match MHD
library(dplyr)
Weather_Hardi<-Weather_Hardi %>%
  rename(Year = Year.Sampled,
    Station = StationName)

 #Make sure date column is in correct format
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)
#Create the week variable
Weather_Hardi$week <- week(Weather_Hardi$date)

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week.
#Function for counting the number of freezes. Starting with identifying freeze days.
ThirtyC.days<-function (TMAX, Day){
  ThirtyC.days<-c()
  for (i in 1:length(TMAX)){
    if(is.na(TMAX[i])){
      ThirtyC<-NA
    }else if (TMAX[i]<30){
      ThirtyC<-0
    }else{
      ThirtyC<-1
    }
    ThirtyC.days<-c(ThirtyC.days, ThirtyC)
  }
  return(ThirtyC.days)
}

#and now the dev zero day counter
Weather_Hardi$thirtyc.day<-ThirtyC.days(Weather_Hardi$TMAX, Weather_Hardi$Day)

#Examine the number of dev zero days to ensure a reasonable number have been recorded
Weather_Hardi %>% group_by(thirtyc.day) %>% tally()


#For 30C Day of Observation Analysis 
ThirtyCData<-left_join(MHD, Weather_Hardi, by=c("date", "id"))
ThirtyCData %>% group_by(thirtyc.day) %>% tally()


```


*30C 7 Day Calculations*
```{r 30C 7 Day Calculations}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
library(tidyr)
ThirtyCSubset = subset(Weather_Hardi, select = c(id, date, thirtyc.day) )
DZS<-ThirtyCSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=thirtyc.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
ThirtyCData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
ThirtyCData<-ThirtyCData %>%  gather(id, thirtyc.day,c(2:ncol(ThirtyCData))) 
#ThirtyCData<-ThirtyCData %>% tidyr::pivot_longer(cols=2:ncol(ThirtyCData),names_to="id",values_to="thirtyc.day")

#Check to ensure that there is a range of freezes 
ThirtyCData %>% group_by(thirtyc.day) %>% tally()
#

#Prepare data columns for the merge
MHD<-YR_MHD_weather_hardi2

library(lubridate)
MHD$date<- parse_date_time(MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
MHD$date<-as.Date(MHD$date) 


MHD$id<-as.factor(MHD$id)
ThirtyCData$id<-as.factor(ThirtyCData$id)

#Merge 
ThirtyCData_7day<-left_join(MHD, ThirtyCData, by=c("date", "id"))
ThirtyCData_7day %>% group_by(thirtyc.day) %>% tally()

```

*30C 30 Day Calculations*
```{r 30C 30 Day Calculations}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
library(tidyr)
ThirtyCSubset = subset(Weather_Hardi, select = c(id, date, thirtyc.day) )
DZS<-ThirtyCSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=thirtyc.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
ThirtyCData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
ThirtyCData<-ThirtyCData %>%  gather(id, thirtyc.day,c(2:ncol(ThirtyCData))) 
#ThirtyCData<-ThirtyCData %>% tidyr::pivot_longer(cols=2:ncol(ThirtyCData),names_to="id",values_to="thirtyc.day")

#Check to ensure that there is a range of freezes 
ThirtyCData %>% group_by(thirtyc.day) %>% tally()
#

#Prepare data columns for the merge
MHD<-YR_MHD_weather_hardi2

library(lubridate)
MHD$date<- parse_date_time(MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
MHD$date<-as.Date(MHD$date) 


MHD$id<-as.factor(MHD$id)
ThirtyCData$id<-as.factor(ThirtyCData$id)

#Merge 
ThirtyCData_30day<-left_join(MHD, ThirtyCData, by=c("date", "id"))
ThirtyCData_30day %>% group_by(thirtyc.day) %>% tally()

```

*Dev Zero 30 Day Calculations*
```{r}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
DailyDevZeroSubset = subset(Weather_Hardi, select = c(id, date, devzero.day) )
DZS<-DailyDevZeroSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=devzero.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
DevZeroData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
DevZeroData<-DevZeroData %>% pivot_longer(cols=2:ncol(DevZeroData),names_to="id",values_to="devzero.day")

#Check to ensure that there is a range of freezes 
DevZeroData %>% group_by(devzero.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
DevZeroData$id<-as.factor(DevZeroData$id)

levels(DevZeroData$id)
levels(MHD$id)

#Merge 
DevZeroData30<-left_join(MHD, DevZeroData, by=c("date", "id"))
summary(DevZeroData30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```














*TMIN Calculations*
```{r TMIN 7 Day Window Calculations}
library(tidyr)
TMINSubset = subset(Weather_Hardi, select = c(id, date, TMIN) )
TMINS<-TMINSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=TMIN) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = TMINS [,c(2:96)],width = 7, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
TMINData <- cbind(date = TMINS$date,TRoll )

#Turn station ID back into a single column for model analysis   
TMINData<-TMINData %>% pivot_longer(cols=2:ncol(TMINData),names_to="id",values_to="TMIN.7day")

#Check to ensure that there is a range of freezes 
TMINData %>% group_by(TMIN.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMINData$id<-as.factor(TMINData$id)

#Merge 
TMINData7<-left_join(MHD, TMINData, by=c("date", "id"))
summary(TMINData7)
```

*TMIN 30 Day Window Calculations*
```{r TMIN 30 Day Window Calculations}
library(tidyr)
TMINSubset = subset(Weather_Hardi, select = c(id, date, TMIN) )
TMINS<-TMINSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=TMIN) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = TMINS [,c(2:96)],width = 30, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
TMINData <- cbind(date = TMINS$date,TRoll )

#Turn station ID back into a single column for model analysis   
TMINData<-TMINData %>% pivot_longer(cols=2:ncol(TMINData),names_to="id",values_to="TMIN.30day")

#Check to ensure that there is a range of freezes 
TMINData %>% group_by(TMIN.30day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMINData$id<-as.factor(TMINData$id)
TMINData$date<-as.Date(TMINData$date)


#Merge 
TMINData30<-left_join(MHD, TMINData, by=c("date", "id"))
summary(TMINData30)
```



*TMAX 7 Day Window Calculations*
```{r TMAX 7 Day Window Calculations}
library(tidyr)
library(dplyr)
TMAXSubset = subset(Weather_Hardi, select = c(id, date, TMAX) )
TMAXS<-TMAXSubset %>% pivot_wider(id_cols=date,names_from = id,values_from= TMAX) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TMRoll<-rollapply(data = TMAXS [,c(2:96)],width = 7, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TMRoll<-as.data.frame(TMRoll)

#Add date column back into
TMAXData <- cbind(date = TMAXS$date,TMRoll )

#Turn station ID back into a single column for model analysis   
TMAXData<-TMAXData %>% pivot_longer(cols=2:ncol(TMAXData),names_to="id",values_to="TMAX.7day")

#Check to ensure that there is a range of freezes 
TMAXData %>% group_by(TMAX.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
TMAXData$id<-as.factor(TMAXData$id)
TMAXData$date<-as.Date(TMAXData$date)

#Merge 
TMAXData7<-left_join(MHD, TMAXData, by=c("date", "id"))
summary(TMAXData7)
```


*TMAX 30 Day Window Calculations*
```{r TMAX 30 Day Window Calculations}
library(tidyr)
TMAXSubset = subset(Weather_Hardi, select = c(id, date, TMAX) )
TMAXS<-TMAXSubset %>% pivot_wider(id_cols=date,names_from = id,values_from= TMAX) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TMRoll<-rollapply(data = TMAXS [,c(2:96)],width = 30, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TMRoll<-as.data.frame(TMRoll)

#Add date column back into
TMAXData <- cbind(date = TMAXS$date,TMRoll )

#Turn station ID back into a single column for model analysis   
TMAXData<-TMAXData %>% pivot_longer(cols=2:ncol(TMAXData),names_to="id",values_to="TMAX.30day")

#Check to ensure that there is a range of freezes 
TMAXData %>% group_by(TMAX.30day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMAXData$id<-as.factor(TMAXData$id)

#Merge 
TMAXData30<-left_join(MHD, TMAXData, by=c("date", "id"))
summary(TMAXData30)
```



*PRCP 7 Day Average Window Calculations*
```{r PRCP 7 Day Window Calculations}
library(tidyr)
library(dplyr)
PRCPSubset = subset(Weather_Hardi, select = c(id, date, PRCP) )
PRCPS<-PRCPSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=PRCP) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = PRCPS [,c(2:96)],width = 7, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
PRCPData <- cbind(date = PRCPS$date,TRoll )

#Turn station ID back into a single column for model analysis   
PRCPData<-PRCPData %>% pivot_longer(cols=2:ncol(PRCPData),names_to="id",values_to="PRCP.7day")

#Check to ensure that there is a range of freezes 
PRCPData %>% group_by(PRCP.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
PRCPData$id<-as.factor(PRCPData$id)
PRCPData$date<-as.Date(PRCPData$date)


#Merge 
PRCPData7<-left_join(MHD, PRCPData, by=c("date", "id"))
summary(TMINData7)
```

*PRCP 7 Day Additive Window Calculations*
```{r PRCP 7 Day Additive Window Calculations}
library(tidyr)
library(dplyr)
PRCPSubset = subset(Weather_Hardi, select = c(id, date, PRCP) )
PRCPS<-PRCPSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=PRCP) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TaRoll<-rollapply(data = PRCPS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TaRoll<-as.data.frame(TRoll)

#Add date column back into
PRCPData <- cbind(date = PRCPS$date,TaRoll )

#Turn station ID back into a single column for model analysis   
PRCPData<-PRCPData %>% pivot_longer(cols=2:ncol(PRCPData),names_to="id",values_to="PRCP.7day")

#Check to ensure that there is a range of freezes 
PRCPData %>% group_by(PRCP.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
PRCPData$id<-as.factor(PRCPData$id)
PRCPData$date<-as.Date(PRCPData$date)


#Merge 
PRCPData7add<-left_join(MHD, PRCPData, by=c("date", "id"))
summary(TMINData7)
```


*PRCP 30 Day Average Window Calculations*
```{r PRCP 30 Day Average Window Calculations}
library(tidyr)
library(dplyr)
PRCPSubset = subset(Weather_Hardi, select = c(id, date, PRCP) )
PRCPS<-PRCPSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=PRCP) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TaRoll2<-rollapply(data = PRCPS [,c(2:96)],width = 30, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TaRoll2<-as.data.frame(TaRoll2)

#Add date column back into
PRCPData2 <- cbind(date = PRCPS$date,TaRoll2 )

#Turn station ID back into a single column for model analysis   
PRCPData2<-PRCPData2 %>% pivot_longer(cols=2:ncol(PRCPData2),names_to="id",values_to="PRCP.30day")

#Check to ensure that there is a range of freezes 
PRCPData2 %>% group_by(PRCP.30day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD$id<-as.factor(MHD$id)
PRCPData2$id<-as.factor(PRCPData2$id)
PRCPData2$date<-as.Date(PRCPData2$date)


#Merge 
PRCPData30<-left_join(MHD, PRCPData2, by=c("date", "id"))
summary(TMINData7)
```


****************GAM Models******************* 

*Base GAM Model* 
The GAM model without Environmental Variables 
```{r Base GAM Model}
library(mgcv)

###################Base###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
O_gam <-mgcv:: gam(Infection ~ s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(O_gam)
AIC(O_gam)
#14165.96
plot(O_gam, pages = 1,  trans = plogis,
     shift = coef(O_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

*FREEZE GAM Models* 
Freeze 7 Day Average GAM
```{r Freeze 7 Day Average GAM}
###################FREEZE 7 Day Average##################################

MHD_freeze$Volunteer<-as.factor(MHD_freeze$Volunteer)
F7_gam <-mgcv:: gam(Infection ~ s(freeze.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = MHD_freeze)
summary(F7_gam)
AIC(F7_gam)
#12523.63
plot(F7_gam, pages = 1,  trans = plogis,
     shift = coef(F7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


Freeze 30 Day Average GAM
```{r Freeze 30 Day Average GAM}

###################FREEZE 30 Day Average##################################
library(mgcv)
MHD_freeze30$Volunteer<-as.factor(MHD_freeze30$Volunteer)
F30_gam <-mgcv:: gam(Infection ~ s(freeze.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = MHD_freeze30)
summary(F30_gam)
AIC(F30_gam)
#12529.84
plot(F30_gam, pages = 1,  trans = plogis,
     shift = coef(F30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```


*DevZero GAM Models* 
DevZero 7 Day Average GAM
```{r DevZero 7 Day Average GAM}

###################DevZero 7 Day Average##################################
library(mgcv)
DevZeroData7$Volunteer<-as.factor(DevZeroData7$Volunteer)
Z7_gam <-mgcv:: gam(Infection ~ s(devzero.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = DevZeroData7)
summary(Z7_gam)
AIC(Z7_gam)
#12527.52
plot(Z7_gam, pages = 1,  trans = plogis,
     shift = coef(Z7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```

DevZero 30 Day Average GAM
```{r DevZero 30 Day Average GAM}

###################DevZero 30 Day Average##################################
library(mgcv)
DevZeroData30$Volunteer<-as.factor(DevZeroData30$Volunteer)
Z30_gam <-mgcv:: gam(Infection ~ s(devzero.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = DevZeroData30)
summary(Z30_gam)
AIC(Z30_gam)
#12499.83
plot(Z30_gam, pages = 1,  trans = plogis,
     shift = coef(Z30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```

*TMIN GAM Models* 
TMIN Raw GAM
```{r TMIN Raw GAM}
library(mgcv)

###################TMIN RAW###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
T_gam <-mgcv:: gam(Infection ~ s(TMIN2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(T_gam)
AIC(T_gam)
#13429.8
plot(T_gam, pages = 1,  trans = plogis,
     shift = coef(T_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMIN 7 Day Average GAM
```{r TMIN 7 Day GAM}
###################TMIN 7 Day Average##################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
T7_gam <-mgcv:: gam(Infection ~ s(TMIN.7day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(T7_gam)
AIC(T7_gam)
#12114.99
plot(T7_gam, pages = 1,  trans = plogis,
     shift = coef(T7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMIN 30 Day Average GAM
```{r TMIN 30 Day GAM}
###################TMIN 30 Day Average##################################
TMINData30$Volunteer<-as.factor(TMINData30$Volunteer)
T30_gam <-mgcv:: gam(Infection ~ s(TMIN.30day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData30)
summary(T30_gam)
AIC(T30_gam)
#12204.9
plot(T30_gam, pages = 1,  trans = plogis,
     shift = coef(T30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


*TMAX Raw GAM*
```{r TMAX Raw GAM}

###################TMIN RAW###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
Max_gam <-mgcv:: gam(Infection ~ s(TMAX2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(Max_gam)
AIC(Max_gam)
#13372.5
plot(Max_gam, pages = 1,  trans = plogis,
     shift = coef(Max_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMAX 7 Day Average GAM
```{r TMAX 7 Day GAM}
###################TMIN 7 Day Average##################################
TMAXData7$Volunteer<-as.factor(TMAXData7$Volunteer)
TM7_gam <-mgcv:: gam(Infection ~ s(TMAX.7day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMAXData7)
summary(TM7_gam)
AIC(TM7_gam)
#12058.9
plot(TM7_gam, pages = 1,  trans = plogis,
     shift = coef(T7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMAX 30 Day Average GAM
```{r TMAX 30 Day GAM}
###################TMIN 7 Day Average##################################
TMAXData30$Volunteer<-as.factor(TMAXData30$Volunteer)
TM30_gam <-mgcv:: gam(Infection ~ s(TMAX.30day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMAXData30)
summary(TM30_gam)
AIC(TM30_gam)
# 12217.76
plot(TM30_gam, pages = 1,  trans = plogis,
     shift = coef(T7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


*30C GAM Models*
ThirtyCData7
```{r 30C 7 Day GAM}
###################TMIN 7 Day Average##################################
ThirtyCData_7day$Volunteer<-as.factor(ThirtyCData_7day$Volunteer)
C30_7daygam <-mgcv:: gam(Infection ~ s(thirtyc.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = ThirtyCData_7day)
summary(C30_7daygam)
AIC(C30_7daygam)
# 12495.43
plot(C30_7daygam, pages = 1,  trans = plogis,
     shift = coef(C30_7daygam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

save(C30_7daygam, file = "C30_7daygam.RData")

```


ThirtyC 30 Day GAM
```{r 30C 30 Day GAM}
###################TMIN 7 Day Average##################################
ThirtyCData_30day$Volunteer<-as.factor(ThirtyCData_30day$Volunteer)
C30_30daygam <-mgcv:: gam(Infection ~ s(thirtyc.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = ThirtyCData_30day)
summary(C30_7daygam)
AIC(C30_7daygam)
# 12495.43
plot(C30_7daygam, pages = 1,  trans = plogis,
     shift = coef(C30_7daygam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

save(C30_7daygam, file = "C30_30daygam.RData")

```


ThirtyC Day of Observation GAM
```{r 30C 30 Day GAM}
###################TMIN 7 Day Average##################################
ThirtyCData$Volunteer<-as.factor(ThirtyCData$Volunteer)
ThirtyCData$Infection<-as.factor(ThirtyCData$Infection)


C30_gam <-mgcv:: gam(Infection ~ thirtyc.day + s(Year.x, k= 5) + s(longitude.x,latitude.x) + s(Volunteer,bs="re") , family = binomial, link = logit, data = ThirtyCData)
summary(C30_gam)
AIC(C30_gam)
# 12495.43
plot(C30_7daygam, pages = 1,  trans = plogis,
     shift = coef(C30_7daygam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

save(C30_7daygam, file = "C30_30daygam.RData")

```







*Precipitation GAM Models*
PRCP Raw GAM
```{r PRCP Raw GAM}
library(mgcv)
write.csv(PRCPData7, "PRCPData7.csv")
save(PRCPData7, file = "PRCPData7.RData")
###################PRCP RAW###########################################
PRCPData7$Volunteer<-as.factor(PRCPData7$Volunteer)
PRCPData7$PRCP2<-as.numeric(PRCPData7$PRCP2)

P_gam <-mgcv:: gam(Infection ~ s(PRCP2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)

summary(P_gam)
AIC(P_gam)
#12372.3
plot(P_gam, pages = 1,  trans = plogis,
     shift = coef(P_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


PRCP 7 Day AVerage Model 
```{r PRCP 7 Day GAM}
library(mgcv)

###################TMIN RAW###########################################
PRCPData7$Volunteer<-as.factor(PRCPData7$Volunteer)
PRCPData7$PRCP.7day<-PRCPData7$PRCP.7day/10



P_gam7 <-mgcv:: gam(Infection ~ s(PRCP.7day) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)
summary(P_gam7)
AIC(P_gam7)
#10968.38

#Filtering High Values#
PRCPData7<- filter(PRCPData7, PRCP.7day < "40")

#filtered 9041.548
plot(P_gam7, pages = 1,  trans = plogis,
     shift = coef(P_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


PRCP 7 Day Summed model 
```{r PRCP 7 Day GAM}
library(mgcv)

###################PRCP 7 Day Additive Model ###########################################
PRCPData7add$Volunteer<-as.factor(PRCPData7add$Volunteer)
PRCPData7add$PRCP.7day<-PRCPData7add$PRCP.7day/10



P_gam7add <-mgcv:: gam(Infection ~ s(PRCP.7day) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7add)
summary(P_gam7add)
AIC(P_gam7add)
#12519.95

plot(P_gam7add, pages = 1,  trans = plogis,  shift = coef(P_gam7add)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

####################ReRun Model with a Filter on Samples ######################
#Filtering High Values#
PRCPData7<- filter(PRCPData7, PRCP.7day < "40")

#filtered 9041.548
plot(P_gam7, pages = 1,  trans = plogis,shift = coef(P_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
    
```

PRCP 30 Day Average Model 
```{r PRCP 30 Day Average GAM}
library(mgcv)
#For cluster 
save(PRCPData30, file = "PRCPData30.RData")
load(file= "P_gam30.RData")
##################PRCP 30 Day Average Model ###########################################
PRCPData30$Volunteer<-as.factor(PRCPData30$Volunteer)

P_gam30 <-mgcv:: gam(Infection ~ s(PRCP.30day) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData30)
summary(P_gam30)
AIC(P_gam30)
#10968.38

plot(P_gam30, pages = 1,  trans = plogis,  shift = coef(P_gam30)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```


Additive Models 
```{r PRCP + TMAX Raw GAM}
library(mgcv)

#For cluster
load(file ="PRCPData7.RData")
load(file= "TP_gam.RData")
###################PRCP + TMAX Raw GAM###########################################
PRCPData7$Volunteer<-as.factor(PRCPData7$Volunteer)
PRCPData7$PRCP2<-as.numeric(PRCPData7$PRCP2)



TP_gam <-mgcv:: gam(Infection ~ s(PRCP2) + s(TMAX2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)
summary(TP_gam)
AIC(TP_gam)
#
plot(TP_gam, pages = 1,  trans = plogis,  shift = coef(TP_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

```{r PRCP + TMAX Average GAM}
library(mgcv)

#Create dataframe with merged averaged 7 day values of PRCP and TMIN 
#Had major problems getting my join to work. Found this Explanation of the code below (that seems to work) online. 
  #I add a row number within each group of dates, so that only the first June from df_2 will be joined to the first June entry from       df_1.
PXT_7day<-left_join(PRCPData7 %>% group_by(date) %>% mutate(rowID = row_number()),
          TMAXData7 %>% group_by(date) %>% mutate(rowID = row_number()), 
          by = c("date", "rowID"))

#For cluster 
save(TPa_gam, file = "TPa_gam.RData")
load(file ="PXT_7day.RData")
load(file= "TPa_gam.RData")

###################PRCP + TMIN 7 Day Average GAM###########################################
PXT_7day$Volunteer.x<-as.factor(PXT_7day$Volunteer.x)

TPa_gam <-mgcv:: gam(Infection.x ~ s(PRCP.7day) + s(TMAX.7day) + s(Year.x, k= 5) + s(longitude.x,latitude.x) + s(Volunteer.x,bs="re") , family = binomial, link = logit, data = PXT_7day)
summary(TPa_gam)
AIC(TPa_gam)
#10836.26
plot(TPa_gam, pages = 1,  trans = plogis,  shift = coef(TPa_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

Interaction Models 
```{r PRCP X MAX Raw GAM}
library(mgcv)
#Create dataframe with merged averaged 7 day values of PRCP and TMIN 
  #completed in previous section 
###################PRCP X MAX Raw GAM###########################################

TXPraw_gam <-mgcv:: gam(Infection ~ s(PRCP2) + s(TMAX2) + ti(PRCP, TMAX2)+ s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = PRCPData7)
summary(TXPraw_gam)
AIC(TXPraw_gam)
#
plot(TXPraw_gam, pages = 1,  trans = plogis,  shift = coef(TXPraw_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

vis.gam(TXPraw_gam, view=c("TMAX2", "PRCP2"),plot.type="contour", color = "terrain", contour.col = "black") 
```

```{r PRCP X TMAX 7 Day Average GAM}
library(mgcv)

###For Cluster Work 
load("PXT_7day.RData")

  #Trying with filtered values 
#Filtering High Values#
library(dplyr)
#TMAX.7day > "-10",
something2<- filter(PXT_7day,  PRCP.7day < "60")
summary(something2)
#something (50 prcp) total of 11607 samples, 5500 lost 
#something 2 (60 prcp) 12095 5000 lost
#something 2 (70 prcp) 12420 4500 lost


something$Volunteer.x<-as.factor(something$Volunteer.x)

TXPfiltered_gam2 <-mgcv:: gam(Infection.x ~ s(PRCP.7day) + s(TMAX.7day) + ti(PRCP.7day, TMAX.7day)+ s(Year.x, k= 5) + s(longitude.x,latitude.x) + s(Volunteer.x,bs="re") , family = binomial, link = logit, data = something2)

save(TXPfiltered_gam, file="TXPfiltered_gam.RData")

plot(TXPfiltered_gam2, pages = 1,  trans = plogis,  shift = coef(TXPfiltered_gam2)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

library(mgcViz)
vis.gam(TXPfiltered_gam2, view=c(  "TMAX.7day", "PRCP.7day"),plot.type="contour", color = "terrain", contour.col = "black") 

###################PRCP X TMAX 7 Day Average GAM###########################################
PXT_7day$Volunteer.x<-as.factor(PXT_7day$Volunteer.x)

TXPa_gam <-mgcv:: gam(Infection.x ~ s(PRCP.7day) + s(TMAX.7day) + ti(PRCP.7day, TMAX.7day)+ s(Year.x, k= 5) + s(longitude.x,latitude.x) + s(Volunteer.x,bs="re") , family = binomial, link = logit, data = PXT_7day)
summary(TXPa_gam)
AIC(TXPa_gam)
#10813.13
plot(TXPa_gam, pages = 1,  trans = plogis,  shift = coef(TXPa_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

b<-getViz(TXPfiltered_gam2)
print(plot(b, allTerms = TRUE), pages = 1)

library(mgcViz)
vis.gam(TXPa_gam, view=c("TMAX.7day", "PRCP.7day"),plot.type="contour", color = "terrain", contour.col = "black") 
plogis(-0.4731)

```
Create a Contour Plot for the Interaction Between Temperature and Precipitation 
```{r}
library(mgcv)
library(tidyr)

PXT_7day$Volunteer.x<-as.factor(PXT_7day$Volunteer.x)
#Using ggplot to control details of contour plot 
 Time_pred <- expand_grid(
  TMAX.7day = seq(from=0, 
              to=50, length.out = 50),
   PRCP.7day = seq(from=0, 
              to=300, length.out = 300),
  Year.x = seq(from=2011, 
              to=2018, length.out = 8 ),
    latitude.x = seq(from=min(PXT_7day$latitude.x), 
              to=max(PXT_7day$latitude.x)),
  longitude.x = seq(from=min(PXT_7day$longitude.x), 
              to=max(PXT_7day$longitude.x)),
   Volunteer.x = 1, length.out = 1)
 
write.csv(Time_pred, "Time.pred.csv")
 
 Tie_pred <- predict(TXPfiltered_gam, newdata = Tie_pred, 
                      se.fit = TRUE) %>%  
  as_tibble() %>% 
  cbind(Tie_pred)

save(Time_pred, file="Time_pred.RData")
load("Tie_pred.csv")
 
dim(Time_pred)   

#Turn log-odds ratio into probability 
Time_pred$fit<-plogis(Time_pred$fit)

#Determine the min/max of the fit column 
summary(Time_pred$fit)

#Graph using ggplot 
cp<-ggplot(Time_pred, aes(x=Day_Year, y=Year)) + 
  geom_tile(aes(fill = fit)) +
  geom_contour(aes(z = fit), colour = "white") 
               
cp+ scale_fill_distiller(palette = "Spectral", name = "Infection Prevalence", breaks=c(0.2,0.3, 0.4,0.5, 0.6, 0.7), labels=c(0.2,0.3, 0.4,0.5, 0.6, 0.7)) +  xlab("Day of Year") + ylab("Year")
```




#Checking the Data to Gain a Better Understanding of the Observed Patterns#


```{r}
#Getting estimates for specific values 
library(mgcv)
  newdata<-data.frame(PRCP.7day= c(10, 20, 30), TMAX.7day=(-10:100), Year.x =(2011:2018), Volunteer.x=(1:300), latitude=(-200:200), longitude=(-200:200))
  
  data<-YR_MHD_weather_hardi2
  
  predict(TXPa_gam,newdata=data(PRCP.7day=xn[1],x1=xn[2],
        x2=xn[3],x3=xn[4]),se=TRUE) 
predict.gam(TXPa_gam, newdata, type = "response")

, values = list())


newdata <- data.frame(PRCP.7day= c(10, 20, 30))
 newdata$estimate <-predict(TXPa_gam, newdata)
```


Graphing Extreme Values 
```{r}
#According to the interaction figure extreme values where an effect is observed are as follows:
  #PRCP >100
  #TMAX >30 
  #TMAX <15 

coldwet<- filter(data, TMAX2 < "20", PRCP2 > "5", Infection =="1")
hotdamp<- filter(data, TMAX2 > "30", PRCP2 > "10")

hotdamp<- filter(hotdamp,  PRCP <"45", Infection =="1")


TMAXfiltered<- filter(TMAXfiltered,  PRCP.7day < "50")
TMAXfiltered<- filter(TMAXfiltered,  Infection.x=="1")
#try filtering PRCP >60 and Temp below 0 

5283 just TMAX 
491 plus PRCP 

, Infection.x=="1"
write.csv(TMAXfiltered, file="ColdWetALL.csv")

TMAXfiltered %>% group_by(Sex) %>% tally()
TMAXfiltered %>% group_by(State ) %>% tally()

TMAXfiltered<- filter(PXT_7day, TMAX.7day > "30", PRCP.7day > "30", Infection.x=="1")

write.csv(TMAXfiltered, file="HotWet.csv")

#Figure out average infection prevalence by state 
= 84% in Sept 
= 66% in Oct 

= 78% in November 
= 63% in December 
= 75% in January 

= 90% in May 
= 88% in June 
= 81% in July 
= 89% in August 





TMAXfiltered<- filter(PXT_7day, State.x=="FL", Month.x=="10")
TMAXfiltered<- filter(TMAXfiltered,  PRCP.7day < "50")
TMAXfiltered<- filter(TMAXfiltered,  Infection.x=="1")
TMAXfiltered %>% group_by(Infection.x) %>% tally()

```


```{r}
#GAMM for individual states 
FLdata<- filter(data, State=="FL")


gam_modFL <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = FLdata)

summary(gam_modFL)
plot()
plot(gam_modFL, pages = 1,  trans = plogis,
     shift = coef(gam_modFL)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```


```{r}
#GAMM for individual states 
LAdata<- filter(data, State=="LA")


gam_modLA <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = LAdata)

summary(gam_modLA)
plot(gam_modLA, pages = 1,  trans = plogis) 
, seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")


```

```{r}
#GAMM for individual states 
TXdata<- filter(data, State=="TX")


gam_modTX <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = TXdata)

summary(gam_modTX)
plot(gam_modTX, pages = 1,  trans = plogis,
     shift = coef(gam_modTX)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```


```{r}
#GAMM for individual states 
GAdata<- filter(data, State=="GA")


gam_modGA <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = GAdata)

summary(gam_modGA)
plot(gam_modGA, pages = 1,  trans = plogis)
     shift = coef(gam_modGA)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```


```{r}
#GAMM for individual states 
NCdata<- filter(data, State=="NC")


gam_modNC <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = NCdata)

summary(gam_modNC)
plot(gam_modNC)

```


```{r}
#GAMM for regions: Coastal TX and LA 
Coastaldata<- filter(data, latitude< "32", longitude >= "-88")


gam_modCoastaldata <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Coastaldata)

summary(gam_modCoastaldata)
plot(gam_modCoastaldata)
plot(gam_modCoastaldata, pages = 1,  trans = plogis)
     shift = coef(gam_modCoastaldata)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")


```





```{r}
#GAMM for regions: South Florida  
library(dplyr)
library(mgcv)


data$Volunteer<-as.factor(data$Volunteer)

SFldata<- filter(data, latitude< "28", longitude <= "-82")


gam_modSFldata <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = SFldata)

gam_modSFldata <- gam(Infection ~  s(Year, k=8)  + s(Day_Year, bs = "cc") + ti(Day_Year, bs = "cc", by = Year, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = SFldata)





gam_modTIME <- gam(Infection.Status ~  s(Day_Year, bs = "cc") + s(Year,  k = 8) + ti(Day_Year, bs = "cc", by = Year, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)



summary(gam_modSFldata)
plot(gam_modSFldata)

plot(gam_modSFldata, pages = 1,  trans = plogis)


```



Hardiness Zone Analysis 
```{r}
#GAMM for all hardiness zones 
library(mgcv)
library(plyr)
data$zone2<-as.factor(data$zone)
data$Volunteer<-as.factor(data$Volunteer)

data$zone2<- revalue(x = data$zone2, 
c("5a"= "5", "6a"= "6" , "6b"= "6" , "7b"="7", "7a"= "7", "8a"="8", "8b" ="8" ,  "9a"= "9" ,"9b" ="9",  "10a"= "10", "10b" ="10", "11a" = "11" ))


data$zone2<-as.numeric(factor(data$zone2, levels = c("5", "6", "7", "8", "9", "10", "11"))) 


gam_modHardi <- gam(Infection ~  s(zone2, k=4) + s(Year, k=8)  + + s(Day_Year, bs = "cc")+ s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

summary(gam_modHardi)
plot(gam_modHardi, trans=plogis)


library(lme4) 
modehardi<-glmer(Infection~ zone2+ Year + (1|Volunteer), data=data,  family = binomial(link="logit"))
```



```{r}
library(ggplot2)
library(tidyverse)
library(viridis)

data$zone<-as.factor(data$zone)


  ggplot( data=data, aes(x=zone2, y=Infection)) +
   stat_boxplot(geom ='errorbar', width = 0.6) +
     geom_boxplot() +
  stat_summary(fun.y=mean, geom="point", shape=20, size=14, color="red", fill="red")+
    xlab("")
  
  
  
  
  my_sum <- data %>%
  group_by(zone2) %>%
  summarise( 
    n=n(),
    mean=mean(Infection),
    sd=sd(Infection)
  ) %>%
  mutate( se=sd/sqrt(n))  %>%
  mutate( ic=se * qt((1-0.05)/2 + .5, n-1))
  
  
  ggplot(my_sum) +
  geom_bar( aes(x=zone2, y=mean), stat="identity", fill="forestgreen", alpha=0.5) +
  geom_errorbar( aes(x=zone2, ymin=mean-sd, ymax=mean+sd), width=0.4, colour="orange", alpha=0.9, size=1.5) +
  ggtitle("using standard deviation")
  
  
means <- aggregate(Infection~zone2,
                      data = data,
                      FUN = mean)

se <- aggregate(Infection~zone2,
                      data = data,
                      FUN = se)  

#Merge standard error and means into one column 
means["stderror"] <- sd$Infection  

 rownames(sd) <- paste("sd", means$standard)
 
 sd$stderror <- sd$Infection 
 sd$stderror <- sd$Infection/sqrt(n())
 
  means$zone2<-as.factor(means$zone2)
  
  ggplot(means) +
    
    
        geom_bar( aes(x=zone2, y=Infection), stat="identity", fill="skyblue", alpha=0.7) +
    scale_x_discrete(breaks=1:7, labels=c("5" , "6" ,"7", "8", "9", "10", "11" ))+
   geom_errorbar(aes(x=zone2, ymin=Infection-stderror, ymax=Infection+stderror), width=0.4, colour="orange", alpha=0.9, size=1.3)

```


Linear Test of Time and Infection Prevalence to support GAM observation of increasing prevalence over time 
```{r}
data$Year<-as.factor(data$Year)
library(lme4) 
lmodTime<-glmer(Infection~  Year + (1|Volunteer), data=data,  family = binomial(link="logit"), glmerControl(optimizer ="bobyqa", optCtrl=list(maxfun=2e5)))


```
```{r}
Time_plot <- ggplot() + 
  #2
geom_point(data=subset(data), aes(Year, Infection)  
  #3
  #geom_point(data=data, aes(x=c.urchinden, y=fit), color="blue") +
  #4
  #geom_line(data=x_urch, aes(x=c.urchinden, y=fit), color="blue") +
  #5
  #geom_ribbon(data= x_urch, aes(x=c.urchinden, ymin=lower, ymax=upper), alpha= 0.3, fill="blue") +
  #6
  #labs(x="Urchins (centered & scaled)", y="Coral Cover")

```


Regional Analysis 
```{r}

library(tidyr)

#Using ggplot to control details of contour plot 
 Time_pred <- expand_grid(
  Day_Year = seq(from=1, 
              to=365, length.out = 300),
  Year = seq(from=min(data$Year), 
              to=max(data$Year), 
              length.out = 8), 
   Volunteer = 1, 
              length.out = 1)
  
library(dplyr)
Time_pred <- predict(gam_modSFldata, newdata = Time_pred, se.fit = TRUE) %>%  
  as_tibble() %>% 
  cbind(Time_pred)


as_tibble(Time_pred)
  
cbind(Time_pred)

dim(Time_pred)   

#Time_pred<-as.data.frame(Time_pred)   

```

```{r}
#Time_pred <- predict(gam_modTIME, newdata = Time_pred)
                      

#dim(Time_pred)   

#Turn log-odds ration into probability 
Time_pred$fit<-plogis(Time_pred$fit)


#Determine the min/max of the fit column 
summary(Time_pred$fit)

#Graph using ggplot 
Time_pred$Year<-as.factor(Time_pred$Year)

ggplot(Time_pred)+ aes(x=Day_Year, y=fit) +  
    #geom_smooth(data = Time_pred, method = "gam", formula = y ~ s(x, bs = "cc"))  +
    geom_point() + 
    facet_wrap(~Year)

cp+ scale_fill_distiller(palette = "Spectral", name = "Infection Prevalence", breaks=c(0.2,0.3, 0.4,0.5, 0.6, 0.7), labels=c(0.2,0.3, 0.4,0.5, 0.6, 0.7)) +  xlab("Day of Year") + ylab("Year")


ggplot(data)+ aes(x=Day_Year, y=Infection) +  
    geom_smooth(data = data, method = "gam", formula = y ~ s(x, bs = "cc"))  +
    #geom_point() + 
    facet_wrap(~Year)



```



Creating Regions Based on Latitude/Longitude Hotspots from Spatial Work and Hardiness Zones

Zone 10: Very far south Florida 
```{r}
#Filter out south Florida hardiness zone (zone 10) 
data$zone<-as.factor(data$zone)
library(tidyverse)
#& would be and (so filter one and then filter other) and the bar is both at the same time "or"
Zone1011data<- filter(data, zone == "10a" | zone == "10b"| zone == "11a"| zone == "11b")




Zone1011data$Volunteer<-as.factor(Zone1011data$Volunteer)

library(mgcv)

gam_modSFldata <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone1011data)


plot(gam_modSFldata, trans=plogis, ylim=c(0, 1))
axis(2)



gam_modSFLdata_year <- gam(Infection.Status ~ s(Day_Year, bs = "cc")+ s(Year, k= 7)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone1011data)
AIC(gam_modSFLdata_year )
plot(gam_modSFLdata_year, trans=plogis, ylim=c(0, 1))






gam_modSFLdata_TEMP <- gam(Infection ~ s(Day_Year, bs = "cc")+ s(Year, k= 7) + s(TMAX2) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone1011data)
AIC(gam_modSFLdata_TEMP)
plot(gam_modSFLdata_TEMP, trans=plogis, ylim=c(0, 1))



ggplot(Zone10data)+ aes(x=Day_Year, y=(Infection)) +  
    geom_smooth(data = Zone10data, method = "gam", formula = plogis(y) ~ s(x, bs = "cc")) 
    #geom_point() + 
    #facet_wrap(~Year)
```



Zone 9: Florida 
```{r}
#Filter out south Florida hardiness zone (zone 9) 
Zone9FL<-filter(data, zone == "9a" | zone == "9b") 
Zone9FL<-filter(Zone9FL, State == "FL")


library(mgcv)

Zone9FL$Volunteer<-as.factor(Zone9FL$Volunteer)


gam_modFldata <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone9FL)


plot(gam_modFldata, trans=plogis, ylim=c(0, 1))






gam_modFLdata_year <- gam(Infection ~ s(Day_Year, bs = "cc")+ s(Year, k= 7)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone9FL)
AIC(gam_modFLdata_year)
#919.54
plot(gam_modFLdata_year, trans=plogis, ylim=c(0, 1))




gam_modFLdata_TEMP <- gam(Infection ~ s(Day_Year, bs = "cc")+ s(Year, k= 7) + s(TMAX2) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone9FL)
AIC(gam_modFLdata_TEMP)
#905.08
plot(gam_modFLdata_TEMP, trans=plogis, ylim=c(0, 1))


ggplot(Zone9FL)+ aes(x=Day_Year, y=plogis(Infection)) +  
    geom_smooth(data = Zone9FL, method = "gam", formula = (y) ~ s(x, bs = "cc"))  
    #geom_point() + 
    #facet_wrap(~Year)
```



Zone 9:Coastal TX and Louisiana 
```{r}
#Filter out south Florida hardiness zone (zone 10) 
Zone9GC<-filter(data, zone == "9a" | zone == "9b") 
Zone9GC<-filter(Zone9GC, State == "LA" | State == "TX")




Zone9GC$Volunteer<-as.factor(Zone9GC$Volunteer)


gam_modGCdata <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone9GC)


plot(gam_modGCdata, trans=plogis, ylim=c(0, 1))


gam_modGCdata_year <- gam(Infection ~  s(Year, k= 7)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone9GC)
plot(gam_modGCdata_year, trans=plogis, ylim=c(0, 1))



gam_modGCdata_year <- gam(Infection ~ s(Day_Year, bs = "cc")+ s(Year, k= 7)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone9GC)
AIC(gam_modGCdata_year)
#3630




gam_modGCdata_TEMP <- gam(Infection ~ s(Day_Year, bs = "cc")+ s(Year, k= 7) + s(TMAX2) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone9GC)
AIC(gam_modGCdata_TEMP)
#3601.08
plot(gam_modGCdata_TEMP, trans=plogis, ylim=c(0, 1))





ggplot(Zone9GC)+ aes(x=Day_Year, y=plogis(Infection)) +  
    geom_smooth(data = Zone9GC, method = "gam", formula = y ~ s(x, bs = "cc"))  
    #geom_point() + 
    #facet_wrap(~Year)
```


Zone 8: North Carolina, South Carolina and Georgia 
```{r}
#Filter out south Florida hardiness zone (zone 10) 
Zone8East<-filter(data, zone == "8a" | zone == "8b") 
Zone8East<-filter(Zone8East, State == "NC" | State == "SC"| State == "GA")







ggplot(Zone8East)+ aes(x=Day_Year, y=plogis(Infection)) +  
    geom_smooth(data = Zone8East, method = "gam", formula = plogis(y) ~ s(x, bs = "cc"))  +
    geom_point() + 
    facet_wrap(~Year)
```

Zone 8: Mississippi, Alabama, Louisiana, Texas 

```{r}
Zone8CenterWest<-filter(data, zone == "8a" | zone == "8b") 
Zone8CenterWest<-filter(Zone8CenterWest, State == "MS" | State == "AL"| State == "LA"| State == "TX")


ggplot(Zone8CenterWest)+ aes(x=Day_Year, y=plogis(Infection))+  
    geom_smooth(data = Zone8CenterWest, method = "gam", formula = plogis(y) ~ s(x, bs = "cc"))  +
    geom_point() + 
    facet_wrap(~Year)
```
Zone 8: ALL
```{r}
Zone8ALL<-filter(data, zone == "8a" | zone == "8b") 
Zone8CenterWest<-filter(Zone8CenterWest, State == "MS" | State == "AL"| State == "LA"| State == "TX")



Zone8ALL$Volunteer<-as.factor(Zone8ALL$Volunteer)


gam_mod8data <- gam(Infection ~  s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone8ALL)


plot(gam_mod8data, trans=plogis, ylim=c(0, 1))


ggplot(Zone8ALL)+ aes(x=Day_Year, y=plogis(Infection))+  
    geom_smooth(data = Zone8ALL, method = "gam", formula = plogis(y) ~ s(x, bs = "cc")) 
    #geom_point() + 
    #facet_wrap(~Year)






gam_mod8data_TEMP <- gam(Infection ~ s(Day_Year, bs = "cc")+ s(Year, k= 7) + s(TMAX2) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone8ALL)
AIC(gam_mod8data_TEMP)
#1819.517
plot(gam_mod8data_TEMP, trans=plogis, ylim=c(0, 1))







gam_mod8data_year <- gam(Infection.Status ~  s(Day_Year, bs = "cc")+ s(Year, k= 7)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = Zone8ALL)
AIC(gam_mod8data_year)
#2140.986
plot(gam_mod8data_year, trans=plogis, ylim=c(0, 1))

```





Does south Florida have higher overall prevalence? 
```{r}
```{r}
#First, filter data to July data only to obtain the infection prevalence during the peak month of July 
data$Month<-as.factor(data$Month)

library(dplyr)
JulyData<- filter(data, Month == "7")
AprilData<- filter(data, Month == "4")
Zone9FL

#Now use plyr to calculate the average infection prevalence per city and year in July  
library(plyr)
JulyData$Infection<-as.numeric(JulyData$Infection)

FLAvgData<-ddply(Zone9FL,.(Year), summarize, Avg=mean(Infection))
GCAvgData<-ddply(Zone9GC,.(Year), summarize, Avg=mean(Infection))
All8AvgData<-ddply(Zone8ALL,.(Year), summarize, Avg=mean(Infection))
Fl10AvgData<-ddply(Zone10data,.(Year), summarize, Avg=mean(Infection))



AprilData$Infection<-as.numeric(AprilData$Infection)

AprilAvgData<-ddply(AprilData,.(Year, City, State), summarize, Avg=mean(Infection-1))
AprilAvgData

#Merge JulyAvgData and the table with the date of leaving Mexico ("SpringMigrationYear")

JulyAvgMigration<-left_join(JulyAvgData, SpringMigrationYear, by=c("Year"))

AprilAvgMigration<-left_join(AprilAvgData, SpringMigrationYear, by=c("Year"))



library(ggplot2)
ggplot(data=JulyAvgMigration, aes(x=as.numeric(DayYear), Avg))+geom_point() + geom_smooth()

ggplot(data=AprilAvgMigration, aes(x=as.numeric(DayYear), Avg))+geom_point() + geom_smooth()


```
```








Correlations between Infection and Spring Migration 

```{r}
#First, filter data to July data only to obtain the infection prevalence during the peak month of July 
data$Month<-as.factor(data$Month)

library(dplyr)
JulyData<- filter(data, Month == "7")
AprilData<- filter(data, Month == "4")


#Now use plyr to calculate the average infection prevalence per city and year in July  
library(plyr)
JulyData$Infection<-as.numeric(JulyData$Infection)

JulyAvgData<-ddply(JulyData,.(Year, City, State), summarize, Avg=mean(Infection-1))
JulyAvgData



AprilData$Infection<-as.numeric(AprilData$Infection)

AprilAvgData<-ddply(AprilData,.(Year, City, State), summarize, Avg=mean(Infection-1))
AprilAvgData

#Merge JulyAvgData and the table with the date of leaving Mexico ("SpringMigrationYear")

JulyAvgMigration<-left_join(JulyAvgData, SpringMigrationYear, by=c("Year"))

AprilAvgMigration<-left_join(AprilAvgData, SpringMigrationYear, by=c("Year"))



library(ggplot2)
ggplot(data=JulyAvgMigration, aes(x=as.numeric(DayYear), Avg))+geom_point() + geom_smooth()

ggplot(data=AprilAvgMigration, aes(x=as.numeric(DayYear), Avg))+geom_point() + geom_smooth()


```





Look at hot spots 
```{r}
TXMigration<- filter(JulyAvgMigration, State == "FL")

ggplot(data=TXMigration, aes(x=as.numeric(DayYear), Avg))+geom_point() + geom_smooth()


FlMigration<- filter(AprilAvgMigration, State == "FL")

ggplot(data=FlMigration, aes(x=as.numeric(DayYear), Avg))+geom_point() + geom_smooth()

```



Model to investigate 
```{r}
lm_migration <- lm(Avg ~ Year ,  data = AprilAvgData)
summary(lm_migration)

require(pscl)
lm_migration <- zeroinfl(Avg ~ Year ,  data = AprilAvgData)

zeroinfl
```



Explore Hardiness Zone as a Variable 
```{r}
#GAMM for regions: South Florida  
data$zone2<-as.factor(data$zone)


data$Volunteer<-as.factor(data$Volunteer)

summary(data$zone2)

library(plyr)

data$zone2<- revalue(x = data$zone2, 
c("5a"= "5", "6a"= "6" , "6b"= "6" , "7b"="7", "7a"= "7", "8a"="8", "8b" ="8" ,  "9a"= "9" ,"9b" ="9",  "10a"= "10", "10b" ="10", "11a" = "11" ))

data$zone2<-as.numeric(factor(data$zone2, levels = c("5", "6", "7", "8", "9", "10", "11"))) 

summary(data$zone2)

library(mgcv)
gam_modZONE <- gam(Infection ~ s(zone2, k=6) +s(Day_Year, bs = "cc")  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)



summary(gam_modZONE)
plot(gam_modZONE)

plot(gam_modZONE, pages = 1,  trans = plogis)
```








