---
title: "GAM_MHD"
author: "Christen"
date: "March 25, 2020"
output:
  html_document:
    df_print: paged
---

GAM Work with Monarch Health Data



*Overall Research Objectives*
1.	Determine how OE infection prevalence changes seasonally (within a year) and across years in the southeastern U.S. 

2.	Determine where OE infection prevalence is most prevalent within each year and how the spatial patterns of infection have changed over time? 

3.	Determine whether climatic variables are correlated with the spatial and temporal changes in OE infection prevalence in the southeastern U.S.

*Why use a GAM?*
We do not expect the patterns in disease prevalence as described by our predictor variables to be linear. For example, infection prevalence may experience increases during periods of warm weather such as in late fall and early spring, while mid winter low temperatures may result in a decline in infection prevalence as population density and therefore transmission potential declines. Graphs of the dataset are provided below for visual inspection of the linear or non-linear trends.  

*Dataset*
```{r}
#The below dataset should be used to examine winter OE data  
#wdata <- read.csv("./data/MHD_WD.csv", colClasses = c("Pop_density_CAT" = "factor", "Hardiness2" = "factor", "Infection_Severity" = "factor", "Month"= "factor", "Year"= "factor"))

#The dataset below should be used to examine annual OE data
data<- read.csv("./data/YR_MHD.csv")

summary (data)
```


*Exploration of the Data through Graphing*
Graph the relationship between infection and day of year to visually inspect the relationship for linearity. 
From this figure it is hard to discern any clear pattern. There seems to be a mostly random distribution of infection prevalence across all year (by day of year). #Note: Day 200 is mid-July 
```{r}
#Convert variables into different forms for graphing purposes
data$Day_Year<-as.factor(data$Day_Year)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
library(plyr)
r2<-ddply(data,.(Day_Year), summarize, mean=mean(Infection))
r3<- (r2$mean)
r2$r3 <- r3

library(ggplot2)
ggplot(data=r2, aes(x=as.numeric(Day_Year),shape =  Year.Sampled, r3))+geom_point() + geom_smooth()
```

Graph the relationship between infection and year to visually inspect the relationship for linearity. 
```{r}
#Convert variables into different forms for graphing purposes
##data$Season<-revalue(data$Season, c("11_12"="1", "12_13"="2", "13_14"="3", "14_15"="4", "15_16"="5", "16_17"="6", "17_18"="7"))

data$Year.Sampled<-as.factor(data$Year.Sampled)
data$Infection<-as.numeric(data$Infection)


#Calculate the probability of infection on each day of year 
library(plyr)
s1<-ddply(data,.(Year.Sampled), summarize, mean=mean(Infection))
s2<- (s1$mean)
s1$s2 <- s2

#Plot 
plot(as.numeric(s1$Year.Sampled), s1$
       s2, xlab = "Year", ylab = "Probability of Infection", type='o')
```


Graph the spatial distribution of the data.  
```{r}
library("maps")
library("dplyr")
library("ggplot2")

states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

gg<-ggplot() + 
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group), fill="grey", colour = "white")+
   geom_point(data=data, aes(x=Longitude, y=Latitude, color = factor(Infection)), position = "jitter", size=1)+
  scale_colour_manual(values = c("1"="chocolate3", "0"= "cyan"))+
 
 # geom_point(data=data, aes(x=Longitude, y=Latitude),size=1, colour = "black")
  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))+
   xlab("Longitude") + ylab("Latitude")

  gg + theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
```


*Objective 1: Analyzing the Temporal Patterns in OE Infection Prevalence in the SE*
  
  *Hypothesis 1: Within Year*
If OE infection prevalence is primarily driven by reproductive behavior and breeding is  occurring year-round in the southeast, then  OE infection prevalence may remain high, showing little correlation with day of year. 

  *Hypothesis 2: Between Years*
  If the population of winterbreeding monarchs in the southeastern US is growing (and therefore becoming more dense), we expect that OE infection prevalence may increase with year as increased monarch density leads to higher rates of transmission.
  
*Model Construction*
This model is designed to characterize temporal infection patterns in the southeast by including day of year, year and an interaction of these two terms in a single model. 

Details on the model:
1. Why "ti"?
  Tensor product smooths are used to model interactions between variables that have different natural scales. They multiply the smooth terms of one variable by the factor levels of the other. 

2. Why "by"?
  "By" variables are used for constructing "varying coefficient models" and for letting smooths interact with parametric terms. In this case our smooth is the Day_Year variable and the Season term is the parametric term (and a factor). 
The interaction between two parametric terms would use the : or * connotation, but smooth interactions cannot. "By" generates an indicator vector for each level of a factor  unless the factor is ordered. If it is ordered then a different smooth is generated for each fator level (except the first level). 


3. Why k=6? 
The term k is also referred to as "knots". These are the natural breaks in the basis functions that make up the "basis" or "smooth function" or "spline". Where the knots break along the spline is usually defined by quantiles. K is usually the max number of degrees of freedom allowed for a smooth term in the model. K in the models will always =  (max degrees of freedom - 1). In this case 

4. Why "family=binomial"? 
The response variable is either 0 (uninfected) or 1 (infected), this we have 2 curves for our response variable distribution and this requires that the model be fit with a binomial family distribution. 

5. Why is "Season" parametric?


6. Why is Volunteer "bs=re"? 
In order to include Volunteer as a random effect, we tell the model to use the random effect basis function to model this variable. This method only applies to mcgv, as a different method is used for gamm. 

7. Why REML? 

8. Should I be using a different basis function? Cubic? 

```{r eval, = FALSE if exists (gam_modIF.Rdata) }
require(mgcv)

###Make sure each variable is in the correct format 
data$Day_Year<-as.numeric(data$Day_Year)
data$Year.Sampled<-as.numeric(data$Year.Sampled)
data$Infection<-as.factor(data$Infection)

gam_modTIME <- gam(Infection ~  s(Day_Year, bs = "cc") + s(Year.Sampled,  k = 8) + ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

save(gam_modTIME, file="gam_modII.Rdata")

summary(gam_modTIME)



```



The following models are the different iterations that eventually led to the final model selected above. They change terms in the model from smooth to linear or investigate the way the interaction term is used. 
```{r eval, = FALSE if exists (gam_modIF.Rdata) }

# #gam_mod: Model with Year as a linear effect
# gam_mod <- gam(Infection ~  Year.Sampled + ti(Day_Year, by = Year.Sampled, k =6) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# summary(gam_mod) 
# save(gam_mod, file="gam_mod.Rdata")
# 
# 
# ###gam_modSS: Fit model with Year smoothed (numeric)
# 
# data$Year.Sampled<-as.numeric(data$Year.Sampled)
# 
# gam_modSS <- gam(Infection ~  s(Year.Sampled, k = 8) + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# summary(gam_modSS)
# save(gam_modSS, file="gam_modSS.Rdata")
# 
# 
# ###Fit model with Day_Year as an added  individual effect with both day_year and year smoothed
# 
# #Running Both as numeric variables
# data$Year.Sampled<-as.numeric(data$Year.Sampled)
# data$Day_Year<-as.numeric(data$Day_Year)
# 
# gam_modIY <- gam(Infection ~  s(Day_Year) + s(Year.Sampled, k = 8) + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modIY, file="gam_modIY.Rdata")
# summary(gam_modIY)
# 
# 
# ###Fit model with Day_Year as an added  individual effect with year as a factor
# data$Year.Sampled<-as.factor(data$Year.Sampled)
# data$Day_Year<-as.numeric(data$Day_Year)
# 
# 
# gam_modIF <- gam(Infection ~  s(Day_Year) + Year.Sampled + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modIF, file="gam_modIF.Rdata")
# summary(gam_modIF)
# 
# 
# #WHen comparing all 4 models:
# #gam_modIF = 12266.17
# #gam_modIY = 12813.5. 
# #gam_mod =   12317.86
# #gam_modSS = 12859.28
# AIC(gam_modIY, gam_modIF, gam_mod, gam_modSS)
# 
# 
# #Model without "by" as the interaction term# 
# gam_modNoby <- gam(Infection ~  s(Day_Year, bs = "cc") + s(Year.Sampled,  k = 8) + ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modNoby, file="gam_modNoby.Rdata")
# 
# AIC(gam_modIY, gam_modIF, gam_mod, gam_modSS, gam_modII)

#Could it be that the AIC value is higher when year is a factor because it is more closely fitting the data, but it is not answering the question we have about Year, which is how does infection change OVER TIME? WIth year as a factor, the question seems to be how does infection change from year to year or within a single year. 

# gam_modIY	264.3734	14116.17		
# gam_modIF	278.3352	13646.39		
# gam_mod	267.6086	  12317.86		
# gam_modSS	242.2135	12859.28		
# gam_modII	295.0423	13675.55	
#with te added: gam_modII	292.4565	13674.03	(without "by")
#with Day_Year as cyclic: 13671.31 (without "by")
#with Day as cyclic and Year as cubic: 13671.31 (so not needed) (without "by")
# "by" added back in 14281.25
#remove "Te"

# #Model with only interaction term for contour plot#
# gam_modCC <- gam(Infection ~   ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

```


*Summarize output of model*
```{r}
summary.gam(gam_modTIME) 

#For standard deviations and confidence intervals 
gam.vcomp(gam_modTIME)
```
Key points from model output: 

The model output summary tells us that the additive temporal variables explain 42.5% of the deviance.We also can convert the outputs from the model to actual probabilities because the GAM model uses a log-odds scale for estimating outputs. After converting the intercept estimate below we find that the model predicts a 44.1% chance of infection overall. 
```{r}
plogis(-0.2373)
```


*Checking the Model*
```{r}
#gam.check(gam_modTIME)

#gam.check does not seem to be the best method for visualizing logistic models

library(arm)

binnedplot(fitted(gam_modTIME), 
           residuals(gam_modTIME, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           main = "S Year", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")
```


*Plot the results of the temporal model* 
Plots of the partial effects of the interaction fo Day_Year and Season for each level of Season. 

```{r}
require(mgcViz)
require(mgcv)

#Year.Sampled = c("2011-2012", "2012-2013", "2013-2014", "2014-2015", "2015-2016", "2016-2017", "2017-2018")
#axis(, at=1:8, xaxp=letters[2011:2018])

#Version1
plot(gam_modII, pages = 1,  trans = plogis,
     shift = coef(gam_modIY)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

#Version2
b <- getViz(gam_modII, trans = plogis)

print(plot(b, allTerms = T))
```

Visualization of the interaction between Year and Day_Year in a 3D plot. 
```{r}

vis.gam(gam_modII, n.grid = 50, theta = 400, phi = 20, ylab = "Day of Year", xlab = "Year", zlab = "",
        ticktype = "detailed", color = "terrain", main = "Interaction between day of year and season")



##theta= rotates the view around horizontally 
##phi = rotates the view vertically 
##n.grid =  controls the number of grid boxes
##z scale = GAM model predictions 

```

Visualization of the interaction between Year and Day_Year in a contour plot.
```{r}
require(mgcv)

vis.gam(gam_modTIME, view=c("Day_Year","Year.Sampled"),plot.type="contour", color = "terrain", contour.col = "black") 

#The contour lines represent points of equal predicted values, and they are labeled. The dotted lines show uncertainty in prediction; they represent how contour lines would move if predictions were one standard error higher or lower.

#Run a model without an interaction to help understand the contour plot 
  #gam_modNI <- gam(Infection ~  s(Day_Year) + s(Year.Sampled, k = 8)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
  #save(gam_modNI, file="gam_modNI.Rdata" )
  #vis.gam(plog, view=c("Day_Year","Year.Sampled"),plot.type="contour", color = "terrain", contour.col = "yellow") 


devtools::install_github('gavinsimpson/gratia')
library('gratia')
mod <- gam(y ~ s(x, z, k = 30), data = dat$data, method = "REML")

mod <- gam(Infection ~   ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)


draw(gam_modII)



```


Summary of plot results: 

*Spatio-temporal Model Example* 

Joint models of space and time. 

Should an interaction between space and time be included? This is compared here and the interaction is found to be significant and produces a significantly lower AIC value compared to the non-interaction term model. 
```{r}
data$Year.Sampled<-as.numeric(data$Year.Sampled)


#First, run a model without an interaction between space and time as done in the example. By comparing this model to the model with a space-tie interaction, we will be able to determine the relative importance of the interaction term. 

STno = gam(Infection~ s(Longitude_city,Latitude_city) + s(Year.Sampled, k = 8) + s(Volunteer, bs="re"),
                          data=data,family=binomial,method="REML")

save(STno, file="STno.Rdata")
summary(STno)
plot(STno,scheme=2)


#Now run the model with the interaction term 

STyes = gam(Infection~ s(Longitude,Latitude) + s(Year.Sampled, k = 8)+ ti(Longitude, Latitude, Year.Sampled, d=c(2,1)) + s(Volunteer, bs="re"),
                          data=data,family=binomial,method="REML")
save(STyes, file="STyes.Rdata")
summary(STyes)
plot(STyes,scheme=2)

load("STyes.Rdata")
AIC(STyes)


```
WHile including the interaction term improves the fit of the model, it doesnt make it easy to determine where we are seeing 1) OE hotspots within each year 2) the greatest change in hotspots over time. 

We will next use the "predict" function from mgcv to better visualize the output of the STyes model. 


Graphing of space-time interactions 


We need to select a volunteer whose observations represent the average infection prevalence found by all volunteers. To do this, we will first determine the average infection prevalence per volunteer and then select the volunteer whose average is closest to this number. 
```{r}
#First use plyr to calculate the average per volunteer 
library(plyr)
r2<-ddply(data,.(Volunteer), summarize, Avg=mean(Infection))
r2
#We now have a table r2, with the average infection prevalence per volunteer for all 269 volunteers 
```

We can visualize the distribution of infection prevalence per volunteer. 
```{r}
library(ggplot2)
ggplot(data=r2, aes(x=as.factor(Volunteer), Avg)) + geom_point()+ geom_smooth()

hist(r2$Avg)
```
Next we summarize the r2 dataframe to determine the overall mean per volunteer. 
```{r}
summary(r2)

#The mean in 0.4681
```
Select the volunteers with an average closest to the average of 0.4681
```{r}

Average_Volunteers <- r2[r2$Avg >= 0.4 & r2$Avg < 0.5, ]
Average_Volunteers$Differ<-0.4681-Average_Volunteers$Avg
Average_Volunteers

#We find that 2 volunteers Ana Maria Agrusa (13) and Laninda Sande (154) have averages closest to the overall average (0.4615385), therefore we can use either volunteer in the model 
```


Create the background map on which the model predictions will be mapped 
```{r}
library("maps")
library("dplyr")
library("ggplot2")


states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

```

We can now move on to creating a grid of predicted infection prevalence using the average observation values of volunteer #13.  
```{r}
data$Latitude<-as.numeric(data$Latitude)
data$Longitude<-as.numeric(data$Longitude)

se_map$Latitude<-as.numeric(se_map$lat)
se_map$Longitude<-as.numeric(se_map$long)

data$Year.Sampled<-as.factor(data$Year.Sampled)

#First we'll create gridded data

predict_infection = expand.grid(
  Latitude= seq(min(data$Latitude), 
                max(data$Latitude),
                length=100),
  Longitude = seq(min(data$Longitude),
                  max(data$Longitude),
                  length=100),
  Volunteer = levels(data$Volunteer)[13],  
  Year.Sampled = seq(2011,2018,by=1))

#head(predict_infection)

#summary(data)
```

Create the Model Fit Column 
```{r}

predict_infection$model_fit = predict(STyes,
                                    predict_infection,type = "response")

```

Create a for loop to build polygons for each group in the dataset 
```{r}
library(ggplot2)
library(viridis)
library(dplyr)

se_map<-se_map %>% mutate(Longitude=long,Latitude=lat)
predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)

for(g in 1:length(groups))
{
  temp<-predict_infection[with(predict_infection %>% dplyr::select("Longitude","Latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("Longitude","Latitude"),Longitude,Latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}

library(mapproj)
library(maps)
library(ggrepel)

#Process for Making cities labels 
data(us.cities)
capitals <- subset(us.cities)
capitals$city <- sub(' [^ ]*$','',capitals$name) # split out city for the label

plotcities <- subset(world.cities, name %in% c("Atlanta", "Houston", "New Orleans", "Austin", "San Antonio", "Tampa", "Orlando", "Miami")) %>% filter(country.etc=="USA")


 
```

Graph the data
```{r}
gg<-ggplot(aes(Longitude, Latitude,  fill= model_fit), 
       data=predict_infection_clipped)+ #%>% filter(Year.Sampled==2011))+ 
  geom_tile()+
  facet_wrap(~Year.Sampled,nrow=4)+
  scale_fill_viridis("Infection Probability")+
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+
  
  # geom_point(x = -84.42, y = 33.76, colour="red", size = 1)+
  # annotate(geom = "text", x = -84.42, y = 33.5, label = "Atlanta", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -95.39, y = 29.77, colour="red", size = 1)+
  # annotate(geom = "text", x = -95.39, y = 29.5, label = "Houston", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -89.93, y = 30.07, colour="red", size = 1)+
  # annotate(geom = "text", x = -88.00, y = 30.1, label = "New Orleans", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -82.48, y = 27.96, colour="red", size = 1)+
  # annotate(geom = "text", x = -83.7, y = 27.88, label = "Tampa", 
  #   fontface = "bold", color = "white", size = 1)+

  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))

  g<-gg+ theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
g
  
save_plot("~/g.png", plot_tmp, base_height = NULL, base_aspect_ratio = 1.618, 
base_width = 6)

#library(OpenImageR)

#img<-OpenImageR::readImage("~/plot_tmp.png")
#imageShow(img)
 
```


Add space by day of year may be the next step in the analysis in order to see if infection changes in different regions through time
It seems like anytime I seem to know what I ma talking about she rears up 
We need to have a story with this. Especially from the spatiotemporal data. Think about story for publication 


Calculate the difference from year to year 
```{r}
predict_infection$model_change =predict(STyes,
                                      predict_infection%>%mutate(Year.Sampled=Year.Sampled+1),
                                      type = "response") - predict_infection$model_fit 



se_map<-se_map %>% mutate(Longitude=long,Latitude=lat)

predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)

for(g in 1:length(groups))
{
  temp<-predict_infection[with(predict_infection %>% dplyr::select("Longitude","Latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("Longitude","Latitude"),Longitude,Latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}
```

```{r}

library(RColorBrewer)

summary(predict_infection_clipped)
#mimimum change = -0.71, max= 0.94

#Labels for each facet 


ggplot(aes(Longitude, Latitude, fill= model_change),
       data=predict_infection_clipped%>% filter(Year.Sampled < 2018))+
  geom_tile()+
  facet_wrap(~ (Year.Sampled = c("2011-2012", "2012-2013", "2013-2014", "2014-2015", "2015-2016", "2016-2017", "2017-2018")),nrow=2)
  #scale_fill_gradient2("Infection Probability", low = "green", high = "red", na.value = NA, limits = c(-1, 1),    breaks = c(-1, -0.5, 0, 0.5, 1),
   #labels = c(-1, -0.5, 0, 0.5, 1))





gg<-ggplot(aes(Longitude, Latitude,  fill= model_change), 
       data=predict_infection_clipped %>% filter(Year.Sampled < 2018))+ 
  geom_tile()+
  facet_wrap(~Year.Sampled,nrow=4)+
   scale_fill_gradient2("Rate of change\n(species per year)")+
  #scale_fill_drsimonj("Infection Probability", palette = "mixed", guide = "none")+
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+ 
  theme_bw(10)+
  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))
  
  geom_point(x = -84.42, y = 33.76, colour="red", size = 2)+
  annotate(geom = "text", x = -84.42, y = 33.5, label = "Atlanta", 
    fontface = "bold", color = "white", size = 2)+
  
  geom_point(x = -95.39, y = 29.77, colour="red", size = 2)+
  annotate(geom = "text", x = -95.39, y = 29.5, label = "Houston", 
    fontface = "bold", color = "white", size = 2)+

  geom_point(x = -89.93, y = 30.07, colour="red", size = 2)+
  annotate(geom = "text", x = -88.00, y = 30.1, label = "New Orleans", 
    fontface = "bold", color = "white", size = 2)+
  
  geom_point(x = -82.48, y = 27.96, colour="red", size = 2)+
  annotate(geom = "text", x = -83.7, y = 27.88, label = "Tampa", 
    fontface = "bold", color = "white", size = 2)+

  

  gg+ theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
 gg     
```


Can we examine the relationship between season and location? 


```{r}


want <- seq(1, nrow(data), length.out = 200)
pdat <- with(data,
              data.frame(Day_Year = Day_Year[want], Year.Sampled = Year.Sampled[want],
                         Volunteer = Volunteer[want]))
 
 ## predict trend contributions
p  <- predict(gam_modII,  newdata = pdat, type = "terms", se.fit = TRUE)
p1 <- predict(m1$gam, newdata = pdat, type = "terms", se.fit = TRUE)
p2 <- predict(m2$gam, newdata = pdat, type = "terms", se.fit = TRUE)
p3 <- predict(m3$gam, newdata = pdat, type = "terms", se.fit = TRUE)

## combine with the predictions data, including fitted and SEs
pdat <- transform(pdat,
                   p  = p$fit[,2],  se  = p$se.fit[,2],
                   p1 = p1$fit[,2], se1 = p1$se.fit[,2],
                   p2 = p2$fit[,2], se2 = p2$se.fit[,2],
                   p3 = p3$fit[,2], se3 = p3$se.fit[,2])




op <- par(mar = c(5,4,2,2) + 0.1)
ylim <- with(pdat, range(p, p1, p2, p3))
> ylim[1] <- floor(ylim[1])
> ylim[2] <- ceiling(ylim[2])
> ylab <- expression(Temperature ~ (degree*C ~ centred))



plot(Infection - mean(Infection) ~ Day_Year, data = data, type = "n")
```





*Research Question 2*
How do the listed exogenous variables impact the spatial dynamics of OE infection prevalence change within year and between years in the sedentary population of monarch butterflies in the SE U.S.?
  
  1. Temperature  
  2. Precipitation 
  3. Monarch larval density 
  

*Environmental Variables Model* 
In the Brown et al. paper, they used the GAMs in the previous step to determine the end of season infection prevalence for each year. They then looked for "bivariate correlations" between these infection prevalences and different environmental variables. 

  *Examining the effect of freezing temperatures at 3 scales*
    1. How does the number of freezes per year impact the overall infection prevalence? 
    2. How does the number of freezes within each hardiness zone impact the overall infection prevalence?
    3. How does the number of freezes preceeding an observation impact infection? 
   
Creating a Weekly Infection MHD File
```{r}
MHD <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/YR_MHD_weather_hardi2.csv")

#Create a data frame with infection, week, hardiness zone (or other geographic variable) and year 

 #Make sure date column is in correct format 
library(lubridate)
MHD$date<- parse_date_time(MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
MHD$date<-as.Date(MHD$date)

  #Create the week variable 
MHD$week <- week(MHD$date)

  #Create new dataframe that averages infection by week, year, zone and ID of the weather station
weeklyMHD<-aggregate(Infection~week+zone+Station +Year, FUN=mean, data=MHD, na.rm=TRUE)

  #Create Year-Week Variable for Graphing 
weeklyMHD$YearWeek=paste(weeklyMHD$Year,"Week",weeklyMHD$week,sep="_")

  #Graph that Caz wanted 
library(ggplot2)
library(dplyr)
weeklyMHD$Year<-as.factor(weeklyMHD$Year)
weeklyMHD %>%
filter(Year == "2016" ,zone == "10a") %>%
ggplot(aes(x=YearWeek, y=Infection)) +
geom_point(alpha = 0.3,  position = position_jitter()) + stat_smooth(method = "loess") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```


Import Weather Data and Count Freezes per Week
```{r}
Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Hardi.csv")

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week. 


#Function for counting the number of freezes. Starting with identifying freeze days. 

#encoding rain days as 0/1 will allow us to simply sum up the number of rainy days for whatever time 
#period we like

freeze.days<-function (TMIN, Day){
  freeze.days<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      freezes<-NA
      }else if (TMIN[i]>0){
      freezes<-0
    }else{
      freezes<-1
    }
    freeze.days<-c(freeze.days, freezes)
  }
  return(freeze.days)
}

#and now the freeze day counter
Weather_Hardi$freeze.day<-freeze.days(Weather_Hardi$TMIN, Weather_Hardi$Day)

#Examine the number of freeze days to ensure a reasonable number have been recorded 
Weather_Hardi %>% group_by(freeze.day) %>% tally()

#Add the number of freezes in each week and add to weekly data frame 
  #Rename Columns to match MHD 
Weather_Hardi<-Weather_Hardi %>%
  rename(Year = Year.Sampled,
    Station = StationName)

 #Make sure date column is in correct format 
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)  

#Create the week variable 
Weather_Hardi$week <- week(Weather_Hardi$date)

#Add it up
FreezeDataFrame<-aggregate(freeze.day~week+zone+id +Year, FUN=sum, data=Weather_Hardi, na.rm=TRUE)

#Merge the freeze.day column into the weeklyMHD dataframe based on week, zone, station and year 
  #Put columns that are shared between dataframes int he same format
FreezeDataFrame$Year<-as.factor(FreezeDataFrame$Year)

weeklyMHD<-weeklyMHD %>%
  rename(id = Station)
  #Merge 
weeklyMHD_freeze<-left_join(weeklyMHD, FreezeDataFrame, by=c("week", "Year", "id"))

#Look at data to ensure accuracy 
weeklyMHD_freeze %>% group_by(zone.y) %>% tally()

```


Repeat the Process of Collecting the Freeze data for Developmental Zero 
```{r}
#Function for counting the number of freezes. Starting with identifying freeze days. 

#encoding rain days as 0/1 will allow us to simply sum up the number of rainy days for whatever time 
#period we like

dev.zero<-function (TMIN, Day){
  dev.zero<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      Dzero<-NA
      }else if (TMIN[i]>11){
      Dzero<-0
    }else{
      Dzero<-1
    }
    dev.zero<-c(dev.zero, Dzero)
  }
  return(dev.zero)
}

#and now the freeze day counter
Weather_Hardi$Dev.Zero.days<-dev.zero(Weather_Hardi$TMIN, Weather_Hardi$Day)

#Examine the number of freeze days to ensure a reasonable number have been recorded 
Weather_Hardi %>% group_by(Dev.Zero.days) %>% tally()

#Add it up
DevZeroDataFrame<-aggregate(Dev.Zero.days~week+zone+id +Year, FUN=sum, data=Weather_Hardi, na.rm=TRUE)

#Merge the freeze.day column into the weeklyMHD dataframe based on week, zone, station and year 
  #Put columns that are shared between dataframes int he same format
DevZeroDataFrame$Year<-as.factor(DevZeroDataFrame$Year)

  #Merge 
weeklyMHD_freeze<-left_join(weeklyMHD_freeze, DevZeroDataFrame, by=c("week", "Year", "id"))

#Look at data to ensure accuracy 
weeklyMHD_freeze %>% group_by(Dev.Zero.days) %>% tally()

```

Now add in TMIN to the weekly data by averaging the weekly TMIN 



```{r}
#Add it up
TMINDataFrame<-aggregate(TMIN~week+zone+id +Year, FUN=mean, data=Weather_Hardi, na.rm=TRUE)

#Merge the freeze.day column into the weeklyMHD dataframe based on week, zone, station and year 
  #Put columns that are shared between dataframes int he same format
TMINDataFrame$Year<-as.factor(TMINDataFrame$Year)

  #Merge 
weeklyMHD_freeze<-left_join(weeklyMHD_freeze, TMINDataFrame, by=c("week", "Year", "id"))

#Look at data to ensure accuracy 
weeklyMHD_freeze %>% group_by(Dev.Zero.days) %>% tally()
```



#To do next
Remove unneeeded olumns from weeklyMHD_freeze and save 
1. WHy is zone not the same? 
2. Models to test effects of different measures. 
3. Add a lag based on slow development time 















#Load weather data for each station on each day
DailyWeath<- read.csv("./data/WeatherData_2011_2018.csv")

#Add day, month and year columns 
# library(lubridate)
# DailyWeath = DailyWeath %>% 
#   mutate(Date = ymd(Date)) %>% 
#   mutate_at(vars(Date), funs(year, month, day))

# Calculate the number of freezes for each year
library(tidyr)
library(dplyr)
FreezeByYear <- DailyWeath %>%
  mutate(below0 = TMIN <= 0,
         below0 = as.numeric(below0)) %>%
  filter(below0==1) %>%
  group_by(Year.Sampled) %>%
  dplyr::summarize(count = n()) %>%
  select(Year.Sampled, count) 

FreezeByYear



r2<-ddply(MHD,.(Date_Sampled), summarize, mean=mean(Infection))
r3<- (r2$mean)
r2$r3 <- r3
```


```{r}
#Create a data frame of the model predicted average infection prevalence per year 

    #Calculate the probability of infection on each day of year 
MHD<- read.csv("./data/YRweather_hardi_MHD.csv")
        
library(plyr)
r2<-ddply(MHD,.(Year), summarize, mean=mean(Infection))
r3<- (r2$mean)
r2$r3 <- r3

new_MHD<-merge(r2, FreezeByYear, by.x="Year", by.y="Year.Sampled")
FreezeYearReg <- lm(r3~ count,data=new_MHD) 

plot(new_MHD$count, new_MHD$r3)
cor_gam_grade <- cor(new_MHD$count, new_MHD$r3, method = "pearson")
cor_gam_grade
```

NEXT: Examine the effects of the number of freezes in each hardiness zone on infection prevalence 

NEXT: Examine the effect of the number of freezes preceeding an observation on the infection observed


NEXT: Examine the relationship between temperature and infection using a GAM model 

*Environmental Variable Models* 
```{r}
library(mgcv)
MHD<- read.csv("./data/YRweather_hardi_MHD.csv")

#Perhaps the question should be more focused - instead "which variables correlate with infection" it should be "which variables best predict the prevalnce of OE during a specific time (like in March when migrants return). THen we could use outputs from the temporal models (infection prevalence in march) as response variables"



###EGAM: LAT AND LONG###
Egam_mod = gam(Infection~ s(TMIN2) + ti(longitude, latitude, bs="re") + s(Volunteer, bs="re"), data=MHD, family=binomial, method="REML")
save(Egam_mod, file="Egam_mod.Rdata")
plot(Egam_mod, pages = 1,  trans = plogis,
     shift = coef(Egam_mod)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")



###EGAM0: NO LAT AND LONG###
Egam_mod0 = gam(Infection~ s(TMIN2) + s(Volunteer, bs="re"), data=MHD, family=binomial, method="REML")
summary(Egam_mod0)
plot(Egam_mod0, pages = 1,  trans = plogis,
     shift = coef(Egam_mod0)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
save(Egam_mod0, file="Egam_mod0.Rdata")




############### DIFFERENT ITERATIONS OF MODELS #############################
Egam_mod2 = gam(Infection~ s(TMIN2) + s(PRCP)+ ti(longitude, by = latitude, bs="re") + s(Volunteer, bs="re"), data=weather_and_MHD, family=binomial, method="REML")

Egam_mod3 = gam(Infection~ s(TMIN2) + s(PRCP)+ ti(TMIN2, by = PRCP) + ti(longitude, latitude, bs="re") + s(Volunteer, bs="re"), data=weather_and_MHD, family=binomial, method="REML")



Egam_mod4 = gam(Infection~ s(TMIN2) + s(PRCP)+ ti(TMIN2, by = PRCP) + s(Volunteer, bs="re"), data=weather_and_MHD, family=binomial, method="REML")


#gam_mod <- gam(Infection_Severity ~  Season + ti(sDay_Year, by = Season, k =6) + s(Volunteer.x, bs="re"), family = binomial, method = "REML", data = data)

summary(Egam_mod2)

```


Graphically Examine Data and Identify Outliers 
```{r}

# #Convert Date column from factor to date 
# require(lubridate)
# weather_and_MHD$Date_Sampled<- parse_date_time(weather_and_MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
# weather_and_MHD$Date_Sampled<- as.Date(weather_and_MHD$Date_Sampled)

#Identify and remove outliers 


library(ggplot2)
ggplot(data=weather_and_MHD, aes(x=Date_Sampled,y = PRCP
                                 )) +  geom_point()

#It looks like outliers would be those with more than 500 mm of rain in one day. Examine these to ensure they are correct. 
PRCP_outliers <- weather_and_MHD[ which(weather_and_MHD$PRCP > 1000), ]
```





Calculating the Number of Days of Freeze Before an Observation 

```{r}
#### __Exploring Temperature Data__
##### I begin by isolating the occurences of extreme temperature below 0 degrees  and count total occurences of extreme temperature by year and by city. I create a table that I can use to visually explore my data. Some cities do not have any occurences of extreme temperature and give back an NA value in the table.

#create dummy variable for when min temp is below 0 
# avg count below 0 btwn 2011-2018 
all_data <- DailyWeath %>%
  mutate(below0 = TMIN <= 0,
         below0 = as.numeric(below0)) %>%
  filter(below0==1) %>%
  group_by(id,year) %>%
  summarize(count = n()) %>%
  select(id, year, count) %>%
  spread(year, count)
all_data
```


```{r}
#Load test data
#1.FInd nearest station to obs 
#2. search weather data from start date(obs date - obs.period) to end date (obs date)
#3. Count each occurance when temp is below 0 in that time period 
#4. Store count of freeze days as integer in new columnnext to original observation

```

```{r}



new_MHD<-merge(Hardiness_Zipcode_Raw, weather_and_MHD, by.x="zipcode", by.y="Zipcode")
```

```{r}


##### How do the number of freezes vary by Year? 
library(dplyr)
all_data2 <- YRweather_hardi_MHD %>%
  mutate(freeze = TMIN2 <= 0,
         freeze = as.numeric(freeze)) %>%
  filter(freeze==1) %>%
  group_by(zone,Year) %>%
  summarize(count = n()) %>%
  select(Year, count)
all_data2

all_data <- new_MHD %>%
  mutate(below0 = TMIN2 <= 0,
         below0 = as.numeric(below0)) %>%
  filter(below0==1) %>%
  group_by(Station,Date_Sampled) %>%
  summarize(count = n()) %>%
  select(Station, Date_Sampled, count) %>%
  spread(Date_Sampled, count)

library(ggplot2)
ggplot(all_data2, aes(x=Year, y=count, color=zone, shape=zone)) +
  geom_point() + 
  geom_smooth(method=lm, aes(fill=zone))+
  facet_wrap(~zone)
#Could I just do a regression with the number of freezes from Jan-March and the OE prevalence for the year? 
```







GRAPH CAZ WANTS 
```{r}
#Calculate average infection prevalence in a region by month/year (can figure out week later)

library(plyr)
r2<-ddply(YRweather_hardi_MHD,.(Month, Year, zone), summarize, mean=mean(Infection))
r3<- (r2$mean)
r2$r3 <- r3

r2<-within(r2, Date <- sprintf("%d-%02d", Year, Month))
r2$Date2<-as.Date(r2$Date, "%Y-%m") 

library(lubridate)
r2$Date2 <- ym(r2$Date)

install.packages("zoo")
library(zoo)
r2$Date2<-as.Date(as.yearmon(r2$Date))

library(ggplot2)
library(dplyr)
attach(r2)
r3<-subset(r2, zone=="10a")

r2 %>%
  filter(r2$zone == "9a") %>%
ggplot(aes(x=Date2, y=mean)) +
  geom_point(alpha = 0.3,  position = position_jitter()) + stat_smooth(method = "loess")+ 
   geom_vline(data = r2, xintercept = (Date2=="2015-01-01"))



 
#Convert to date format

YRweather_hardi_MHD$Date_Sampled<- parse_date_time(YRweather_hardi_MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
YRweather_hardi_MHD$Date_Sampled<- as.Date(YRweather_hardi_MHD$Date_Sampled)

# Set Weeks number. Date already of class `Date`
YRweather_hardi_MHD$Week <- week(YRweather_hardi_MHD$Date_Sampled)

# Aggregate over week number and Hardiness zone 
library(lubridate)
weeklyMHD$WeekYear <-transform(weeklyMHD,WeekYear=interaction(Week,Year,sep=''))
weeklyMHD$WeekYear <- paste(weeklyMHD$Week,".",weeklyMHD$Year)



weeklyMHD<-aggregate(Infection~Week+Year+zone, FUN=mean, data=YRweather_hardi_MHD, na.rm=TRUE)


summary(weeklyMHD)

weeklyMHD$YearWeek=paste(weeklyMHD$Year,"Week",weeklyMHD$Week,sep="_")




library(dplyr)
library(ggplot2)

weeklyMHD$YearWeek = reorder(weeklyMHD$YearWeek,weeklyMHD$Week)
#new <- weeklyMHD[with(weeklyMHD, order(weeklyMHD$YearWeek,weeklyMHD$Year, weeklyMHD$Week)),]


weeklyMHD %>%
  filter(weeklyMHD$zone == "10a" & weeklyMHD$Year == "2014") %>%
ggplot(aes(x=YearWeek, y=Infection)) +
  geom_point(alpha = 0.3,  position = position_jitter()) + stat_smooth(method = "loess") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.25, hjust=0.25))




   
geom_vline(data = weeklyMHD, xintercept = (Week=="2015-01-01"))

```
```{r}
#Count the number of days below freezing in a week 

#Add a week column to weather data 

WeatherData_2011_2018$Date<- parse_date_time(WeatherData_2011_2018$date, orders = c("mdy", "dmy","ymd"))
WeatherData_2011_2018$Date<- as.Date(WeatherData_2011_2018$Date)
WeatherData_2011_2018$Week <- week(WeatherData_2011_2018$Date)

#encoding rain days as 0/1 will allow us to simply sum up the number of rainy days for whatever time 
#period we like

freeze.days<-function (freeze, Week){
  freeze.days<-c()
  for (i in 1:length(freeze)){
    if(freeze[i]>0){
      freezedays<-1
    }else{
      freezedays<-0
    }
    freeze.days<-c(freeze.days, freezedays)
  }
  return(freeze.days)
}

#and now the rain day counter
WeatherData_2011_2018$freezy.days<-freeze.days(WeatherData_2011_2018$TMIN, WeatherData_2011_2018$Week)

```



*Checking the Model*
```{r}
gam.check(Egam_mod)

#gam.check does not seem to be the best method for visualizing logistic models

library(arm)

binnedplot(fitted(Egam_mod), 
           residuals(Egam_mod, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           main = "S Year", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")
```

```{r}
PRCP<-weather_and_MHD$PRCP
TMIN2<-weather_and_MHD$TMIN2
new<-data.frame(PRCP,TMIN2)
vif(new)

new <- mutate(weather_and_MHD, PRCP = PRCP, TMIN2 = TMIN2)
```


```{r}
plot(Egam_mod3, residuals=FALSE, pages = 1,  trans = plogis,
      seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

```{r}
vis.gam(Egam_mod3, view=c("TMIN2","PRCP"),plot.type="contour", color = "terrain", contour.col = "black") 

```

```{r}
#Load the GAM model output 
load(file = "data/STyes.RData")
pp <- predict(STyes, type = "terms")

df$Latitude<-df$latitude
df$Longitude<-df$longitude
df$Year.Sampled<-df$Year


 grade_gam_pred <- predict.gam(grade_gam, newdata = eva, type = "response")
 
 
year_avg_predict <- predict(STyes,newdata = DailyWeath, type = "response")


    gam_pred<- cbind.data.frame(year_avg_predict, weather_and_MHD)
    
    
    obs_pred_grade_gam$Pred<-obs_pred_grade_gam$year_avg_predict
        obs_pred_grade_gam$PRCP<-obs_pred_grade_gam$df$PRCP

colnames(obs_pred_grade_gam) <- c("pred_gam_grade", "obs_gam_grade")
cor_gam_grade <- cor(gam_pred$mean, gam_pred$count, method = "pearson")

    gam_pred<- cbind.data.frame(r2, all_data2)

cor_gam_grade <- cor(new_MHD$mean, new_MHD$count, method = "pearson")
    
```

