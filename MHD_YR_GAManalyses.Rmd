---
title: "GAM_MHD"
author: "Christen"
date: "March 25, 2020"
output:
  html_document:
    df_print: paged
---

GAM Work with Monarch Health Data



*Overall Research Objectives*
1.	Determine how OE infection prevalence changes seasonally (within a year) and across years in the southeastern U.S. 

2.	Determine where OE infection prevalence is most prevalent within each year and how the spatial patterns of infection have changed over time? 

3.	Determine whether climatic variables are correlated with the spatial and temporal changes in OE infection prevalence in the southeastern U.S.

*Why use a GAM?*
We do not expect the patterns in disease prevalence as described by our predictor variables to be linear. For example, infection prevalence may experience increases during periods of warm weather such as in late fall and early spring, while mid winter low temperatures may result in a decline in infection prevalence as population density and therefore transmission potential declines. Graphs of the dataset are provided below for visual inspection of the linear or non-linear trends.  

*Dataset*
```{r MHD Dataset}
#The below dataset should be used to examine winter OE data  
#wdata <- read.csv("./data/MHD_WD.csv", colClasses = c("Pop_density_CAT" = "factor", "Hardiness2" = "factor", "Infection_Severity" = "factor", "Month"= "factor", "Year"= "factor"))

#The dataset below should be used to examine annual OE data
data<- read.csv("./data/YR_MHD.csv")

summary (data)
```


*Exploration of the Data through Graphing*
Graph the relationship between infection and day of year to visually inspect the relationship for linearity. 
From this figure it is hard to discern any clear pattern. There seems to be a mostly random distribution of infection prevalence across all year (by day of year). #Note: Day 200 is mid-July 
```{r}
#Convert variables into different forms for graphing purposes
data$Day_Year<-as.factor(data$Day_Year)
data$Infection<-as.numeric(data$Infection)

#Calculate the probability of infection on each day of year 
library(plyr)
r2<-ddply(data,.(Day_Year), summarize, mean=mean(Infection))
r3<- (r2$mean)
r2$r3 <- r3

library(ggplot2)
ggplot(data=r2, aes(x=as.numeric(Day_Year),shape =  Year.Sampled, r3))+geom_point() + geom_smooth()
```

Graph the relationship between infection and year to visually inspect the relationship for linearity. 
```{r}
#Convert variables into different forms for graphing purposes
##data$Season<-revalue(data$Season, c("11_12"="1", "12_13"="2", "13_14"="3", "14_15"="4", "15_16"="5", "16_17"="6", "17_18"="7"))

data$Year.Sampled<-as.factor(data$Year.Sampled)
data$Infection<-as.numeric(data$Infection)


#Calculate the probability of infection on each day of year 
library(plyr)
s1<-ddply(data,.(Year.Sampled), summarize, mean=mean(Infection))
s2<- (s1$mean)
s1$s2 <- s2

#Plot 
plot(as.numeric(s1$Year.Sampled), s1$
       s2, xlab = "Year", ylab = "Probability of Infection", type='o')
```


Graph the spatial distribution of the data.  
```{r}
library("maps")
library("dplyr")
library("ggplot2")

states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

gg<-ggplot() + 
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group), fill="grey", colour = "white")+
   geom_point(data=data, aes(x=Longitude, y=Latitude, color = factor(Infection)), position = "jitter", size=1)+
  scale_colour_manual(values = c("1"="chocolate3", "0"= "cyan"))+
 
 # geom_point(data=data, aes(x=Longitude, y=Latitude),size=1, colour = "black")
  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))+
   xlab("Longitude") + ylab("Latitude")

  gg + theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
```


*Objective 1: Analyzing the Temporal Patterns in OE Infection Prevalence in the SE*
  
  *Hypothesis 1: Within Year*
If OE infection prevalence is primarily driven by reproductive behavior and breeding is  occurring year-round in the southeast, then  OE infection prevalence may remain high, showing little correlation with day of year. 

  *Hypothesis 2: Between Years*
  If the population of winterbreeding monarchs in the southeastern US is growing (and therefore becoming more dense), we expect that OE infection prevalence may increase with year as increased monarch density leads to higher rates of transmission.
  
*Model Construction*
This model is designed to characterize temporal infection patterns in the southeast by including day of year, year and an interaction of these two terms in a single model. 

Details on the model:
1. Why "ti"?
  Tensor product smooths are used to model interactions between variables that have different natural scales. They multiply the smooth terms of one variable by the factor levels of the other. 

2. Why "by"?
  "By" variables are used for constructing "varying coefficient models" and for letting smooths interact with parametric terms. In this case our smooth is the Day_Year variable and the Season term is the parametric term (and a factor). 
The interaction between two parametric terms would use the : or * connotation, but smooth interactions cannot. "By" generates an indicator vector for each level of a factor  unless the factor is ordered. If it is ordered then a different smooth is generated for each fator level (except the first level). 


3. Why k=6? 
The term k is also referred to as "knots". These are the natural breaks in the basis functions that make up the "basis" or "smooth function" or "spline". Where the knots break along the spline is usually defined by quantiles. K is usually the max number of degrees of freedom allowed for a smooth term in the model. K in the models will always =  (max degrees of freedom - 1). In this case 

4. Why "family=binomial"? 
The response variable is either 0 (uninfected) or 1 (infected), this we have 2 curves for our response variable distribution and this requires that the model be fit with a binomial family distribution. 

5. Why is "Season" parametric?


6. Why is Volunteer "bs=re"? 
In order to include Volunteer as a random effect, we tell the model to use the random effect basis function to model this variable. This method only applies to mcgv, as a different method is used for gamm. 

7. Why REML? 

8. Should I be using a different basis function? Cubic? 

```{r eval, = FALSE if exists (gam_modIF.Rdata) }
require(mgcv)

###Make sure each variable is in the correct format 
data$Day_Year<-as.numeric(data$Day_Year)
data$Year.Sampled<-as.numeric(data$Year.Sampled)
data$Infection<-as.factor(data$Infection)

gam_modTIME <- gam(Infection ~  s(Day_Year, bs = "cc") + s(Year.Sampled,  k = 8) + ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

save(gam_modTIME, file="gam_modII.Rdata")

summary(gam_modTIME)



```



The following models are the different iterations that eventually led to the final model selected above. They change terms in the model from smooth to linear or investigate the way the interaction term is used. 
```{r eval, = FALSE if exists (gam_modIF.Rdata) }

# #gam_mod: Model with Year as a linear effect
# gam_mod <- gam(Infection ~  Year.Sampled + ti(Day_Year, by = Year.Sampled, k =6) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# summary(gam_mod) 
# save(gam_mod, file="gam_mod.Rdata")
# 
# 
# ###gam_modSS: Fit model with Year smoothed (numeric)
# 
# data$Year.Sampled<-as.numeric(data$Year.Sampled)
# 
# gam_modSS <- gam(Infection ~  s(Year.Sampled, k = 8) + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# summary(gam_modSS)
# save(gam_modSS, file="gam_modSS.Rdata")
# 
# 
# ###Fit model with Day_Year as an added  individual effect with both day_year and year smoothed
# 
# #Running Both as numeric variables
# data$Year.Sampled<-as.numeric(data$Year.Sampled)
# data$Day_Year<-as.numeric(data$Day_Year)
# 
# gam_modIY <- gam(Infection ~  s(Day_Year) + s(Year.Sampled, k = 8) + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modIY, file="gam_modIY.Rdata")
# summary(gam_modIY)
# 
# 
# ###Fit model with Day_Year as an added  individual effect with year as a factor
# data$Year.Sampled<-as.factor(data$Year.Sampled)
# data$Day_Year<-as.numeric(data$Day_Year)
# 
# 
# gam_modIF <- gam(Infection ~  s(Day_Year) + Year.Sampled + ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modIF, file="gam_modIF.Rdata")
# summary(gam_modIF)
# 
# 
# #WHen comparing all 4 models:
# #gam_modIF = 12266.17
# #gam_modIY = 12813.5. 
# #gam_mod =   12317.86
# #gam_modSS = 12859.28
# AIC(gam_modIY, gam_modIF, gam_mod, gam_modSS)
# 
# 
# #Model without "by" as the interaction term# 
# gam_modNoby <- gam(Infection ~  s(Day_Year, bs = "cc") + s(Year.Sampled,  k = 8) + ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
# 
# save(gam_modNoby, file="gam_modNoby.Rdata")
# 
# AIC(gam_modIY, gam_modIF, gam_mod, gam_modSS, gam_modII)

#Could it be that the AIC value is higher when year is a factor because it is more closely fitting the data, but it is not answering the question we have about Year, which is how does infection change OVER TIME? WIth year as a factor, the question seems to be how does infection change from year to year or within a single year. 

# gam_modIY	264.3734	14116.17		
# gam_modIF	278.3352	13646.39		
# gam_mod	267.6086	  12317.86		
# gam_modSS	242.2135	12859.28		
# gam_modII	295.0423	13675.55	
#with te added: gam_modII	292.4565	13674.03	(without "by")
#with Day_Year as cyclic: 13671.31 (without "by")
#with Day as cyclic and Year as cubic: 13671.31 (so not needed) (without "by")
# "by" added back in 14281.25
#remove "Te"

# #Model with only interaction term for contour plot#
# gam_modCC <- gam(Infection ~   ti(Day_Year, by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)

```


*Summarize output of model*
```{r}
summary.gam(gam_modTIME) 

#For standard deviations and confidence intervals 
gam.vcomp(gam_modTIME)
```
Key points from model output: 

The model output summary tells us that the additive temporal variables explain 42.5% of the deviance.We also can convert the outputs from the model to actual probabilities because the GAM model uses a log-odds scale for estimating outputs. After converting the intercept estimate below we find that the model predicts a 44.1% chance of infection overall. 
```{r}
plogis(-0.2373)
```


*Checking the Model*
```{r}
#gam.check(gam_modTIME)

#gam.check does not seem to be the best method for visualizing logistic models

library(arm)

binnedplot(fitted(gam_modTIME), 
           residuals(gam_modTIME, type = "response"), 
           nclass = NULL, 
           xlab = "Expected Values", 
           ylab = "Average residual", 
           main = "S Year", 
           cex.pts = 0.8, 
           col.pts = 1, 
           col.int = "gray")
```


*Plot the results of the temporal model* 
Plots of the partial effects of the interaction fo Day_Year and Season for each level of Season. 

```{r}
require(mgcViz)
require(mgcv)

#Year.Sampled = c("2011-2012", "2012-2013", "2013-2014", "2014-2015", "2015-2016", "2016-2017", "2017-2018")
#axis(, at=1:8, xaxp=letters[2011:2018])

#Version1
plot(gam_modII, pages = 1,  trans = plogis,
     shift = coef(gam_modIY)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

#Version2
b <- getViz(gam_modII, trans = plogis)

print(plot(b, allTerms = T))
```

Visualization of the interaction between Year and Day_Year in a 3D plot. 
```{r}

vis.gam(gam_modII, n.grid = 50, theta = 400, phi = 20, ylab = "Day of Year", xlab = "Year", zlab = "",
        ticktype = "detailed", color = "terrain", main = "Interaction between day of year and season")



##theta= rotates the view around horizontally 
##phi = rotates the view vertically 
##n.grid =  controls the number of grid boxes
##z scale = GAM model predictions 

```

Visualization of the interaction between Year and Day_Year in a contour plot.
```{r}
require(mgcv)

vis.gam(gam_modTIME, view=c("Day_Year","Year.Sampled"),plot.type="contour", color = "terrain", contour.col = "black") 

#The contour lines represent points of equal predicted values, and they are labeled. The dotted lines show uncertainty in prediction; they represent how contour lines would move if predictions were one standard error higher or lower.

#Run a model without an interaction to help understand the contour plot 
  #gam_modNI <- gam(Infection ~  s(Day_Year) + s(Year.Sampled, k = 8)  + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)
  #save(gam_modNI, file="gam_modNI.Rdata" )
  #vis.gam(plog, view=c("Day_Year","Year.Sampled"),plot.type="contour", color = "terrain", contour.col = "yellow") 


devtools::install_github('gavinsimpson/gratia')
library('gratia')
mod <- gam(y ~ s(x, z, k = 30), data = dat$data, method = "REML")

mod <- gam(Infection ~   ti(Day_Year, bs = "cc", by = Year.Sampled, k= 8) + s(Volunteer, bs="re"), family = binomial, method = "REML", data = data)


draw(gam_modII)



```


Summary of plot results: 

*Spatio-temporal Model Example* 

Joint models of space and time. 

Should an interaction between space and time be included? This is compared here and the interaction is found to be significant and produces a significantly lower AIC value compared to the non-interaction term model. 
```{r}
data$Year.Sampled<-as.numeric(data$Year.Sampled)


#First, run a model without an interaction between space and time as done in the example. By comparing this model to the model with a space-tie interaction, we will be able to determine the relative importance of the interaction term. 

STno = gam(Infection~ s(Longitude_city,Latitude_city) + s(Year.Sampled, k = 8) + s(Volunteer, bs="re"),
                          data=data,family=binomial,method="REML")

save(STno, file="STno.Rdata")
summary(STno)
plot(STno,scheme=2)


#Now run the model with the interaction term 

STyes = gam(Infection~ s(Longitude,Latitude) + s(Year.Sampled, k = 8)+ ti(Longitude, Latitude, Year.Sampled, d=c(2,1)) + s(Volunteer, bs="re"),
                          data=data,family=binomial,method="REML")
save(STyes, file="STyes.Rdata")
summary(STyes)
plot(STyes,scheme=2)

load("STyes.Rdata")
AIC(STyes)


```
WHile including the interaction term improves the fit of the model, it doesnt make it easy to determine where we are seeing 1) OE hotspots within each year 2) the greatest change in hotspots over time. 

We will next use the "predict" function from mgcv to better visualize the output of the STyes model. 


Graphing of space-time interactions 


We need to select a volunteer whose observations represent the average infection prevalence found by all volunteers. To do this, we will first determine the average infection prevalence per volunteer and then select the volunteer whose average is closest to this number. 
```{r}
#First use plyr to calculate the average per volunteer 
library(plyr)
r2<-ddply(data,.(Volunteer), summarize, Avg=mean(Infection))
r2
#We now have a table r2, with the average infection prevalence per volunteer for all 269 volunteers 
```

We can visualize the distribution of infection prevalence per volunteer. 
```{r}
library(ggplot2)
ggplot(data=r2, aes(x=as.factor(Volunteer), Avg)) + geom_point()+ geom_smooth()

hist(r2$Avg)
```
Next we summarize the r2 dataframe to determine the overall mean per volunteer. 
```{r}
summary(r2)

#The mean in 0.4681
```
Select the volunteers with an average closest to the average of 0.4681
```{r}

Average_Volunteers <- r2[r2$Avg >= 0.4 & r2$Avg < 0.5, ]
Average_Volunteers$Differ<-0.4681-Average_Volunteers$Avg
Average_Volunteers

#We find that 2 volunteers Ana Maria Agrusa (13) and Laninda Sande (154) have averages closest to the overall average (0.4615385), therefore we can use either volunteer in the model 
```


Create the background map on which the model predictions will be mapped 
```{r}
library("maps")
library("dplyr")
library("ggplot2")


states <- map_data("state")

se_map <- subset(states, region %in% c("florida", "georgia", "louisiana", "alabama", "mississippi", "texas", "south carolina", "oklahoma", "arkansas", "north carolina", "tennessee", "virginia", "kentucky", "west virginia", "missouri", "illinois", "kansas"))

```

We can now move on to creating a grid of predicted infection prevalence using the average observation values of volunteer #13.  
```{r}
data$Latitude<-as.numeric(data$Latitude)
data$Longitude<-as.numeric(data$Longitude)

se_map$Latitude<-as.numeric(se_map$lat)
se_map$Longitude<-as.numeric(se_map$long)

data$Year.Sampled<-as.factor(data$Year.Sampled)

#First we'll create gridded data

predict_infection = expand.grid(
  Latitude= seq(min(data$Latitude), 
                max(data$Latitude),
                length=100),
  Longitude = seq(min(data$Longitude),
                  max(data$Longitude),
                  length=100),
  Volunteer = levels(data$Volunteer)[13],  
  Year.Sampled = seq(2011,2018,by=1))

#head(predict_infection)

#summary(data)
```

Create the Model Fit Column 
```{r}

predict_infection$model_fit = predict(STyes,
                                    predict_infection,type = "response")

```

Create a for loop to build polygons for each group in the dataset 
```{r}
library(ggplot2)
library(viridis)
library(dplyr)

se_map<-se_map %>% mutate(Longitude=long,Latitude=lat)
predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)

for(g in 1:length(groups))
{
  temp<-predict_infection[with(predict_infection %>% dplyr::select("Longitude","Latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("Longitude","Latitude"),Longitude,Latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}

library(mapproj)
library(maps)
library(ggrepel)

#Process for Making cities labels 
data(us.cities)
capitals <- subset(us.cities)
capitals$city <- sub(' [^ ]*$','',capitals$name) # split out city for the label

plotcities <- subset(world.cities, name %in% c("Atlanta", "Houston", "New Orleans", "Austin", "San Antonio", "Tampa", "Orlando", "Miami")) %>% filter(country.etc=="USA")


 
```

Graph the data
```{r}
gg<-ggplot(aes(Longitude, Latitude,  fill= model_fit), 
       data=predict_infection_clipped)+ #%>% filter(Year.Sampled==2011))+ 
  geom_tile()+
  facet_wrap(~Year.Sampled,nrow=4)+
  scale_fill_viridis("Infection Probability")+
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+
  
  # geom_point(x = -84.42, y = 33.76, colour="red", size = 1)+
  # annotate(geom = "text", x = -84.42, y = 33.5, label = "Atlanta", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -95.39, y = 29.77, colour="red", size = 1)+
  # annotate(geom = "text", x = -95.39, y = 29.5, label = "Houston", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -89.93, y = 30.07, colour="red", size = 1)+
  # annotate(geom = "text", x = -88.00, y = 30.1, label = "New Orleans", 
  #   fontface = "bold", color = "white", size = 1)+
  # 
  # geom_point(x = -82.48, y = 27.96, colour="red", size = 1)+
  # annotate(geom = "text", x = -83.7, y = 27.88, label = "Tampa", 
  #   fontface = "bold", color = "white", size = 1)+

  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))

  g<-gg+ theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
g
  
save_plot("~/g.png", plot_tmp, base_height = NULL, base_aspect_ratio = 1.618, 
base_width = 6)

#library(OpenImageR)

#img<-OpenImageR::readImage("~/plot_tmp.png")
#imageShow(img)
 
```


Add space by day of year may be the next step in the analysis in order to see if infection changes in different regions through time
It seems like anytime I seem to know what I ma talking about she rears up 
We need to have a story with this. Especially from the spatiotemporal data. Think about story for publication 


Calculate the difference from year to year 
```{r}
predict_infection$model_change =predict(STyes,
                                      predict_infection%>%mutate(Year.Sampled=Year.Sampled+1),
                                      type = "response") - predict_infection$model_fit 



se_map<-se_map %>% mutate(Longitude=long,Latitude=lat)

predict_infection_clipped<-data.frame()
groups<-unique(se_map$group)

for(g in 1:length(groups))
{
  temp<-predict_infection[with(predict_infection %>% dplyr::select("Longitude","Latitude"), 
                               inSide(se_map %>% filter(group==groups[g]) %>% dplyr::select("Longitude","Latitude"),Longitude,Latitude)),]
  predict_infection_clipped<-rbind(predict_infection_clipped,temp)
  
}
```

```{r}

library(RColorBrewer)

summary(predict_infection_clipped)
#mimimum change = -0.71, max= 0.94

#Labels for each facet 


ggplot(aes(Longitude, Latitude, fill= model_change),
       data=predict_infection_clipped%>% filter(Year.Sampled < 2018))+
  geom_tile()+
  facet_wrap(~ (Year.Sampled = c("2011-2012", "2012-2013", "2013-2014", "2014-2015", "2015-2016", "2016-2017", "2017-2018")),nrow=2)
  #scale_fill_gradient2("Infection Probability", low = "green", high = "red", na.value = NA, limits = c(-1, 1),    breaks = c(-1, -0.5, 0, 0.5, 1),
   #labels = c(-1, -0.5, 0, 0.5, 1))





gg<-ggplot(aes(Longitude, Latitude,  fill= model_change), 
       data=predict_infection_clipped %>% filter(Year.Sampled < 2018))+ 
  geom_tile()+
  facet_wrap(~Year.Sampled,nrow=4)+
   scale_fill_gradient2("Rate of change\n(species per year)")+
  #scale_fill_drsimonj("Infection Probability", palette = "mixed", guide = "none")+
  geom_polygon(data=se_map,aes(x=long,y=lat,group=group),color="black",inherit.aes = FALSE,alpha=0)+ 
  theme_bw(10)+
  coord_map(xlim = c(-98.5, -75),ylim = c(24, 37))
  
  geom_point(x = -84.42, y = 33.76, colour="red", size = 2)+
  annotate(geom = "text", x = -84.42, y = 33.5, label = "Atlanta", 
    fontface = "bold", color = "white", size = 2)+
  
  geom_point(x = -95.39, y = 29.77, colour="red", size = 2)+
  annotate(geom = "text", x = -95.39, y = 29.5, label = "Houston", 
    fontface = "bold", color = "white", size = 2)+

  geom_point(x = -89.93, y = 30.07, colour="red", size = 2)+
  annotate(geom = "text", x = -88.00, y = 30.1, label = "New Orleans", 
    fontface = "bold", color = "white", size = 2)+
  
  geom_point(x = -82.48, y = 27.96, colour="red", size = 2)+
  annotate(geom = "text", x = -83.7, y = 27.88, label = "Tampa", 
    fontface = "bold", color = "white", size = 2)+

  

  gg+ theme(panel.background = element_rect(fill = "skyblue3",
                                colour = "skyblue3"),
          panel.grid.major = element_blank(),
          panel.grid.minor = element_blank())
 gg     
```


Can we examine the relationship between season and location? 


```{r}


want <- seq(1, nrow(data), length.out = 200)
pdat <- with(data,
              data.frame(Day_Year = Day_Year[want], Year.Sampled = Year.Sampled[want],
                         Volunteer = Volunteer[want]))
 
 ## predict trend contributions
p  <- predict(gam_modII,  newdata = pdat, type = "terms", se.fit = TRUE)
p1 <- predict(m1$gam, newdata = pdat, type = "terms", se.fit = TRUE)
p2 <- predict(m2$gam, newdata = pdat, type = "terms", se.fit = TRUE)
p3 <- predict(m3$gam, newdata = pdat, type = "terms", se.fit = TRUE)

## combine with the predictions data, including fitted and SEs
pdat <- transform(pdat,
                   p  = p$fit[,2],  se  = p$se.fit[,2],
                   p1 = p1$fit[,2], se1 = p1$se.fit[,2],
                   p2 = p2$fit[,2], se2 = p2$se.fit[,2],
                   p3 = p3$fit[,2], se3 = p3$se.fit[,2])




op <- par(mar = c(5,4,2,2) + 0.1)
ylim <- with(pdat, range(p, p1, p2, p3))
> ylim[1] <- floor(ylim[1])
> ylim[2] <- ceiling(ylim[2])
> ylab <- expression(Temperature ~ (degree*C ~ centred))



plot(Infection - mean(Infection) ~ Day_Year, data = data, type = "n")
```





*Research Question 2*
How do the listed exogenous variables impact the spatial dynamics of OE infection prevalence change within year and between years in the sedentary population of monarch butterflies in the SE U.S.?
  
  1. Temperature  
  2. Precipitation 
  3. Monarch larval density 
  

*Environmental Variables Model* 
In the Brown et al. paper, they used the GAMs in the previous step to determine the end of season infection prevalence for each year. They then looked for "bivariate correlations" between these infection prevalences and different environmental variables. 

  *Examining the effect of freezing temperatures at 3 scales*
    1. How does the number of freezes per year impact the overall infection prevalence? 
    2. How does the number of freezes within each hardiness zone impact the overall infection prevalence?
    3. How does the number of freezes preceeding an observation impact infection? 
   



First, explore the relationship between temperature and infection by graphing the data. 
```{r Temperature by Infection Graph}
#Import the infection data 
library(readr)
#MHD<- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/YR_MHD_weather_hardi2.csv")
MHD <- read_csv("./data/YR_MHD_weather_hardi2.csv")


#Create a data frame with infection, week, hardiness zone (or other geographic variable) and year 

 #Make sure date column is in correct format 
library(lubridate)
MHD$date<- parse_date_time(MHD$Date_Sampled, orders = c("mdy", "dmy","ymd"))
MHD$date<-as.Date(MHD$date)

  #Create the week variable 
MHD$week <- week(MHD$date)

  #Create new dataframe that averages infection by week, year, zone and ID of the weather station
weeklyMHD<-aggregate(Infection~week+zone+Station +Year, FUN=mean, data=MHD, na.rm=TRUE)

  #Create Year-Week Variable for Graphing 
weeklyMHD$YearWeek=paste(weeklyMHD$Year,"Week",weeklyMHD$week,sep="_")


  #Graph that Caz wanted 
library(ggplot2)
library(dplyr)
#weeklyMHD$Year<-as.factor(weeklyMHD$Year)
weeklyMHD %>%
filter(Year == "2016" ,zone == "10a") %>%
ggplot(aes(x=reorder(YearWeek, week), y=Infection)) + #Reorder by week so that there will be order 
geom_point(alpha = 0.3,  position = position_jitter()) + stat_smooth(method = "loess") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))



#Create dataframe with YearWeek Freezes 
# weeklyMHD_all3 %>%
# filter(Year == "2016" ,zone == "10a") %>%
#   #Need to add uniqe values of freeze weeks and then add to figure as x intercepts 
  
```




*Build Freeze Dataframe*
Import Weather Data and Code Days when *Freezes* Occurred as 1 in the Weather Data Set 
```{r Create Daily Freeze Dataframe}
############################################
###########DAILY FREEZE DATA################
############################################
# #Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Hardi.csv")
# Weather_Hardi <- read_csv("./data/Weather_Hardi.csv")
# 
# #Modify the weather data to prepare it for analysis 
#   #Rename Columns to match MHD 
 library(dplyr)

Weather_Hardi<-Weather_Hardi %>% rename(Year = Year.Sampled,Station = StationName)
# 
 #Make sure date column is in correct format
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)
#Create the week variable
Weather_Hardi$week <- week(Weather_Hardi$date)

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week.
#Function for counting the number of freezes. Starting with identifying freeze days.
freeze.days<-function (TMIN, Day){
  freeze.days<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      freezes<-NA
    }else if (TMIN[i]>0){
      freezes<-0
    }else{
      freezes<-1
    }
    freeze.days<-c(freeze.days, freezes)
  }
  return(freeze.days)
}

#and now the freeze day counter
#this took my computer 1.5 minutes to run
Weather_Hardi$freeze.day<-freeze.days(Weather_Hardi$TMIN, Weather_Hardi$Day)
#Weather_Hardi$freeze.day<- Weather_Hardi$TMIN < 0


#Examine the number of freeze days to ensure a reasonable number have been recorded
Weather_Hardi %>% group_by(freeze.day) %>% tally()

#write.csv(Weather_Hardi, "C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Freeze.csv" )


#Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Freeze.csv")
#Weather_Hardi <- read_csv("./data/Weather_Freeze.csv")

Weather_Hardi %>% group_by(freeze.day) %>% tally()
```

Next, the data need to go through a series of steps to arrange them for calculating the number of freezes inside of the desired window
```{r Preparing freeze data for 7 day window tabulation}
###############################################################
#Prepare data for the "roll apply"" function by arranging the stations as columns with date 
###############################################################
  
  #Create a data frame with only the id of the weather station, the date and the freeze code 
DailyFreezeSubset = subset(Weather_Hardi, select = c(id, date, freeze.day) )

  #Split the "id" column into 95 separate columns (one for each station)
library(tidyr)
#DFS<-DailyFreezeSubset %>% 
#   mutate(rn = row_number()) %>%
#   spread(id,  freeze.day) %>%
#   select(-rn)

DFS<-DailyFreezeSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=freeze.day) %>% arrange(date)

DFS %>% group_by(USC00083163) %>% tally()
#Each station should have over 2500 observations (note: this station is in Fort Lauderdale and should have no freezes)


  #Remove NAs and collapse date column to 1 date per day
#library(data.table)
#DFS2<-setDT(DFS)[, lapply(.SD, function(x) x[!is.na(x)][1L]), by = date]
#DFS2 %>% group_by(USC00083163) %>% tally()
#Same number of observations for 0 

```




*7 Day Freeze Window Calculations*
Count the number of *freezes* in the previous 7 days, 30 days and 60 days from when an observation was made 
```{r Count the number of freezes in the previous 7 days}
############################################
###########Apply RollApply Function#########
############################################

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
DFSroll<-rollapply(data = DFS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
DFSroll<-as.data.frame(DFSroll)

#Add date column back into
Freeze7 <- cbind(date = DFS$date,DFSroll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
Freeze7<-Freeze7 %>% pivot_longer(cols=2:ncol(Freeze7),names_to="id",values_to="freeze.day")

#Check to ensure that there is a range of freezes 
Freeze7 %>% group_by(freeze.day) %>% tally()

#Now Merge the freeze data with the monarch health observation data, allowing each monarch health observation to be paired with a corresponding weather observation including the summed window freeze data. 

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
Freeze7$id<-as.factor(Freeze7$id)

levels(Freeze7$id)
levels(MHD$id)

#Merge 
MHD_freeze<-left_join(MHD, Freeze7, by=c("date", "id"))
summary(MHD_freeze)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))

```


*30 Day Freeze Window*
```{r 30 Day Freeze Window Calculations}
#Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Freeze.csv")
Weather_Hardi <- read_csv("./data/Weather_Freeze.csv")

DailyFreezeSubset = subset(Weather_Hardi, select = c(id, date, freeze.day) )
DFS<-DailyFreezeSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=freeze.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
DFSroll<-rollapply(data = DFS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
DFSroll<-as.data.frame(DFSroll)

#Add date column back into
Freeze7 <- cbind(date = DFS$date,DFSroll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
Freeze7<-Freeze7 %>% pivot_longer(cols=2:ncol(Freeze7),names_to="id",values_to="freeze.day")

#Check to ensure that there is a range of freezes 
Freeze7 %>% group_by(freeze.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
Freeze7$id<-as.factor(Freeze7$id)

levels(Freeze7$id)
levels(MHD$id)

#Merge 
MHD_freeze30<-left_join(MHD, Freeze7, by=c("date", "id"))
summary(MHD_freeze30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```



*Developmental Zero Calculations* 

```{r}
###########################################
##########DAILY DevZero DATA###############
###########################################
#Weather_Hardi <- read_csv("C:/Users/CHS/Desktop/RFiles/MHD/MHD_5RFiles/MHD_AnalysesFiles/data/Weather_Hardi.csv")
Weather_Hardi <- read_csv("./data/Weather_Hardi.csv")

#Modify the weather data to prepare it for analysis
  #Rename Columns to match MHD
library(dplyr)
Weather_Hardi<-Weather_Hardi %>%
  rename(Year = Year.Sampled,
    Station = StationName)

 #Make sure date column is in correct format
library(lubridate)
Weather_Hardi$date<- parse_date_time(Weather_Hardi$date, orders = c("mdy", "dmy","ymd"))
Weather_Hardi$date<-as.Date(Weather_Hardi$date)
#Create the week variable
Weather_Hardi$week <- week(Weather_Hardi$date)

#Turn each daily observation into a 0 or 1 for freezes, developmental 0 and then count the number of occurrences in a week.
#Function for counting the number of freezes. Starting with identifying freeze days.
DevZero.days<-function (TMIN, Day){
  DevZero.days<-c()
  for (i in 1:length(TMIN)){
    if(is.na(TMIN[i])){
      devzero<-NA
    }else if (TMIN[i]>10){
      devzero<-0
    }else{
      devzero<-1
    }
    DevZero.days<-c(DevZero.days, devzero)
  }
  return(DevZero.days)
}

#and now the dev zero day counter
Weather_Hardi$devzero.day<-DevZero.days(Weather_Hardi$TMIN, Weather_Hardi$Day)

#Examine the number of dev zero days to ensure a reasonable number have been recorded
Weather_Hardi %>% group_by(devzero.day) %>% tally()
```


*Dev Zero 7 Day Calculations*
```{r Dev Zero 7 Day Calculations}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
DailyDevZeroSubset = subset(Weather_Hardi, select = c(id, date, devzero.day) )
DZS<-DailyDevZeroSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=devzero.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 7, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
DevZeroData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
DevZero_7DayData<-DevZeroData %>% pivot_longer(cols=2:ncol(DevZeroData),names_to="id",values_to="devzero.day")

#Check to ensure that there is a range of freezes 
DevZero_7DayData %>% group_by(devzero.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
DevZeroData$id<-as.factor(DevZeroData$id)

levels(DevZeroData$id)
levels(MHD$id)

#Merge 
DevZeroData7<-left_join(MHD, DevZero_7DayData, by=c("date", "id"))
summary(DevZeroData30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```

*Dev Zero 30 Day Calculations*
```{r}

##################################
#Use RollApply to Count DEvZero in 7, 30 day increments#
##################################
DailyDevZeroSubset = subset(Weather_Hardi, select = c(id, date, devzero.day) )
DZS<-DailyDevZeroSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=devzero.day) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
ZRoll<-rollapply(data = DZS [,c(2:96)],width = 30, 
                                     FUN = sum, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
ZRoll<-as.data.frame(ZRoll)

#Add date column back into
DevZeroData <- cbind(date = DZS$date,ZRoll )

#Turn station ID back into a single column for model analysis   
#Freeze7<-Freeze7 %>%  gather(id, freeze.day,c(2:ncol(Freeze7))) 
DevZeroData<-DevZeroData %>% pivot_longer(cols=2:ncol(DevZeroData),names_to="id",values_to="devzero.day")

#Check to ensure that there is a range of freezes 
DevZeroData %>% group_by(devzero.day) %>% tally()
#

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
DevZeroData$id<-as.factor(DevZeroData$id)

levels(DevZeroData$id)
levels(MHD$id)

#Merge 
DevZeroData30<-left_join(MHD, DevZeroData, by=c("date", "id"))
summary(DevZeroData30)

#check that all  stations in the MHD data frame are paired correctly with the weather stations. should return TRUE
all(unique(MHD$id) %in% unique(Freeze7$id))
```


*TMIN Calculations*

TMIN 7 Day Window Calculations
```{r TMIN 7 Day Window Calculations}
library(tidyr)
TMINSubset = subset(Weather_Hardi, select = c(id, date, TMIN) )
TMINS<-TMINSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=TMIN) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = TMINS [,c(2:96)],width = 7, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
TMINData <- cbind(date = TMINS$date,TRoll )

#Turn station ID back into a single column for model analysis   
TMINData<-TMINData %>% pivot_longer(cols=2:ncol(TMINData),names_to="id",values_to="TMIN.7day")

#Check to ensure that there is a range of freezes 
TMINData %>% group_by(TMIN.7day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMINData$id<-as.factor(TMINData$id)

#Merge 
TMINData7<-left_join(MHD, TMINData, by=c("date", "id"))
summary(TMINData7)
```

TMIN 30 Day Window Calculations
```{r TMIN 30 Day Window Calculations}
library(tidyr)
TMINSubset = subset(Weather_Hardi, select = c(id, date, TMIN) )
TMINS<-TMINSubset %>% pivot_wider(id_cols=date,names_from = id,values_from=TMIN) %>% arrange(date)

#Calculate the number of freezes 7 days prior to every observation
library(zoo)
TRoll<-rollapply(data = TMINS [,c(2:96)],width = 30, 
                                     FUN = mean, 
                                     align = "right", 
                                     fill = NA, 
                                     by.column = TRUE,
                                     na.rm = T)


#Turn zoo matrix into data frame
TRoll<-as.data.frame(TRoll)

#Add date column back into
TMINData <- cbind(date = TMINS$date,TRoll )

#Turn station ID back into a single column for model analysis   
TMINData<-TMINData %>% pivot_longer(cols=2:ncol(TMINData),names_to="id",values_to="TMIN.30day")

#Check to ensure that there is a range of freezes 
TMINData %>% group_by(TMIN.30day) %>% tally()

#Prepare data columns for the merge
MHD$date<-as.Date(MHD$date)
MHD <- MHD %>% rename( id = Station)
MHD$id<-as.factor(MHD$id)
TMINData$id<-as.factor(TMINData$id)

#Merge 
TMINData30<-left_join(MHD, TMINData, by=c("date", "id"))
summary(TMINData30)
```

*GAM Models* 
Base Model without Environmental Variables 
```{r Base GAM Model}
library(mgcv)

###################Base###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
O_gam <-mgcv:: gam(Infection ~ s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(O_gam)
AIC(O_gam)
#14165.96
plot(O_gam, pages = 1,  trans = plogis,
     shift = coef(O_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

TMINData7$Sex<- as.factor(TMINData7$Sex)
O_gam2 <-mgcv:: gam(Infection ~ s(Year, k= 5) + s(longitude,latitude) + Sex + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
AIC(O_gam2)
#AIC without Sex as a factor 13685.25, same with it as a factor 
summary(O_gam2)
plot(O_gam2, pages = 1,  trans = plogis,
     shift = coef(O_gam2)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
#Not allowing sex to be grpahed for some reason?!? SIgnificantly better though AIC value - hhh because can't be smoothed
#Need to change U sex to NA so that only male, female and NA are in column

```

TMAX Raw GAM
```{r TMAX Raw GAM}

###################TMIN RAW###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
Max_gam <-mgcv:: gam(Infection ~ s(TMAX2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(Max_gam)
AIC(Max_gam)
#13372.5
plot(Max_gam, pages = 1,  trans = plogis,
     shift = coef(Max_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

*TMIN GAM Models* 
TMIN Raw GAM
```{r TMIN Raw GAM}
library(mgcv)

###################TMIN RAW###########################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
T_gam <-mgcv:: gam(Infection ~ s(TMIN2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(T_gam)
AIC(T_gam)
#13429.8
plot(T_gam, pages = 1,  trans = plogis,
     shift = coef(T_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMIN Previous 7 Day Average GAM
```{r TMIN 7 Day GAM}
###################TMIN 7 Day Average##################################
TMINData7$Volunteer<-as.factor(TMINData7$Volunteer)
T7_gam <-mgcv:: gam(Infection ~ s(TMIN.7day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(T7_gam)
AIC(T7_gam)
#12114.99
plot(T7_gam, pages = 1,  trans = plogis,
     shift = coef(T7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

TMIN Previous 30 Day Average GAM
```{r TMIN 30 Day}
###################TMIN 30 Day Average##################################
TMINData30$Volunteer<-as.factor(TMINData30$Volunteer)
T30_gam <-mgcv:: gam(Infection ~ s(TMIN.30day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData30)
summary(T30_gam)
AIC(T30_gam)
#12204.9
plot(T30_gam, pages = 1,  trans = plogis,
     shift = coef(T30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```

*FREEZE GAM Models* 

Freeze 7 Day Average GAM
```{r Freeze 7 Day Average GAM}
###################FREEZE 7 Day Average##################################

MHD_freeze$Volunteer<-as.factor(MHD_freeze$Volunteer)
F7_gam <-mgcv:: gam(Infection ~ s(freeze.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = MHD_freeze)
summary(F7_gam)
AIC(F7_gam)
#12523.63
plot(F7_gam, pages = 1,  trans = plogis,
     shift = coef(F7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```


Freeze 30 Day Average GAM
```{r Freeze 30 Day Average GAM}

###################FREEZE 30 Day Average##################################
library(mgcv)
MHD_freeze30$Volunteer<-as.factor(MHD_freeze30$Volunteer)
F30_gam <-mgcv:: gam(Infection ~ s(freeze.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = MHD_freeze30)
summary(F30_gam)
AIC(F30_gam)
#12529.84
plot(F30_gam, pages = 1,  trans = plogis,
     shift = coef(F30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```

*DevZero GAM Models* 
DevZero 7 Day Average GAM
```{r}

###################DevZero 7 Day Average##################################
library(mgcv)
DevZeroData7$Volunteer<-as.factor(DevZeroData7$Volunteer)
Z7_gam <-mgcv:: gam(Infection ~ s(devzero.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = DevZeroData7)
summary(Z7_gam)
AIC(Z7_gam)
#12527.52
plot(Z7_gam, pages = 1,  trans = plogis,
     shift = coef(Z7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```

DevZero 30 Day Average GAM
```{r}

###################DevZero 30 Day Average##################################
library(mgcv)
DevZeroData30$Volunteer<-as.factor(DevZeroData30$Volunteer)
Z30_gam <-mgcv:: gam(Infection ~ s(devzero.day, k=6) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = DevZeroData30)
summary(Z30_gam)
AIC(Z30_gam)
#12499.83
plot(Z30_gam, pages = 1,  trans = plogis,
     shift = coef(Z30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

```



*Precipitation GAM Models*
Create the following models: 
PRCP RAW 
PRCP 7 Day 
PRCP 30 Day 
Interaction between TMIN and PRCP 

PRCP Raw GAM
```{r TMIN Raw GAM}
library(mgcv)

###################TMIN RAW###########################################
PRCP$Volunteer<-as.factor(TMINData7$Volunteer)
T_gam <-mgcv:: gam(Infection ~ s(TMIN2) + s(Year, k= 5) + s(longitude,latitude) + s(Volunteer,bs="re") , family = binomial, link = logit, data = TMINData7)
summary(T_gam)
AIC(T_gam)
#13429.8
plot(T_gam, pages = 1,  trans = plogis,
     shift = coef(T_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")
```









































*Environmental Variable Models* 
Each of these GAM models includes 
```{r GAM Models with Freeze Data}
library(mgcv)

F7_gam <-mgcv:: gam(Infection ~ s(freeze.day, k=6) + s(Year, k= 5) , family = binomial, link = logit, data = data)
summary(F7_gam)
AIC(F7_gam)
#18864.28
plot(F7_gam, pages = 1,  trans = plogis,
     shift = coef(F7_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")


F30_gam <-mgcv:: gam(Infection ~ s(freeze.30.previous) +  s(Year.x, k= 5)  , family = binomial, link = logit, data = data)
summary(F30_gam)
AIC(F30_gam)
#15771.4
plot(F30_gam, pages = 1,  trans = plogis,
     shift = coef(F30_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")

F60_gam <-mgcv:: gam(Infection ~ s(freeze.60.previous) + + s(Year.x, k= 5)  , family = binomial, link = logit, data = data)
summary(F60_gam)
AIC(F60_gam)
#14036.62
plot(F60_gam, pages = 1,  trans = plogis,
     shift = coef(F60_gam)[1], seWithMean = TRUE, rug = FALSE, shade = TRUE, shade.col = "lightgrey")


```




